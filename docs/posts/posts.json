[
  {
    "path": "posts/S&P_Bayes_Project/",
    "title": "Bayes Finance Project",
    "description": "Using Bayes Methods to predict future earnings!",
    "author": [
      {
        "name": "Nicholas Di, Nolan Meyer, and Duc Ngo",
        "url": {}
      }
    ],
    "date": "2021-12-11",
    "categories": [],
    "contents": "\nProject\nWelcome to our STAT 454: Bayesian Statistics capstone project. All of\nus group members have an interest in and connections to the financial\nworld, whether that be through our majors or internships, which led us\ntoward this topic. Financial information, like stock market prices, are\nknown to be notoriously hard to predict. We wanted to take a Bayesian\napproach to try and tackle a similar situation: predicting the future\nearnings of S&P 500 companies. In this project we seek to model\nfuture earnings using other financial information about a company, like\nprevious earnings and sales. We explore a few Bayesian hierarchical\nmodels, as well as a SARIMA model using the bayesforcast package to try\nand identify one that can provide insight and better predictions for\nfuture company’s earnings.\nProject\nLink\n\n\n\n",
    "preview": "https://wealthface.com/blog/wp-content/uploads/2021/05/SP500.jpg",
    "last_modified": "2022-05-23T15:08:09-07:00",
    "input_file": "Bayes-Project.knit.md"
  },
  {
    "path": "posts/YouTube_Viz/",
    "title": "YouTube Shiny App",
    "description": "Interactive visualization for youtube viewing data!",
    "author": [],
    "date": "2021-07-01",
    "categories": [],
    "contents": "\n\n\nlibrary(shiny)\nlibrary(tidyverse)\ndata <- read.csv(\"~/Documents/Intro To Data Science/USvideos.txt\")\ndata_cleaned<- data %>%\n  filter(category_id %in% c(\"1\", \"2\", \"10\", \"15\", \"17\", \"19\", \"22\", \"23\", \"24\", \"26\", \"27\", \"28\")) %>%\n  mutate(time_string = toString(publish_time)) %>%\n  mutate(day = substr(time_string, 9, 10)) %>%\n  mutate(likes = as.numeric(likes, na.rm = TRUE)) %>%\n  mutate(dislikes = as.numeric(dislikes, na.rm = TRUE))%>%\n  mutate(like_dislike_ratio = likes / (dislikes + 1))%>%\n  select(title, category_id, tags, views, likes, comment_count, dislikes, like_dislike_ratio)\n\ndata_cleaned <- data_cleaned %>%\n  mutate(category = ifelse(category_id == \"1\", \"Autos & Vehicles\", ifelse(category_id == \"2\", \"Music\", ifelse(category_id == \"10\", \"Comedy\", \n                                                                                                              ifelse(category_id == \"15\", \"Science & Technology\", ifelse(category_id == \"17\", \"Science & Technology\", \n                                                                                                                                                                         ifelse(category_id == \"17\", \"Movies\", ifelse(category_id == \"19\", \"Action/Adventure\", ifelse(category_id == \"22\", \"Documentary\",\n                                                                                                                                                                                                                                                                      ifelse(category_id == \"23\", \"Drama\", ifelse(category_id == \"24\", \"Family\", ifelse(category_id == \"26\", \"Horror\", \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ifelse(category_id == \"27\", \"Sci-Fi/Fantasy\", \"Thriller\")))))))))))))\ngraphdata <- data_cleaned %>%\n  mutate(category_id_c = as.character(category)) %>% \n  group_by(category_id_c) %>% \n  mutate(views = as.numeric(views, na.rm = TRUE)) %>%\n  mutate(comment_count = as.numeric(comment_count, na.rm = TRUE)) %>%\n  summarise(totallikes = sum(likes, na.rm = TRUE), totaldislikes = sum(dislikes, na.rm = TRUE), \n            totalviews = sum(views, na.rm = TRUE), totalcomment = sum(comment_count, na.rm = TRUE),\n            totalvideos = n(), likes_per_video = totallikes/totalvideos, \n            dislikes_per_video = totaldislikes/ totalvideos, views_per_video = totalviews/totalvideos,\n            comment_count_per_video = totalcomment/totalvideos, like_dislike_ratio_per_video = mean(like_dislike_ratio, na.rm = TRUE))\n\nui <- fluidPage(\n  \n  title = \"Youtube Recommendation\",\n  \n  sidebarLayout(\n    sidebarPanel(\n      conditionalPanel(\n        'input.dataset === \"Users Manual\"',\n        helpText(\"Here is the user's manual\")\n      ),\n      conditionalPanel(\n        'input.dataset === \"Likes\"',\n        selectInput(\"category1\",\n                    \"Category\",\n                    choices = list(\"Autos & Vehicles\" = \"1\", \"Music\" = \"2\", \"Comedy\" = \"10\", \"Science & Technology\" = \"15\", \"Movies\" = \"17\", \"Action/Adventure\" = \"19\", \n                                   \"Documentary\" = \"22\", \"Drama\" = \"23\", \"Family\" = \"24\", \"Horror\" = \"26\", \"Sci-Fi/Fantasy\" = \"27\", \"Thriller\" = \"28\")),\n        checkboxGroupInput(\"show_vars1\",\"Columns to show:\", \n                           list(\"Title\" = \"title\", \"Views\" = \"views\", \"Likes\" = \"likes\", \n                                \"Comment Count\" = \"comment_count\", \"Dislikes\" = \"dislikes\", \"Like Dislike Ratio\" = \"like_dislike_ratio\"), \n                           selected = list(\"title\", \"likes\"))\n      ),\n      conditionalPanel(\n        'input.dataset === \"Dislikes\"',\n        selectInput(\"category2\",\n                    \"Category\",\n                    choices = list(\"Autos & Vehicles\" = \"1\", \"Music\" = \"2\", \"Comedy\" = \"10\", \"Science & Technology\" = \"15\", \"Movies\" = \"17\", \"Action/Adventure\" = \"19\", \n                                   \"Documentary\" = \"22\", \"Drama\" = \"23\", \"Family\" = \"24\", \"Horror\" = \"26\", \"Sci-Fi/Fantasy\" = \"27\", \"Thriller\" = \"28\")),\n        checkboxGroupInput(\"show_vars2\",\"Columns to show:\", \n                          list(\"Title\" = \"title\", \"Views\" = \"views\", \"Likes\" = \"likes\", \n                               \"Comment Count\" = \"comment_count\", \"Dislikes\" = \"dislikes\", \"Like Dislike Ratio\" = \"like_dislike_ratio\"), \n                          selected = list(\"title\", \"dislikes\"))\n      ),\n      conditionalPanel(\n        'input.dataset === \"Comment Count\"',\n        selectInput(\"category3\",\n                    \"Category\",\n                    choices = list(\"Autos & Vehicles\" = \"1\", \"Music\" = \"2\", \"Comedy\" = \"10\", \"Science & Technology\" = \"15\", \"Movies\" = \"17\", \"Action/Adventure\" = \"19\", \n                                   \"Documentary\" = \"22\", \"Drama\" = \"23\", \"Family\" = \"24\", \"Horror\" = \"26\", \"Sci-Fi/Fantasy\" = \"27\", \"Thriller\" = \"28\")),\n        checkboxGroupInput(\"show_vars3\",\"Columns to show:\", \n                           list(\"Title\" = \"title\", \"Views\" = \"views\", \"Likes\" = \"likes\", \n                                \"Comment Count\" = \"comment_count\", \"Dislikes\" = \"dislikes\", \"Like Dislike Ratio\" = \"like_dislike_ratio\"), \n                           selected = list(\"title\", \"comment_count\"))\n      ),\n      conditionalPanel(\n        'input.dataset === \"Views\"',\n        selectInput(\"category4\",\n                    \"Category\",\n                    choices = list(\"Autos & Vehicles\" = \"1\", \"Music\" = \"2\", \"Comedy\" = \"10\", \"Science & Technology\" = \"15\", \"Movies\" = \"17\", \"Action/Adventure\" = \"19\", \n                                   \"Documentary\" = \"22\", \"Drama\" = \"23\", \"Family\" = \"24\", \"Horror\" = \"26\", \"Sci-Fi/Fantasy\" = \"27\", \"Thriller\" = \"28\")),\n        checkboxGroupInput(\"show_vars4\",\"Columns to show:\", \n                           list(\"Title\" = \"title\", \"Views\" = \"views\", \"Likes\" = \"likes\", \n                                \"Comment Count\" = \"comment_count\", \"Dislikes\" = \"dislikes\", \"Like Dislike Ratio\" = \"like_dislike_ratio\"), \n                           selected = list(\"title\", \"views\"))\n      ),\n      conditionalPanel(\n        'input.dataset === \"Like Dislike Ratio\"',\n        selectInput(\"category5\",\n                    \"Category\",\n                    choices = list(\"Autos & Vehicles\" = \"1\", \"Music\" = \"2\", \"Comedy\" = \"10\", \"Science & Technology\" = \"15\", \"Movies\" = \"17\", \"Action/Adventure\" = \"19\", \n                                   \"Documentary\" = \"22\", \"Drama\" = \"23\", \"Family\" = \"24\", \"Horror\" = \"26\", \"Sci-Fi/Fantasy\" = \"27\", \"Thriller\" = \"28\")),\n        checkboxGroupInput(\"show_vars5\",\"Columns to show:\", \n                           list(\"Title\" = \"title\", \"Views\" = \"views\", \"Likes\" = \"likes\", \n                                \"Comment Count\" = \"comment_count\", \"Dislikes\" = \"dislikes\", \"Like Dislike Ratio\" = \"like_dislike_ratio\"), \n                           selected = list(\"title\", \"like_dislike_ratio\"))\n      ),\n      conditionalPanel(\n        'input.dataset === \"Ranking Plot\"',\n        selectInput(\"Vars\",\n                    \"Ranking Categories of Video according to:\",\n                    choices = list(\"Views Per Video\" = \"views\", \"Likes Per Video\" = \"likes\",\n                                   \"Comment Count Per Video\" = \"comment_count\", \"Dislikes Per Video\" = \"dislikes\", \"Average Like Dislike Ratio\" = \"like_dislike_ratio\"))\n      ),\n      \n      conditionalPanel(\n        'input.dataset === \"Distribution of Likes by Categories\"',\n        checkboxGroupInput(\"Categoreis\",\"Categories to be included in the Visulization\",\n                           list(\"Autos & Vehicles\" = \"Autos & Vehicles\", \"Music\" = \"Music\", \"Comedy\" = \"Comedy\", \"Science & Technology\" = \"Science & Technology\", \"Movies\" = \"Movies\", \"Action/Adventure\" = \"Action/Adventure\",\n                                \"Documentary\" = \"Documentary\", \"Drama\" = \"Drama\", \"Family\" = \"Family\", \"Horror\" = \"Horror\", \"Sci-Fi/Fantasy\" = \"Sci-Fi/Fantasy\", \"Thriller\" = \"Thriller\"),\n                           selected = (\"Autos & Vehicles\")\n        )\n      )\n    ),\n    \n    mainPanel(\n      tabsetPanel(\n        id = 'dataset',\n        tabPanel(\"Users Manual\", tags$h2(\"User's Manual\"), htmlOutput(\"manual\")),\n        tabPanel(\"Likes\", DT::dataTableOutput(\"mytable_likes\")),\n        tabPanel(\"Dislikes\", DT::dataTableOutput(\"mytable_dislikes\")),\n        tabPanel(\"Comment Count\", DT::dataTableOutput(\"mytable_comment_count\")),\n        tabPanel(\"Views\", DT::dataTableOutput(\"mytable_views\")),\n        tabPanel(\"Like Dislike Ratio\", DT::dataTableOutput(\"mytable_like_dislike_ratio\")),\n        tabPanel(\"Ranking Plot\", plotOutput(outputId = \"Rankplot\")),\n        tabPanel(\"Distribution of Likes by Categories\", plotOutput(outputId = \"Boxplot\"))\n        \n      )\n    )\n  )\n)\n\n\n\nserver <- function(input, output) {\n    \n    output$mytable_likes <- DT::renderDataTable({\n      data_cleaned %>%\n        mutate(likes = as.numeric(likes)) %>%\n        filter(category_id == input$category1) %>%\n        arrange(desc(likes)) %>%\n        select(input$show_vars1)\n    })\n    \n    output$mytable_dislikes <- DT::renderDataTable({\n      data_cleaned %>%\n        mutate(dislikes = as.numeric(dislikes)) %>%\n        filter(category_id == input$category2) %>%\n        arrange(dislikes) %>%\n        select(input$show_vars2)\n    })\n    \n    output$mytable_comment_count <- DT::renderDataTable({\n      data_cleaned %>%\n        mutate(comment_count = as.numeric(comment_count)) %>%\n        filter(category_id == input$category3) %>%\n        arrange(desc(comment_count)) %>%\n        select(input$show_vars3)\n    })\n    \n    output$mytable_views <- DT::renderDataTable({\n      data_cleaned %>%\n        mutate(views = as.numeric(views)) %>%\n        filter(category_id == input$category4) %>%\n        arrange(desc(views)) %>%\n        select(input$show_vars4)\n    })\n    \n    output$mytable_like_dislike_ratio <- DT::renderDataTable({\n      data_cleaned %>%\n        mutate(like_dislike_ratio = as.numeric(like_dislike_ratio))%>%\n        filter(category_id == input$category5)%>%\n        arrange(desc(like_dislike_ratio)) %>%\n        select(input$show_vars5)\n    })\n    \n    output$manual <- renderText({\n      paste(\n        \"<p>Welcome to our shiny app! This is a fairly self-explanatory app. First off, start off my selecting the categories in which you would like to browse the data in. To select a category, clock on the drop-down box and scroll up or down until you see what you want to explore. Simply click on the option you’d like. We then select what results we would like to view. Check or uncheck what columns we would like to show in the results.<\/p>\n         <p> Then, we will be able to rank the videos by Likes, Dislikes, Comment Count, Views, and Like-Dislike Ratio. By selecting one, the app will bring you to a page where you can search by keywords and sort by likes and alphabetical order of the title. You may also choose to see how many entries you view per page. To go to the next page, click on the numbers or next.<\/p>\n        \"\n      )\n    })\n    \n    output$Rankplot <- renderPlot(\n      graphdata %>%\n        ggplot(aes(y = fct_reorder(category_id_c,\n                                   eval(as.name(paste(input$Vars,\"_per_video\", sep=\"\")))\n                                   ),\n                   x = eval(as.name(paste(input$Vars, \"_per_video\", sep=\"\"))), \n                   fill = category_id_c)) +\n        geom_bar(stat = 'identity') +\n        labs(y = \"Category of Music\",\n             title = paste(\"Ranking of Category by\",input$Vars),\n             x = \"\"\n             )+\n        theme(legend.position = \"none\")\n    )\n\n    output$Boxplot <- renderPlot(\n      data_cleaned %>%\n        filter(category == input$Categoreis) %>%\n        mutate(category_c = as.character(category)) %>%\n        group_by(category_c) %>%\n        summarize(category_c, likes) %>%\n        ggplot(aes(y = reorder(category_c,likes,median), x=likes, fill = category_c)) +\n        geom_boxplot()+\n        labs(y = \"Category ID\",\n             x = \"Likes\",\n             title = \"Distribution by Category\")+\n        theme_minimal()+\n        xlim(0,100000)+\n        theme(legend.position = \"none\")\n    )\n    \n}\n\nshinyApp(ui = ui, server = server)\n\n\nShiny applications not supported in static R Markdown documents\n\n\n\n\n",
    "preview": "https://cdn.mos.cms.futurecdn.net/8gzcr6RpGStvZFA2qRt4v6.jpg",
    "last_modified": "2022-05-23T11:52:03-07:00",
    "input_file": {}
  },
  {
    "path": "posts/Spotify_Project/",
    "title": "Machine Learning Semester Project",
    "description": "We analyzed data from Spotify with the goal of predicting song popularity!",
    "author": [
      {
        "name": "Nicholas Di and Amy Xu",
        "url": {}
      }
    ],
    "date": "2020-05-01",
    "categories": [],
    "contents": "\nData Context\nWe selected the dataset “Top Spotify songs from 2010-2019 - BY YEAR”\nfrom Kaggle, which consists of the top songs by year in the world on\nSpotify, and the data is based on Billboard. The dataset contains 13\nvariables to be explored, including information about the songs such as\nthe song’s title, the song’s artist, the genre of the track, the year in\nwhich the song was in the Billboard rankings, its duration and\nacousticness. There are also variables describing the music, such as\nbeats per minute (which characterizes the tempo of the song), energy\n(the higher the value, the more energetic the song is), danceability\n(the higher the value, the easier it is to dance to this song), loudness\n(measured in dB), liveness (the higher the value, the more likely the\nsong is a live recording), valence (the higher the value, the more\npositive the mood is for the song), and speechiness (the higher the\nvalue, the more spoken words the song contains). The outcome variable in\nthis dataset is popularity, where the higher the value, the more popular\nthe song is.\nThe data were extracted from: http://organizeyourmusic.playlistmachinery.com/, and the\ndataset was constructed by Leonardo Henrique, updated in 2020. He was\ninterested in what we could know about the specific music genre based on\nthe popularity of the songs, and what elements would contribute to this\npopularity.\nResearch questions\nRegression: How can we predict the energy level of a song based on\nall other predictors? Classification: How can we predict whether the\nsong is amongst the most popular based on all other predictors?\n\n\n# Load data and required packages\nlibrary(caret)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ISLR)\nlibrary(splines)\nlibrary(caret)\nlibrary(stats)\nlibrary(lattice)\nlibrary(leaps)\nlibrary(gam)\ntop_spotify <- read_csv('https://www.dropbox.com/s/fi22whryueo4q85/top10s.csv?dl=1')\n\n\n\n\n\n# Any code to clean the data\ntop_spotify_new <- top_spotify %>% select(-artist,-'top genre',-title,-...1)\n#There seems to be an outlier \ntop_spotify_new <- filter(top_spotify_new, bpm > 1)\n\n\n\nInitial\ninvestigation 1: ignoring nonlinearity (for now)\nWe ordinary least squares (OLS) regression, forward and/or backward\nselection, and LASSO to build initial models for our quantitative\noutcome as a function of the predictors of interest.\nOLS Model\nFit the ordinary least squares (OLS) regression model:\n\n\nOLS <- lm(nrgy ~ year + acous + bpm + pop + dnce + live + spch + dur, data = top_spotify_new)\nsummary(OLS)\n\n\n\nCall:\nlm(formula = nrgy ~ year + acous + bpm + pop + dnce + live + \n    spch + dur, data = top_spotify_new)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.223  -8.030   1.092   8.540  31.239 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.347e+03  4.228e+02   5.552 4.26e-08 ***\nyear        -1.119e+00  2.097e-01  -5.337 1.34e-07 ***\nacous       -4.123e-01  2.611e-02 -15.790  < 2e-16 ***\nbpm         -3.427e-03  2.178e-02  -0.157  0.87504    \npop         -4.448e-02  3.713e-02  -1.198  0.23131    \ndnce         1.833e-03  4.190e-02   0.044  0.96511    \nlive         1.239e-01  4.012e-02   3.087  0.00212 ** \nspch         2.046e-01  6.916e-02   2.958  0.00322 ** \ndur         -7.281e-02  1.568e-02  -4.644 4.20e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.51 on 593 degrees of freedom\nMultiple R-squared:  0.402, Adjusted R-squared:  0.3939 \nF-statistic: 49.83 on 8 and 593 DF,  p-value: < 2.2e-16\n\nset.seed(253)\nOLS_cv <- train(\n    nrgy ~ year + acous + bpm + pop + dnce + live + spch + dur,\n    data = top_spotify_new,\n    method = \"lm\",\n    trControl = trainControl(method = \"cv\", number = 9),\n    na.action = na.omit)\n\n\n\nBackward Stepwise Selection\nModel\nFit the Backward Stepwise Selection model:\n\n\nfull_model <- lm(nrgy ~ ., data = top_spotify_new)\nsummary(full_model)\n\n\n\nCall:\nlm(formula = nrgy ~ ., data = top_spotify_new)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-28.7877  -6.2817   0.6035   6.8994  27.0338 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 812.704006 344.864487   2.357 0.018769 *  \nyear         -0.350636   0.171100  -2.049 0.040874 *  \nbpm          -0.004972   0.017149  -0.290 0.771957    \ndnce         -0.106738   0.037418  -2.853 0.004488 ** \ndB            4.479083   0.268874  16.659  < 2e-16 ***\nlive          0.103313   0.031586   3.271 0.001135 ** \nval           0.113503   0.022958   4.944 9.99e-07 ***\ndur          -0.017890   0.012827  -1.395 0.163629    \nacous        -0.289294   0.021547 -13.426  < 2e-16 ***\nspch          0.206303   0.055593   3.711 0.000226 ***\npop          -0.074613   0.029247  -2.551 0.010988 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.84 on 591 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.625 \nF-statistic: 101.2 on 10 and 591 DF,  p-value: < 2.2e-16\n\nset.seed(253)\nback_step_mod <- train(\n    nrgy ~ .,\n    data = top_spotify_new,\n    method = 'leapBackward',\n    tuneGrid = data.frame(nvmax = 1:10),\n    trControl = trainControl(method = 'cv',number = 9),\n    metric = 'MAE',\n    na.action = na.omit\n)\n\n\n\nForward Stepwise Selection\nModel\nFit the Forward Stepwise Selection model:\n\n\nset.seed(253)\nfor_step_mod <- train(\n    nrgy ~ .,\n    data = top_spotify_new,\n    method = 'leapForward',\n    tuneGrid = data.frame(nvmax = 1:10),\n    trControl = trainControl(method = 'cv',number = 9),\n    metric = 'MAE',\n    na.action = na.omit\n)\n\n\n\nLASSO Model\nFit the LASSO model:\n\n\nset.seed(253)\nlasso_mod <- train(\n    nrgy ~ .,\n    data = top_spotify_new,\n    method = \"glmnet\",\n    tuneGrid = data.frame(alpha = 1, lambda = seq(0, 10, length.out = 100)),\n    trControl = trainControl(method = \"cv\", number = 9, selectionFunction = 'oneSE'),\n    metric = \"MAE\",\n    na.action = na.omit)\n\n\n\nCompare performances\nof different models:\nEstimate test performance of the models from these different methods.\nReport and interpret (with units) these estimates along with a measure\nof uncertainty in the estimate (SD is most readily available from\ncaret).\nExamine OLS model outputs:\n\n\nsummary(OLS_cv)\n\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.223  -8.030   1.092   8.540  31.239 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.347e+03  4.228e+02   5.552 4.26e-08 ***\nyear        -1.119e+00  2.097e-01  -5.337 1.34e-07 ***\nacous       -4.123e-01  2.611e-02 -15.790  < 2e-16 ***\nbpm         -3.427e-03  2.178e-02  -0.157  0.87504    \npop         -4.448e-02  3.713e-02  -1.198  0.23131    \ndnce         1.833e-03  4.190e-02   0.044  0.96511    \nlive         1.239e-01  4.012e-02   3.087  0.00212 ** \nspch         2.046e-01  6.916e-02   2.958  0.00322 ** \ndur         -7.281e-02  1.568e-02  -4.644 4.20e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.51 on 593 degrees of freedom\nMultiple R-squared:  0.402, Adjusted R-squared:  0.3939 \nF-statistic: 49.83 on 8 and 593 DF,  p-value: < 2.2e-16\n\nOLS_cv$results\n\n\n  intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD   MAESD\n1      TRUE 12.62445 0.3880631 10.13206 1.140868  0.1381483 0.83748\n\nOn average, we’re off in top song energy predictions by about\n10.13206 points.\nResidual plot for OLS model:\n\n\nOLS_mod_output <- broom::augment(OLS, newdata = top_spotify_new)\n\nggplot(OLS_mod_output, aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\")\n\n\n\n\nExamine Backward Stepwise Selection model output:\n\n\nsummary(back_step_mod)\n\n\nSubset selection object\n10 Variables  (and intercept)\n      Forced in Forced out\nyear      FALSE      FALSE\nbpm       FALSE      FALSE\ndnce      FALSE      FALSE\ndB        FALSE      FALSE\nlive      FALSE      FALSE\nval       FALSE      FALSE\ndur       FALSE      FALSE\nacous     FALSE      FALSE\nspch      FALSE      FALSE\npop       FALSE      FALSE\n1 subsets of each size up to 9\nSelection Algorithm: backward\n         year bpm dnce dB  live val dur acous spch pop\n1  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"   \" \"  \" \"\n2  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \"*\"   \" \"  \" \"\n3  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \"*\"   \"*\"  \" \"\n4  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \"*\" \" \" \"*\"   \"*\"  \" \"\n5  ( 1 ) \" \"  \" \" \" \"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \" \"\n6  ( 1 ) \" \"  \" \" \"*\"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \" \"\n7  ( 1 ) \" \"  \" \" \"*\"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \"*\"\n8  ( 1 ) \"*\"  \" \" \"*\"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \"*\"\n9  ( 1 ) \"*\"  \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\" \"*\"   \"*\"  \"*\"\n\nplot(back_step_mod)\n\n\n\nback_step_mod$bestTune\n\n\n  nvmax\n9     9\n\nWe chose the model with 6 predictors. Although all 10 yields better\nmodel metrics, we believe that we will run into problems of\nover-fitting.\nOn average, we’re off in top song energy predictions by about 8.286\npercentage points, if we use the model with 6 predictors.\n\n\ncoef(back_step_mod$finalModel, id = 6)\n\n\n(Intercept)        dnce          dB        live         val \n 97.4672886  -0.1228588   4.5587752   0.1128596   0.1264183 \n      acous        spch \n -0.2930557   0.2006552 \n\nback_step_mod$results %>% filter(nvmax==6)\n\n\n  nvmax     RMSE  Rsquared      MAE    RMSESD RsquaredSD    MAESD\n1     6 10.18581 0.5910122 8.286481 0.9244708  0.1070956 0.825095\n\nback_step_mod_eq <- lm(nrgy ~ dnce + dB + live + val + acous + spch, data =top_spotify_new)\n\n\n\nResidual plot for Backward Step-wise Selection model:\n\n\nback_step_mod_out <- top_spotify_new %>%\n    mutate(\n        fitted = predict(back_step_mod_eq, newdata = top_spotify_new),\n        resid = nrgy - fitted\n    )\n\nggplot(back_step_mod_out, aes(x = fitted, y = resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\") +\n    theme_classic() +\n    labs(x = \"Fitted values\", y = \"Residuals\")\n\n\n\n\nFor Loop looking at all predictors in the Backward Stepwise Selection\nmodel:\n\n\npredictors <- setdiff(colnames(top_spotify_new), c(\"year\",\"bpm\",\"nrgy\",\"dur\",\"spch\"))\nfor (pred in predictors) {\n    p <- ggplot(back_step_mod_out, aes(x = .data[[pred]], y = resid)) +\n        geom_point() +\n        geom_smooth() +\n        geom_hline(yintercept = 0, color = \"red\") +\n        theme_classic() +\n        labs(x = pred, y = \"Residuals\")\n    print(p)\n}\n\n\n\n\nExamine Forward Stepwise Selection model output:\n\n\nsummary(for_step_mod)\n\n\nSubset selection object\n10 Variables  (and intercept)\n      Forced in Forced out\nyear      FALSE      FALSE\nbpm       FALSE      FALSE\ndnce      FALSE      FALSE\ndB        FALSE      FALSE\nlive      FALSE      FALSE\nval       FALSE      FALSE\ndur       FALSE      FALSE\nacous     FALSE      FALSE\nspch      FALSE      FALSE\npop       FALSE      FALSE\n1 subsets of each size up to 9\nSelection Algorithm: forward\n         year bpm dnce dB  live val dur acous spch pop\n1  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"   \" \"  \" \"\n2  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \"*\"   \" \"  \" \"\n3  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \"*\"   \"*\"  \" \"\n4  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \"*\" \" \" \"*\"   \"*\"  \" \"\n5  ( 1 ) \" \"  \" \" \" \"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \" \"\n6  ( 1 ) \" \"  \" \" \"*\"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \" \"\n7  ( 1 ) \" \"  \" \" \"*\"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \"*\"\n8  ( 1 ) \"*\"  \" \" \"*\"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \"*\"\n9  ( 1 ) \"*\"  \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\" \"*\"   \"*\"  \"*\"\n\nplot(for_step_mod)\n\n\n\nfor_step_mod$results\n\n\n   nvmax      RMSE  Rsquared      MAE    RMSESD RsquaredSD     MAESD\n1      1 11.993192 0.4404595 9.440092 1.1663410 0.08326330 0.7602086\n2      2 10.477550 0.5683678 8.533998 0.6892741 0.09392739 0.6832649\n3      3 10.421451 0.5699856 8.442041 0.8481219 0.10845490 0.7918324\n4      4 10.333804 0.5765861 8.412541 0.8391433 0.11095779 0.7986408\n5      5 10.260625 0.5846211 8.383712 0.8495003 0.11238655 0.7669223\n6      6 10.142928 0.5941810 8.277152 0.8950607 0.10739226 0.8186073\n7      7  9.950075 0.6082421 8.067531 0.9987578 0.11136222 0.8886527\n8      8  9.940663 0.6092391 8.072114 0.9905555 0.10712761 0.9066449\n9      9  9.902507 0.6120420 8.049632 0.9827727 0.10630862 0.9021970\n10    10  9.915936 0.6110377 8.062302 0.9829506 0.10702524 0.9087888\n\nUsing Forward selection, we chose the model with 5 predictors. The\nfive predictors being acous, dB, live, spch, and val.\nOn average, we’re off in top song energy predictions by about\n8.383712 percentage points.\n\n\ncoef(for_step_mod$finalModel, id = 5)\n\n\n(Intercept)          dB        live         val       acous \n91.50592402  4.65576857  0.11831750  0.09041016 -0.28031999 \n       spch \n 0.22151861 \n\nfor_step_mod$results %>% filter(nvmax==5)\n\n\n  nvmax     RMSE  Rsquared      MAE    RMSESD RsquaredSD     MAESD\n1     5 10.26063 0.5846211 8.383712 0.8495003  0.1123866 0.7669223\n\nfor_step_mod_eq <- lm(nrgy ~ acous + dB + val + spch + live, data =top_spotify_new)\n\n\n\nResidual plot for Forward Stepwise Selection model:\n\n\nfor_step_mod_out <- top_spotify_new %>%\n    mutate(\n        fitted = predict(for_step_mod_eq, newdata = top_spotify_new),\n        resid = nrgy - fitted\n    )\n\nggplot(for_step_mod_out, aes(x = fitted, y = resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\") +\n    theme_classic() +\n    labs(x = \"Fitted values\", y = \"Residuals\")\n\n\n\n\nFor Loop looking at all predictors in the Forward Stepwise Selection\nmodel:\n\n\npredictors <- setdiff(colnames(top_spotify_new), c(\"year\",\"bpm\",\"nrgy\",\"dur\",\"pop\", \"dnce\"))\nfor (pred in predictors) {\n    p <- ggplot(for_step_mod_out, aes(x = .data[[pred]], y = resid)) +\n        geom_point() +\n        geom_smooth() +\n        geom_hline(yintercept = 0, color = \"red\") +\n        theme_classic() +\n        labs(x = pred, y = \"Residuals\")\n    print(p)\n}\n\n\n\n\nExamine LASSO model output:\n\n\nplot(lasso_mod$finalModel, xvar = \"lambda\", label = TRUE, col = rainbow(20))\n\n\n\nlasso_mod$bestTune\n\n\n   alpha   lambda\n11     1 1.010101\n\n# lasso_mod$results\nrownames(lasso_mod$finalModel$beta)[c(4,8)]\n\n\n[1] \"dB\"    \"acous\"\n\nWe chose a lambda value of 1.010101, dB and acous seem to be two of\nthe strongest/persistent predictors when it comes to energy-level.\n\n\ncoef(lasso_mod$finalModel, 1.010101)\n\n\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept) 427.22094252\nyear         -0.16551620\nbpm           .         \ndnce          .         \ndB            4.25384058\nlive          0.05257947\nval           0.06575686\ndur           .         \nacous        -0.25154488\nspch          0.10397394\npop          -0.02164891\n\nlasso_mod$results[11,]\n\n\n   alpha   lambda     RMSE Rsquared     MAE    RMSESD RsquaredSD\n11     1 1.010101 10.17634 0.597547 8.32769 0.8552498  0.1057273\n       MAESD\n11 0.7932597\n\nOn average, we’re off in top song energy predictions by about 8.32769\npercentage points using LASSO with a lambda of 1.010101.\nResidual plot for LASSO model:\n\n\nlasso_mod_out <- top_spotify_new %>%\n    mutate(\n        fitted = predict(lasso_mod, newdata = top_spotify_new),\n        resid = nrgy - fitted)\nggplot(lasso_mod_out, aes(x = fitted, y = resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\") +\n    theme_classic() +\n    labs(x = \"Fitted values\", y = \"Residuals\")\n\n\n\n\nThere does not seem to be any noticable patterns of over and\nunderpredicting here!\nFor Loop for LASSO model:\n\n\npredictors <- setdiff(colnames(top_spotify_new), \"Top Spotify\")\nfor (pred in predictors) {\n    p <- ggplot(lasso_mod_out, aes(x = .data[[pred]], y = resid)) +\n        geom_point() +\n        geom_smooth() +\n        geom_hline(yintercept = 0, color = \"red\") +\n        theme_classic() +\n        labs(x = pred, y = \"Residuals\")\n    print(p)\n}\n\n\n\n\nCompare estimated test performance across methods. Which method(s)\nmight you prefer?\nModel\nTraining MAE\nMAESD\nOLS Model\n10.13206\n0.83748\nBackward\n8.286481\n0.825095\nForward\n8.383712\n0.7669223\nLASSO\n8.32769\n0.7932597\nComparing the four models, LASSO and Backward Stepwise Selection\nmodels seem to be yielding the best results as the predictions for top\nsong energy level are closest to the test value.\nCompare insights from variable importance analyses from the different\nmethods (stepwise and LASSO, but not OLS). Are there variables for which\nthe methods reach consensus? What insights are expected? Surprising?\nAcross all models, the “top” 3 predictors for song energy level are\nacous (Acousticness- the higher the value the more acoustic the song\nis), dB(Loudness, the higher the value, the louder the song), and\nval(Valence, the higher the value, the more positive mood for the song).\nThis is mostly consistent with our expectation, as when the song is\nlouder, and more positive, the song has a higher energy level. It is a\nbit surprising that when a song is more acoustic, it is less\nenergetic.\nNote that if some (but not all) of the indicator terms for a\ncategorical predictor are selected in the final models, the whole\npredictor should be treated as selected.\nInvestigation 2:\nAccounting for nonlinearity\nUpdate your stepwise selection model(s) and LASSO model to use\nnatural splines for the quantitative predictors.\nYou’ll need to update the model formula from y ~ . to\nsomething like\ny ~ cat_var1 + ns(quant_var1, df) + ....\nIt’s recommended to use few knots (e.g., 2 knots = 3 degrees of\nfreedom).\nNote that ns(x,3) replaces x with 3\ntransformations of x. Keep this in mind when setting\nnvmax in stepwise selection.\n\n\nggplot(top_spotify_new, aes(x = val, y = nrgy)) +\n    geom_point(alpha = 0.25) +\n    geom_smooth(color = \"blue\", se = FALSE) +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\nggplot(top_spotify_new, aes(x = dB, y = nrgy)) +\n    geom_point(alpha = 0.25) +\n    geom_smooth(color = \"blue\", se = FALSE) +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\nggplot(top_spotify_new, aes(x = acous, y = nrgy)) +\n    geom_point(alpha = 0.25) +\n    geom_smooth(color = \"blue\", se = FALSE) +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\nggplot(top_spotify_new, aes(x = year, y = nrgy)) +\n    geom_point(alpha = 0.25) +\n    geom_smooth(color = \"blue\", se = FALSE) +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\nBackward\nStepwise Selection model with natural splines\nUpdate the Backward Stepwise Selection model to use natural splines\nfor the quantitative predictors:\n\n\nset.seed(253)\nback_spline_mod <- train(\n    nrgy ~ ns(acous, 3) + ns(pop, 3) + ns(dnce, 3) + ns(live, 3) + ns(dB, 3) + ns(val, 3),\n    data = top_spotify_new,\n    method = \"leapBackward\",\n    trControl = trainControl(method = \"cv\", number = 9, selectionFunction = \"oneSE\"),\n    metric = \"MAE\",\n    na.action = na.omit\n)\n\n\n\nForward\nStepwise Selection model with natural splines\nUpdate the Forward Stepwise Selection model to use natural splines\nfor the quantitative predictors:\n\n\nset.seed(253)\nfor_spline_mod <- train(\n    nrgy ~ ns(year, 3) + ns(acous, 3) + ns(bpm, 3) + ns(pop, 3) + ns(dnce, 3) + ns(live, 3) + ns(spch, 3) + ns(dur, 3),\n    data = top_spotify_new,\n    method = \"leapForward\",\n    trControl = trainControl(method = \"cv\", number = 9, selectionFunction = \"oneSE\"),\n    metric = \"MAE\",\n    na.action = na.omit\n)\n\n\n\nLASSO model with natural\nsplines\nUpdate the LASSO model to use natural splines for the quantitative\npredictors:\n\n\nset.seed(253)\nLASSO_spline_mod <- train(\n    nrgy ~ ns(year, 3) + ns(acous, 3) + ns(bpm, 3) + ns(pop, 3) + ns(dnce, 3) + ns(live, 3) + ns(spch, 3) + ns(dur, 3),\n    data = top_spotify_new,\n    method = \"glmnet\",\n    tuneGrid = data.frame(alpha = 1, lambda = seq(0, 10, length.out = 100)),\n    trControl = trainControl(method = \"cv\", number = 9, selectionFunction = \"oneSE\"),\n    metric = \"MAE\",\n    na.action = na.omit\n)\n\n\n\nCompare\ninsights from variable importance analyses\nCompare insights from variable importance analyses here and the\ncorresponding results from Investigation 1. Now after having accounted\nfor nonlinearity, have the most relevant predictors changed?\nNote that if some (but not all) of the spline terms are selected in\nthe final models, the whole predictor should be treated as\nselected.\nExamine Backward Stepwise Selection model with natural splines\noutput:\n\n\nsummary(back_spline_mod)\n\n\nSubset selection object\n18 Variables  (and intercept)\n              Forced in Forced out\nns(acous, 3)1     FALSE      FALSE\nns(acous, 3)2     FALSE      FALSE\nns(acous, 3)3     FALSE      FALSE\nns(pop, 3)1       FALSE      FALSE\nns(pop, 3)2       FALSE      FALSE\nns(pop, 3)3       FALSE      FALSE\nns(dnce, 3)1      FALSE      FALSE\nns(dnce, 3)2      FALSE      FALSE\nns(dnce, 3)3      FALSE      FALSE\nns(live, 3)1      FALSE      FALSE\nns(live, 3)2      FALSE      FALSE\nns(live, 3)3      FALSE      FALSE\nns(dB, 3)1        FALSE      FALSE\nns(dB, 3)2        FALSE      FALSE\nns(dB, 3)3        FALSE      FALSE\nns(val, 3)1       FALSE      FALSE\nns(val, 3)2       FALSE      FALSE\nns(val, 3)3       FALSE      FALSE\n1 subsets of each size up to 4\nSelection Algorithm: backward\n         ns(acous, 3)1 ns(acous, 3)2 ns(acous, 3)3 ns(pop, 3)1\n1  ( 1 ) \" \"           \" \"           \" \"           \" \"        \n2  ( 1 ) \" \"           \" \"           \"*\"           \" \"        \n3  ( 1 ) \" \"           \"*\"           \"*\"           \" \"        \n4  ( 1 ) \" \"           \"*\"           \"*\"           \" \"        \n         ns(pop, 3)2 ns(pop, 3)3 ns(dnce, 3)1 ns(dnce, 3)2\n1  ( 1 ) \" \"         \" \"         \" \"          \" \"         \n2  ( 1 ) \" \"         \" \"         \" \"          \" \"         \n3  ( 1 ) \" \"         \" \"         \" \"          \" \"         \n4  ( 1 ) \" \"         \" \"         \" \"          \" \"         \n         ns(dnce, 3)3 ns(live, 3)1 ns(live, 3)2 ns(live, 3)3\n1  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n2  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n3  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n4  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n         ns(dB, 3)1 ns(dB, 3)2 ns(dB, 3)3 ns(val, 3)1 ns(val, 3)2\n1  ( 1 ) \" \"        \" \"        \"*\"        \" \"         \" \"        \n2  ( 1 ) \" \"        \" \"        \"*\"        \" \"         \" \"        \n3  ( 1 ) \" \"        \" \"        \"*\"        \" \"         \" \"        \n4  ( 1 ) \"*\"        \" \"        \"*\"        \" \"         \" \"        \n         ns(val, 3)3\n1  ( 1 ) \" \"        \n2  ( 1 ) \" \"        \n3  ( 1 ) \" \"        \n4  ( 1 ) \" \"        \n\nplot(back_spline_mod)\n\n\n\nback_spline_mod$bestTune\n\n\n  nvmax\n3     4\n\nback_spline_mod$results\n\n\n  nvmax     RMSE  Rsquared       MAE    RMSESD RsquaredSD     MAESD\n1     2 12.94073 0.3520225 10.551403 1.0723596 0.09809830 0.7221742\n2     3 11.21314 0.5103530  9.152245 0.9208166 0.07104034 0.7258275\n3     4 10.28440 0.5900269  8.392090 0.8083434 0.09138549 0.7327469\n\nAccording to the Backward Stepwise Selection model with natural\nsplines, the top predictors for song energy level are acous and dB.\nExamine Forward Stepwise Selection model output:\n\n\nsummary(for_spline_mod)\n\n\nSubset selection object\n24 Variables  (and intercept)\n              Forced in Forced out\nns(year, 3)1      FALSE      FALSE\nns(year, 3)2      FALSE      FALSE\nns(year, 3)3      FALSE      FALSE\nns(acous, 3)1     FALSE      FALSE\nns(acous, 3)2     FALSE      FALSE\nns(acous, 3)3     FALSE      FALSE\nns(bpm, 3)1       FALSE      FALSE\nns(bpm, 3)2       FALSE      FALSE\nns(bpm, 3)3       FALSE      FALSE\nns(pop, 3)1       FALSE      FALSE\nns(pop, 3)2       FALSE      FALSE\nns(pop, 3)3       FALSE      FALSE\nns(dnce, 3)1      FALSE      FALSE\nns(dnce, 3)2      FALSE      FALSE\nns(dnce, 3)3      FALSE      FALSE\nns(live, 3)1      FALSE      FALSE\nns(live, 3)2      FALSE      FALSE\nns(live, 3)3      FALSE      FALSE\nns(spch, 3)1      FALSE      FALSE\nns(spch, 3)2      FALSE      FALSE\nns(spch, 3)3      FALSE      FALSE\nns(dur, 3)1       FALSE      FALSE\nns(dur, 3)2       FALSE      FALSE\nns(dur, 3)3       FALSE      FALSE\n1 subsets of each size up to 3\nSelection Algorithm: forward\n         ns(year, 3)1 ns(year, 3)2 ns(year, 3)3 ns(acous, 3)1\n1  ( 1 ) \" \"          \" \"          \" \"          \" \"          \n2  ( 1 ) \" \"          \" \"          \" \"          \" \"          \n3  ( 1 ) \" \"          \" \"          \" \"          \" \"          \n         ns(acous, 3)2 ns(acous, 3)3 ns(bpm, 3)1 ns(bpm, 3)2\n1  ( 1 ) \" \"           \"*\"           \" \"         \" \"        \n2  ( 1 ) \"*\"           \"*\"           \" \"         \" \"        \n3  ( 1 ) \"*\"           \"*\"           \" \"         \" \"        \n         ns(bpm, 3)3 ns(pop, 3)1 ns(pop, 3)2 ns(pop, 3)3 ns(dnce, 3)1\n1  ( 1 ) \" \"         \" \"         \" \"         \" \"         \" \"         \n2  ( 1 ) \" \"         \" \"         \" \"         \" \"         \" \"         \n3  ( 1 ) \" \"         \" \"         \" \"         \" \"         \" \"         \n         ns(dnce, 3)2 ns(dnce, 3)3 ns(live, 3)1 ns(live, 3)2\n1  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n2  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n3  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n         ns(live, 3)3 ns(spch, 3)1 ns(spch, 3)2 ns(spch, 3)3\n1  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n2  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n3  ( 1 ) \" \"          \" \"          \"*\"          \" \"         \n         ns(dur, 3)1 ns(dur, 3)2 ns(dur, 3)3\n1  ( 1 ) \" \"         \" \"         \" \"        \n2  ( 1 ) \" \"         \" \"         \" \"        \n3  ( 1 ) \" \"         \" \"         \" \"        \n\nplot(for_spline_mod)\n\n\n\nfor_spline_mod$bestTune\n\n\n  nvmax\n2     3\n\nfor_spline_mod$results\n\n\n  nvmax     RMSE  Rsquared      MAE    RMSESD RsquaredSD     MAESD\n1     2 12.86781 0.3498257 10.55154 0.9235936 0.10765745 0.6993457\n2     3 12.92971 0.3449085 10.51615 0.9716964 0.09693563 0.6348611\n3     4 12.69644 0.3685762 10.30346 0.8366573 0.10280846 0.6553263\n\nAccording to the Forward Stepwise Selection model with natural\nsplines, the top predictor for song energy level is acous.\nExamine LASSO model with natural splines output:\n\n\nsummary(LASSO_spline_mod)\n\n\n            Length Class      Mode     \na0            72   -none-     numeric  \nbeta        1728   dgCMatrix  S4       \ndf            72   -none-     numeric  \ndim            2   -none-     numeric  \nlambda        72   -none-     numeric  \ndev.ratio     72   -none-     numeric  \nnulldev        1   -none-     numeric  \nnpasses        1   -none-     numeric  \njerr           1   -none-     numeric  \noffset         1   -none-     logical  \ncall           5   -none-     call     \nnobs           1   -none-     numeric  \nlambdaOpt      1   -none-     numeric  \nxNames        24   -none-     character\nproblemType    1   -none-     character\ntuneValue      2   data.frame list     \nobsLevels      1   -none-     logical  \nparam          0   -none-     list     \n\nplot(LASSO_spline_mod)\n\n\n\nLASSO_spline_mod$bestTune\n\n\n  alpha    lambda\n6     1 0.5050505\n\n# LASSO_spline_mod$results\n\n\n\nThe lambda value provided by the LASSO model with splines is\n0.5050505.\nGAM with LOESS terms\nFit a GAM using LOESS terms using the set of variables deemed to be\nmost relevant based on your investigations so far.\nHow does test performance of the GAM compare to other models you\nexplored?\nDo you gain any insights from the GAM output plots for each\npredictor?\n\n\nset.seed(253)\ngam_mod <- train(\n    nrgy ~ acous + val + dB,\n    data = top_spotify_new,\n    method = \"gamLoess\",\n    tuneGrid = data.frame(degree = 1, span = seq(0.1, 0.9, by = 0.1)),\n    trControl = trainControl(method = \"cv\", number = 9, selectionFunction = \"best\"),\n    metric = \"MAE\",\n    na.action = na.omit\n)\n\n\n\nExamine GAM with LOESS output:\n\n\ngam_mod$results[3,]\n\n\n  degree span     RMSE  Rsquared      MAE   RMSESD RsquaredSD\n3      1  0.3 10.20914 0.5906928 8.252745 0.892152  0.1020878\n      MAESD\n3 0.8103724\n\n\n\nplot(gam_mod)\n\n\n\n#Metrics for the best model \ngam_mod$results %>%\n    filter(span==gam_mod$bestTune$span)\n\n\n  degree span     RMSE  Rsquared      MAE   RMSESD RsquaredSD\n1      1  0.3 10.20914 0.5906928 8.252745 0.892152  0.1020878\n      MAESD\n1 0.8103724\n\n#Graphing Each Predictor \npar(mfrow = c(3,4)) # Sets up a grid of plots\nplot(gam_mod$finalModel, se = TRUE) # Dashed lines are +/- 2 SEs\n\n\n\n\nGAM with a span of 0.3 offers a MAE of 8.252745, indicating that we\nour predictions for top song energy level would be off by 8.368585\npercentage points in this case. This result is actually better than all\nfour previous models fitted in the first section.\nSummarize investigations\nDecide on an overall best model based on your investigations so far.\nTo do this, make clear your analysis goals. Predictive accuracy?\nInterpretability? A combination of both?\nOverall, based on the output given by all of the models we fitted\nabove, it seems that a GAM with LOESS model achieves the lowest MAE for\nour dataset. For our analysis, since we want to correctly predict the\nenergy level of a popular song, we care about the predictive accuracy of\nthe model. We are also interested in knowing what contributes to an\nenergetic song, thus interpretability is also essential for the model.\nTherefore splines doesn’t seem the most straightforward choice for us,\nwhereas either GAM with LOESS or LASSO seems like a better option.\nSocietal impact\nAre there any harms that may come from your analyses and/or how the\ndata were collected? What cautions do you want to keep in mind when\ncommunicating your work?\nOur models takes a harmless look at the deciding elements of an\nenergetic song, as under the environment of a global pandemic where\nsocial interactions are limited, it is important to look for means to\nmaintain a positive mood, and it seems that listening to uplifting pop\nmusic is a favorable way to do so. Given our dataset, though, since the\nsource is Spotify and Billboard, our scope of pop music is limited and\nmay result in a certain pattern in our predictions. We want to caution\nthat good music choice should by no means be limited, and it should\nalways be optimal to listen to whatever one’s heart desires.\nClassification analysis\n(Methods)\nWe used logistic regression and random forest for building\nclassification models.\nLogistic Regression\nWe converted the predictor “pop” to categorical, assigning the\nobservations with value above 75 to be top songs.\n\n\ntop_spotify_new$IsPop <- \"NO\"\ntop_spotify_new$IsPop[top_spotify_new$pop >= 75] <- \"YES\"\ntable(top_spotify_new$IsPop)\ntable(top_spotify_new$pop)\ntop_spotify_new$IsPop <- factor(top_spotify_new$IsPop)\n\n\n\nWe then fit the logistic regression model predicting whether a given\nsong is a popular song with all other predictors. We selected the\nmetrics Accuracy so that the model we fit would prioritize making the\nmost accurate predictions.\n\n\nset.seed(253)\nlogistic_mod <- train(\n    IsPop ~ .-pop,\n    data = top_spotify_new,\n    method = \"glm\",\n    family = \"binomial\",\n    trControl = trainControl(method = \"cv\", number = 10),\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\nsummary(logistic_mod$results)\n\n\n  parameter            Accuracy          Kappa       \n Length:1           Min.   :0.7124   Min.   :0.2109  \n Class :character   1st Qu.:0.7124   1st Qu.:0.2109  \n Mode  :character   Median :0.7124   Median :0.2109  \n                    Mean   :0.7124   Mean   :0.2109  \n                    3rd Qu.:0.7124   3rd Qu.:0.2109  \n                    Max.   :0.7124   Max.   :0.2109  \n   AccuracySD         KappaSD       \n Min.   :0.02915   Min.   :0.07924  \n 1st Qu.:0.02915   1st Qu.:0.07924  \n Median :0.02915   Median :0.07924  \n Mean   :0.02915   Mean   :0.07924  \n 3rd Qu.:0.02915   3rd Qu.:0.07924  \n Max.   :0.02915   Max.   :0.07924  \n\ncoefficients(logistic_mod$finalModel) %>% exp()\n\n\n  (Intercept)          year           bpm          nrgy          dnce \n1.508016e-192  1.246311e+00  1.000621e+00  9.717299e-01  1.007890e+00 \n           dB          live           val           dur         acous \n 1.147383e+00  9.915256e-01  1.006670e+00  9.968091e-01  1.001887e+00 \n         spch \n 9.881283e-01 \n\nWe also fit the LASSO logistic regression, gaining insight about\nvariable importance.\n\n\ntwoClassSummaryCustom <- function (data, lev = NULL, model = NULL) {\n    if (length(lev) > 2) {\n        stop(paste(\"Your outcome has\", length(lev), \"levels. The twoClassSummary() function isn't appropriate.\"))\n    }\n    caret:::requireNamespaceQuietStop(\"pROC\")\n    if (!all(levels(data[, \"pred\"]) == lev)) {\n        stop(\"levels of observed and predicted data do not match\")\n    }\n    rocObject <- try(pROC::roc(data$obs, data[, lev[1]], direction = \">\", \n        quiet = TRUE), silent = TRUE)\n    rocAUC <- if (inherits(rocObject, \"try-error\")) \n        NA\n    else rocObject$auc\n    out <- c(rocAUC, sensitivity(data[, \"pred\"], data[, \"obs\"], \n        lev[1]), specificity(data[, \"pred\"], data[, \"obs\"], lev[2]))\n    out2 <- postResample(data[, \"pred\"], data[, \"obs\"])\n    out <- c(out, out2[1])\n    names(out) <- c(\"AUC\", \"Sens\", \"Spec\", \"Accuracy\")\n    out\n}\nset.seed(253)\nlasso_logistic_mod <- train(\n    IsPop ~ .-pop,\n    data = top_spotify_new,\n    method = \"glmnet\",\n    family = \"binomial\",\n    tuneGrid = data.frame(alpha = 1, lambda = seq(0, 1, length.out = 100)),\n    trControl = trainControl(method = \"cv\", number = 10, selectionFunction = \"oneSE\", classProbs = TRUE, summaryFunction = twoClassSummaryCustom),\n    metric = \"AUC\",\n    na.action = na.omit\n)\n\nplot(lasso_logistic_mod)\n\n\n\n\n\n\nlasso_logistic_mod$bestTune\nlasso_logistic_mod$results\nlasso_logistic_mod$results %>%\n    filter(lambda==lasso_logistic_mod$bestTune$lambda)\nplot(lasso_logistic_mod$finalModel, xvar = \"lambda\", label = TRUE, col = rainbow(20), ylim = c(-0.5,7))\n\nrownames(lasso_logistic_mod$finalModel$beta)[c(5,3,1)]\n\n\n\nTrees and Random Forest\nWe fit trees and random forest to make predictions as well, using all\nother predictors to predict whether an observation is a popular song or\nnot. The metrics we selected is Accuracy, so that the model would\nprioritize making accurate predictions.\n\n\nset.seed(253)\ntree_mod <- train(\n    IsPop ~ .-pop,\n    data = top_spotify_new,\n    method = \"rpart\",\n    tuneGrid = data.frame(cp = seq(0, 0.5, length.out = 50)),\n    trControl = trainControl(method = \"cv\", number = 10, selectionFunction = \"oneSE\"),\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\n\nplot(tree_mod)\n\n\n\ntree_mod$results %>%\n    filter(cp==tree_mod$bestTune$cp)\n\n\n         cp  Accuracy    Kappa AccuracySD KappaSD\n1 0.1020408 0.7143012 0.181339 0.03461561 0.13422\n\n\n\nrf_mod <- train(\n    IsPop ~ .-pop,\n    data = top_spotify_new,\n    method = \"rf\",\n    tuneGrid = data.frame(mtry = c(2,3,4,5,6,7,8)),\n    trControl = trainControl(method = \"oob\", selectionFunction = \"best\"),\n    metric = \"Accuracy\",\n    ntree = 750, # To force fitting 1000 trees (can help with stability of results)\n    na.action = na.omit\n)\nplot(rf_mod)\n\n\n\nrf_mod$results\n\n\n   Accuracy     Kappa mtry\n1 0.7524917 0.3347524    2\n2 0.7425249 0.3147939    3\n3 0.7342193 0.3007419    4\n4 0.7392027 0.3082283    5\n5 0.7342193 0.2961637    6\n6 0.7358804 0.2947911    7\n7 0.7375415 0.3026788    8\n\nrf_mod$finalModel\n\n\n\nCall:\n randomForest(x = x, y = y, ntree = 750, mtry = min(param$mtry,      ncol(x))) \n               Type of random forest: classification\n                     Number of trees: 750\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 26.08%\nConfusion matrix:\n     NO YES class.error\nNO  384  29  0.07021792\nYES 128  61  0.67724868\n\nOur model is better at identifying songs that are not popular. In our\ncontext, it helps us avoid bad songs, which is preferable than the more\nlenient alternative which is more likely to falsely categorize a song as\npopular.\n\n\nvar_imp_rf <- randomForest::importance(rf_mod$finalModel)\n\n# Sort by importance with dplyr's arrange()\nvar_imp_rf <- data.frame(\n        predictor = rownames(var_imp_rf),\n        MeanDecreaseGini = var_imp_rf[,\"MeanDecreaseGini\"]\n    ) %>%\n    arrange(desc(MeanDecreaseGini))\n\n# Top 10\nhead(var_imp_rf, 10)\n\n\n      predictor MeanDecreaseGini\nyear       year         34.01757\nval         val         29.91566\ndur         dur         29.62221\nnrgy       nrgy         29.08262\nbpm         bpm         27.71691\ndnce       dnce         26.28274\nacous     acous         24.00443\nlive       live         23.57061\nspch       spch         18.39002\ndB           dB         15.18356\n\nIt seems that the most important predictor, given contributions to\ndecreasing the Gini index, is year, which is pretty interesting. This\ntells us that knowing what year the song is released would offer us much\ninsight into whether the song is likely to be popular.\nClassification\nAnalysis (Results- Variable Importance)\nFor our logistic regression model, we utilized a LASSO logistic\nregression to gain insight on variable importance. The results\ndemonstrate that dnce, and bpm are the most important variables for\npredicting the popularity of a song. These results are sensible because\nit is plausible that more upbeat songs that you can dance to will be\nvaluable traits that may lead a song to be more popular.\nIn our random forest model, it shows that year is by far the most\nimportant variable for predicting song popularity, which also\ncorresponds to the most important variable as determined by the variable\nimportance measure of a single decision tree. This is because it lowers\nto Gini index the most on average for all the trees. Year is not very\ninsightful, though, because it is possible that the popular songs\nfeatured in this dataset came more from particular years than others. It\ndoesn’t really help us predict the future popularity of a song. More\ninterestingly, the energy displayed by a song has the second most\nmeaningful mean decrease in the Gini index. Again, this is sensible\nbecause the goal of a song often times is to portray energy to its\nlistener, so it makes sense that songs that accomplish this goal would\nbe more popular. 22\nClassification analysis\n(Summary)\nCompare models\nCompare the 2 different classification models tried in light of\nevaluation metrics, variable importance, and data context:\nWe have compared a logistic regression and a decision tree model. To\ncomplement the models, we have ran a LASSO logistic regression and a\nrandom forest. We are trying to predict whether a song will be\nrelatively popular. We have created our own binary variable with a\nthreshold of > 75 in pop to be considered popular (IsPop = YES). In\nall models, the most important variable seemed to be year. In this\ncontext, songs released in a certain year seem to be the most popular.\nOther important variables were energy levels and dance-ibility, both of\nwhich intuitively make sense as they would be more commonly enjoyed\namong music listeners.\nEvaluation metrics\nDisplay evaluation metrics for different models in a clean, organized\nway. This display should include both the estimated metric as well as\nits standard deviation. (This won’t be available from OOB error\nestimation. If using OOB, don’t worry about reporting the SD.):\nLogistic Regression Accuracy: 0.7124414 Logistic Regression Accuracy\nSD: 0.0291491\nLasso Logistic Regression Accuracy: 0.6860489 Lasso Logistic\nRegression Accuracy SD: 0.003961554 Lasso Logistic Regression AUC:\n0.6715804 Lasso Logistic Regression AUC SD: 0.07519223\nDecision Tree Accuracy: 0.7143012 Decision Tree Accuracy SD:\n0.03461561\nRandom Forest Accuracy: 0.7524917 Random Forest Confusion Matrix:\nPREDICTED\n  NO YES class.error\nNO 380 33 0.07990315 YES 122 67 0.64550265\nNIR: (413)/(413+189) = 68.6% The NIR is calculated using the whole\ntraining set.\nBroadly summarize conclusions from looking at these evaluation\nmetrics and their measures of uncertainty:\nWe can see that the model that gives us the least amount of variance\nis lasso logistic regression. However, we note that we are not trying to\nbuild the model with the smallest variance, as the variance is just a\nway to look at the uncertainty of in estimation of test performance,\nwhich is not so high when using lasso logistic regression. Random Forest\nseems to have the highest accuracy. With an OOB estimate error rate of\n25.75%, this is reflective of our accuracy.\nOverall most preferable\nmodel\nThe overall most preferable model would be our random forest model.\nThe accuracy in this model far outweighs all other models at an accuracy\nof 75.25% Random forests also provide out of bag error estimations,\nwhich give us an accountable measurement of error in our model.\nInterpretion of evaluation metric(s) for the final model in context.\nDoes the model show an acceptable amount of error:\nWith an overall accuracy of 75.2%, and a NIR of 68.6%, we believe\nthis model shows an acceptable amount of error.\nIf using OOB error estimation, display the test (OOB) confusion\nmatrix, and use it to interpret the strengths and weaknesses of the\nfinal model:\nWe can see that we have a sensitivity of (67)/(67+112) = 37.43% and a\nspecificity of (380)/(380+33) = 92.01%. Our model is good at correctly\npredicting songs that won’t be as popular. However, our model is bad at\ncorrectly predicting songs that won’t be popular.\n\n\nPopularSongs <- top_spotify_new[top_spotify_new$pop >= 75, ]\ntable(PopularSongs$year)\n\n\n\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 \n   9   10   10   16   12   24   23   25   32   28 \n\ntable(top_spotify_new$year)\n\n\n\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 \n  51   53   35   71   58   95   79   65   64   31 \n\nWe can see that although years are fairly balanced, the years when\nlooking at popular songs seem to be skewed towards later years. This\nmeans makes sense as the more “popular” songs seem to be the more recent\nones.\n\n\n\n",
    "preview": "https://storage.googleapis.com/pr-newsroom-wp/1/2018/11/Spotify_Logo_CMYK_Green.png",
    "last_modified": "2022-05-23T14:36:38-07:00",
    "input_file": {}
  }
]
