[
  {
    "path": "posts/S&P_Bayes_Project/",
    "title": "Bayes Finance Project",
    "description": "Using Bayes Methods to predict future earnings!",
    "author": [],
    "date": "2021-12-11",
    "categories": [],
    "contents": "\nProject\nWelcome to our STAT 454: Bayesian Statistics capstone project. All of us group members have an interest in and connections to the financial world, whether that be through our majors or internships, which led us toward this topic. Financial information, like stock market prices, are known to be notoriously hard to predict. We wanted to take a Bayesian approach to try and tackle a similar situation: predicting the future earnings of S&P 500 companies. In this project we seek to model future earnings using other financial information about a company, like previous earnings and sales. We explore a few Bayesian hierarchical models, as well as a SARIMA model using the bayesforcast package to try and identify one that can provide insight and better predictions for future company’s earnings.\nProject Link\n\n\n\n",
    "preview": "https://wealthface.com/blog/wp-content/uploads/2021/05/SP500.jpg",
    "last_modified": "2021-12-16T12:51:47-06:00",
    "input_file": {}
  },
  {
    "path": "posts/YouTube_Viz/",
    "title": "YouTube Shiny App",
    "description": "Interactive visualization for youtube viewing data!",
    "author": [],
    "date": "2021-07-01",
    "categories": [],
    "contents": "\n\n\nlibrary(shiny)\nlibrary(tidyverse)\ndata <- read.csv(\"~/Documents/Intro To Data Science/USvideos.txt\")\ndata_cleaned<- data %>%\n  filter(category_id %in% c(\"1\", \"2\", \"10\", \"15\", \"17\", \"19\", \"22\", \"23\", \"24\", \"26\", \"27\", \"28\")) %>%\n  mutate(time_string = toString(publish_time)) %>%\n  mutate(day = substr(time_string, 9, 10)) %>%\n  mutate(likes = as.numeric(likes, na.rm = TRUE)) %>%\n  mutate(dislikes = as.numeric(dislikes, na.rm = TRUE))%>%\n  mutate(like_dislike_ratio = likes / (dislikes + 1))%>%\n  select(title, category_id, tags, views, likes, comment_count, dislikes, like_dislike_ratio)\n\ndata_cleaned <- data_cleaned %>%\n  mutate(category = ifelse(category_id == \"1\", \"Autos & Vehicles\", ifelse(category_id == \"2\", \"Music\", ifelse(category_id == \"10\", \"Comedy\", \n                                                                                                              ifelse(category_id == \"15\", \"Science & Technology\", ifelse(category_id == \"17\", \"Science & Technology\", \n                                                                                                                                                                         ifelse(category_id == \"17\", \"Movies\", ifelse(category_id == \"19\", \"Action/Adventure\", ifelse(category_id == \"22\", \"Documentary\",\n                                                                                                                                                                                                                                                                      ifelse(category_id == \"23\", \"Drama\", ifelse(category_id == \"24\", \"Family\", ifelse(category_id == \"26\", \"Horror\", \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ifelse(category_id == \"27\", \"Sci-Fi/Fantasy\", \"Thriller\")))))))))))))\ngraphdata <- data_cleaned %>%\n  mutate(category_id_c = as.character(category)) %>% \n  group_by(category_id_c) %>% \n  mutate(views = as.numeric(views, na.rm = TRUE)) %>%\n  mutate(comment_count = as.numeric(comment_count, na.rm = TRUE)) %>%\n  summarise(totallikes = sum(likes, na.rm = TRUE), totaldislikes = sum(dislikes, na.rm = TRUE), \n            totalviews = sum(views, na.rm = TRUE), totalcomment = sum(comment_count, na.rm = TRUE),\n            totalvideos = n(), likes_per_video = totallikes/totalvideos, \n            dislikes_per_video = totaldislikes/ totalvideos, views_per_video = totalviews/totalvideos,\n            comment_count_per_video = totalcomment/totalvideos, like_dislike_ratio_per_video = mean(like_dislike_ratio, na.rm = TRUE))\n\nui <- fluidPage(\n  \n  title = \"Youtube Recommendation\",\n  \n  sidebarLayout(\n    sidebarPanel(\n      conditionalPanel(\n        'input.dataset === \"Users Manual\"',\n        helpText(\"Here is the user's manual\")\n      ),\n      conditionalPanel(\n        'input.dataset === \"Likes\"',\n        selectInput(\"category1\",\n                    \"Category\",\n                    choices = list(\"Autos & Vehicles\" = \"1\", \"Music\" = \"2\", \"Comedy\" = \"10\", \"Science & Technology\" = \"15\", \"Movies\" = \"17\", \"Action/Adventure\" = \"19\", \n                                   \"Documentary\" = \"22\", \"Drama\" = \"23\", \"Family\" = \"24\", \"Horror\" = \"26\", \"Sci-Fi/Fantasy\" = \"27\", \"Thriller\" = \"28\")),\n        checkboxGroupInput(\"show_vars1\",\"Columns to show:\", \n                           list(\"Title\" = \"title\", \"Views\" = \"views\", \"Likes\" = \"likes\", \n                                \"Comment Count\" = \"comment_count\", \"Dislikes\" = \"dislikes\", \"Like Dislike Ratio\" = \"like_dislike_ratio\"), \n                           selected = list(\"title\", \"likes\"))\n      ),\n      conditionalPanel(\n        'input.dataset === \"Dislikes\"',\n        selectInput(\"category2\",\n                    \"Category\",\n                    choices = list(\"Autos & Vehicles\" = \"1\", \"Music\" = \"2\", \"Comedy\" = \"10\", \"Science & Technology\" = \"15\", \"Movies\" = \"17\", \"Action/Adventure\" = \"19\", \n                                   \"Documentary\" = \"22\", \"Drama\" = \"23\", \"Family\" = \"24\", \"Horror\" = \"26\", \"Sci-Fi/Fantasy\" = \"27\", \"Thriller\" = \"28\")),\n        checkboxGroupInput(\"show_vars2\",\"Columns to show:\", \n                          list(\"Title\" = \"title\", \"Views\" = \"views\", \"Likes\" = \"likes\", \n                               \"Comment Count\" = \"comment_count\", \"Dislikes\" = \"dislikes\", \"Like Dislike Ratio\" = \"like_dislike_ratio\"), \n                          selected = list(\"title\", \"dislikes\"))\n      ),\n      conditionalPanel(\n        'input.dataset === \"Comment Count\"',\n        selectInput(\"category3\",\n                    \"Category\",\n                    choices = list(\"Autos & Vehicles\" = \"1\", \"Music\" = \"2\", \"Comedy\" = \"10\", \"Science & Technology\" = \"15\", \"Movies\" = \"17\", \"Action/Adventure\" = \"19\", \n                                   \"Documentary\" = \"22\", \"Drama\" = \"23\", \"Family\" = \"24\", \"Horror\" = \"26\", \"Sci-Fi/Fantasy\" = \"27\", \"Thriller\" = \"28\")),\n        checkboxGroupInput(\"show_vars3\",\"Columns to show:\", \n                           list(\"Title\" = \"title\", \"Views\" = \"views\", \"Likes\" = \"likes\", \n                                \"Comment Count\" = \"comment_count\", \"Dislikes\" = \"dislikes\", \"Like Dislike Ratio\" = \"like_dislike_ratio\"), \n                           selected = list(\"title\", \"comment_count\"))\n      ),\n      conditionalPanel(\n        'input.dataset === \"Views\"',\n        selectInput(\"category4\",\n                    \"Category\",\n                    choices = list(\"Autos & Vehicles\" = \"1\", \"Music\" = \"2\", \"Comedy\" = \"10\", \"Science & Technology\" = \"15\", \"Movies\" = \"17\", \"Action/Adventure\" = \"19\", \n                                   \"Documentary\" = \"22\", \"Drama\" = \"23\", \"Family\" = \"24\", \"Horror\" = \"26\", \"Sci-Fi/Fantasy\" = \"27\", \"Thriller\" = \"28\")),\n        checkboxGroupInput(\"show_vars4\",\"Columns to show:\", \n                           list(\"Title\" = \"title\", \"Views\" = \"views\", \"Likes\" = \"likes\", \n                                \"Comment Count\" = \"comment_count\", \"Dislikes\" = \"dislikes\", \"Like Dislike Ratio\" = \"like_dislike_ratio\"), \n                           selected = list(\"title\", \"views\"))\n      ),\n      conditionalPanel(\n        'input.dataset === \"Like Dislike Ratio\"',\n        selectInput(\"category5\",\n                    \"Category\",\n                    choices = list(\"Autos & Vehicles\" = \"1\", \"Music\" = \"2\", \"Comedy\" = \"10\", \"Science & Technology\" = \"15\", \"Movies\" = \"17\", \"Action/Adventure\" = \"19\", \n                                   \"Documentary\" = \"22\", \"Drama\" = \"23\", \"Family\" = \"24\", \"Horror\" = \"26\", \"Sci-Fi/Fantasy\" = \"27\", \"Thriller\" = \"28\")),\n        checkboxGroupInput(\"show_vars5\",\"Columns to show:\", \n                           list(\"Title\" = \"title\", \"Views\" = \"views\", \"Likes\" = \"likes\", \n                                \"Comment Count\" = \"comment_count\", \"Dislikes\" = \"dislikes\", \"Like Dislike Ratio\" = \"like_dislike_ratio\"), \n                           selected = list(\"title\", \"like_dislike_ratio\"))\n      ),\n      conditionalPanel(\n        'input.dataset === \"Ranking Plot\"',\n        selectInput(\"Vars\",\n                    \"Ranking Categories of Video according to:\",\n                    choices = list(\"Views Per Video\" = \"views\", \"Likes Per Video\" = \"likes\",\n                                   \"Comment Count Per Video\" = \"comment_count\", \"Dislikes Per Video\" = \"dislikes\", \"Average Like Dislike Ratio\" = \"like_dislike_ratio\"))\n      ),\n      \n      conditionalPanel(\n        'input.dataset === \"Distribution of Likes by Categories\"',\n        checkboxGroupInput(\"Categoreis\",\"Categories to be included in the Visulization\",\n                           list(\"Autos & Vehicles\" = \"Autos & Vehicles\", \"Music\" = \"Music\", \"Comedy\" = \"Comedy\", \"Science & Technology\" = \"Science & Technology\", \"Movies\" = \"Movies\", \"Action/Adventure\" = \"Action/Adventure\",\n                                \"Documentary\" = \"Documentary\", \"Drama\" = \"Drama\", \"Family\" = \"Family\", \"Horror\" = \"Horror\", \"Sci-Fi/Fantasy\" = \"Sci-Fi/Fantasy\", \"Thriller\" = \"Thriller\"),\n                           selected = (\"Autos & Vehicles\")\n        )\n      )\n    ),\n    \n    mainPanel(\n      tabsetPanel(\n        id = 'dataset',\n        tabPanel(\"Users Manual\", tags$h2(\"User's Manual\"), htmlOutput(\"manual\")),\n        tabPanel(\"Likes\", DT::dataTableOutput(\"mytable_likes\")),\n        tabPanel(\"Dislikes\", DT::dataTableOutput(\"mytable_dislikes\")),\n        tabPanel(\"Comment Count\", DT::dataTableOutput(\"mytable_comment_count\")),\n        tabPanel(\"Views\", DT::dataTableOutput(\"mytable_views\")),\n        tabPanel(\"Like Dislike Ratio\", DT::dataTableOutput(\"mytable_like_dislike_ratio\")),\n        tabPanel(\"Ranking Plot\", plotOutput(outputId = \"Rankplot\")),\n        tabPanel(\"Distribution of Likes by Categories\", plotOutput(outputId = \"Boxplot\"))\n        \n      )\n    )\n  )\n)\n\n\n\nserver <- function(input, output) {\n    \n    output$mytable_likes <- DT::renderDataTable({\n      data_cleaned %>%\n        mutate(likes = as.numeric(likes)) %>%\n        filter(category_id == input$category1) %>%\n        arrange(desc(likes)) %>%\n        select(input$show_vars1)\n    })\n    \n    output$mytable_dislikes <- DT::renderDataTable({\n      data_cleaned %>%\n        mutate(dislikes = as.numeric(dislikes)) %>%\n        filter(category_id == input$category2) %>%\n        arrange(dislikes) %>%\n        select(input$show_vars2)\n    })\n    \n    output$mytable_comment_count <- DT::renderDataTable({\n      data_cleaned %>%\n        mutate(comment_count = as.numeric(comment_count)) %>%\n        filter(category_id == input$category3) %>%\n        arrange(desc(comment_count)) %>%\n        select(input$show_vars3)\n    })\n    \n    output$mytable_views <- DT::renderDataTable({\n      data_cleaned %>%\n        mutate(views = as.numeric(views)) %>%\n        filter(category_id == input$category4) %>%\n        arrange(desc(views)) %>%\n        select(input$show_vars4)\n    })\n    \n    output$mytable_like_dislike_ratio <- DT::renderDataTable({\n      data_cleaned %>%\n        mutate(like_dislike_ratio = as.numeric(like_dislike_ratio))%>%\n        filter(category_id == input$category5)%>%\n        arrange(desc(like_dislike_ratio)) %>%\n        select(input$show_vars5)\n    })\n    \n    output$manual <- renderText({\n      paste(\n        \"<p>Welcome to our shiny app! This is a fairly self-explanatory app. First off, start off my selecting the categories in which you would like to browse the data in. To select a category, clock on the drop-down box and scroll up or down until you see what you want to explore. Simply click on the option you’d like. We then select what results we would like to view. Check or uncheck what columns we would like to show in the results.<\/p>\n         <p> Then, we will be able to rank the videos by Likes, Dislikes, Comment Count, Views, and Like-Dislike Ratio. By selecting one, the app will bring you to a page where you can search by keywords and sort by likes and alphabetical order of the title. You may also choose to see how many entries you view per page. To go to the next page, click on the numbers or next.<\/p>\n        \"\n      )\n    })\n    \n    output$Rankplot <- renderPlot(\n      graphdata %>%\n        ggplot(aes(y = fct_reorder(category_id_c,\n                                   eval(as.name(paste(input$Vars,\"_per_video\", sep=\"\")))\n                                   ),\n                   x = eval(as.name(paste(input$Vars, \"_per_video\", sep=\"\"))), \n                   fill = category_id_c)) +\n        geom_bar(stat = 'identity') +\n        labs(y = \"Category of Music\",\n             title = paste(\"Ranking of Category by\",input$Vars),\n             x = \"\"\n             )+\n        theme(legend.position = \"none\")\n    )\n\n    output$Boxplot <- renderPlot(\n      data_cleaned %>%\n        filter(category == input$Categoreis) %>%\n        mutate(category_c = as.character(category)) %>%\n        group_by(category_c) %>%\n        summarize(category_c, likes) %>%\n        ggplot(aes(y = reorder(category_c,likes,median), x=likes, fill = category_c)) +\n        geom_boxplot()+\n        labs(y = \"Category ID\",\n             x = \"Likes\",\n             title = \"Distribution by Category\")+\n        theme_minimal()+\n        xlim(0,100000)+\n        theme(legend.position = \"none\")\n    )\n    \n}\n\nshinyApp(ui = ui, server = server)\n\n\nShiny applications not supported in static R Markdown documents\n\n\n\n\n",
    "preview": "https://cdn.mos.cms.futurecdn.net/8gzcr6RpGStvZFA2qRt4v6.jpg",
    "last_modified": "2021-12-01T23:47:04-06:00",
    "input_file": {}
  },
  {
    "path": "posts/Spotify_Project/",
    "title": "Machine Learning Semester Project",
    "description": "We analyzed data from Spotify with the goal of predicting song popularity!",
    "author": [],
    "date": "2020-05-01",
    "categories": [],
    "contents": "\nProject Work\nData Context\nWe selected the dataset “Top Spotify songs from 2010-2019 - BY YEAR” from Kaggle, which consists of the top songs by year in the world on Spotify, and the data is based on Billboard. The dataset contains 13 variables to be explored, including information about the songs such as the song’s title, the song’s artist, the genre of the track, the year in which the song was in the Billboard rankings, its duration and acousticness. There are also variables describing the music, such as beats per minute (which characterizes the tempo of the song), energy (the higher the value, the more energetic the song is), danceability (the higher the value, the easier it is to dance to this song), loudness (measured in dB), liveness (the higher the value, the more likely the song is a live recording), valence (the higher the value, the more positive the mood is for the song), and speechiness (the higher the value, the more spoken words the song contains). The outcome variable in this dataset is popularity, where the higher the value, the more popular the song is.\nThe data were extracted from: http://organizeyourmusic.playlistmachinery.com/, and the dataset was constructed by Leonardo Henrique, updated in 2020. He was interested in what we could know about the specific music genre based on the popularity of the songs, and what elements would contribute to this popularity.\nResearch questions\nRegression: How can we predict the energy level of a song based on all other predictors? Classification: How can we predict whether the song is amongst the most popular based on all other predictors?\n\n\n# Load data and required packages\nlibrary(caret)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ISLR)\nlibrary(splines)\nlibrary(caret)\nlibrary(stats)\nlibrary(lattice)\nlibrary(leaps)\nlibrary(gam)\ntop_spotify <- read_csv('https://www.dropbox.com/s/fi22whryueo4q85/top10s.csv?dl=1')\n\n\n\n\n\n# Any code to clean the data\ntop_spotify_new <- top_spotify %>% select(-artist,-'top genre',-title,-...1)\n#There seems to be an outlier \ntop_spotify_new <- filter(top_spotify_new, bpm > 1)\n\n\n\nInitial investigation 1: ignoring nonlinearity (for now)\nUse ordinary least squares (OLS) regression, forward and/or backward selection, and LASSO to build initial models for your quantitative outcome as a function of the predictors of interest. (As part of data cleaning, exclude any variables that you don’t want to consider as predictors.)\nThese models should not include any transformations to deal with nonlinearity. You’ll explore this in the next investigation.\nNote: If you have highly collinear/redundant variables, you might see the message “Reordering variables and trying again” and associated warning()s about linear dependencies being found. Sometimes stepwise selection is able to handle the collinearity/redundancy by modifying the order of the variables tried. If collinearity/redundancy cannot be handled and causes an error, try reducing nvmax.\nOLS Model\nFit the ordinary least squares (OLS) regression model:\n\n\nOLS <- lm(nrgy ~ year + acous + bpm + pop + dnce + live + spch + dur, data = top_spotify_new)\nsummary(OLS)\n\n\n\nCall:\nlm(formula = nrgy ~ year + acous + bpm + pop + dnce + live + \n    spch + dur, data = top_spotify_new)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.223  -8.030   1.092   8.540  31.239 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.347e+03  4.228e+02   5.552 4.26e-08 ***\nyear        -1.119e+00  2.097e-01  -5.337 1.34e-07 ***\nacous       -4.123e-01  2.611e-02 -15.790  < 2e-16 ***\nbpm         -3.427e-03  2.178e-02  -0.157  0.87504    \npop         -4.448e-02  3.713e-02  -1.198  0.23131    \ndnce         1.833e-03  4.190e-02   0.044  0.96511    \nlive         1.239e-01  4.012e-02   3.087  0.00212 ** \nspch         2.046e-01  6.916e-02   2.958  0.00322 ** \ndur         -7.281e-02  1.568e-02  -4.644 4.20e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.51 on 593 degrees of freedom\nMultiple R-squared:  0.402, Adjusted R-squared:  0.3939 \nF-statistic: 49.83 on 8 and 593 DF,  p-value: < 2.2e-16\n\nset.seed(253)\nOLS_cv <- train(\n    nrgy ~ year + acous + bpm + pop + dnce + live + spch + dur,\n    data = top_spotify_new,\n    method = \"lm\",\n    trControl = trainControl(method = \"cv\", number = 9),\n    na.action = na.omit)\n\n\n\nBackward Stepwise Selection Model\nFit the Backward Stepwise Selection model:\n\n\nfull_model <- lm(nrgy ~ ., data = top_spotify_new)\nsummary(full_model)\n\n\n\nCall:\nlm(formula = nrgy ~ ., data = top_spotify_new)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-28.7877  -6.2817   0.6035   6.8994  27.0338 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 812.704006 344.864487   2.357 0.018769 *  \nyear         -0.350636   0.171100  -2.049 0.040874 *  \nbpm          -0.004972   0.017149  -0.290 0.771957    \ndnce         -0.106738   0.037418  -2.853 0.004488 ** \ndB            4.479083   0.268874  16.659  < 2e-16 ***\nlive          0.103313   0.031586   3.271 0.001135 ** \nval           0.113503   0.022958   4.944 9.99e-07 ***\ndur          -0.017890   0.012827  -1.395 0.163629    \nacous        -0.289294   0.021547 -13.426  < 2e-16 ***\nspch          0.206303   0.055593   3.711 0.000226 ***\npop          -0.074613   0.029247  -2.551 0.010988 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.84 on 591 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.625 \nF-statistic: 101.2 on 10 and 591 DF,  p-value: < 2.2e-16\n\nset.seed(253)\nback_step_mod <- train(\n    nrgy ~ .,\n    data = top_spotify_new,\n    method = 'leapBackward',\n    tuneGrid = data.frame(nvmax = 1:10),\n    trControl = trainControl(method = 'cv',number = 9),\n    metric = 'MAE',\n    na.action = na.omit\n)\n\n\n\nForward Stepwise Selection Model\nFit the Forward Stepwise Selection model:\n\n\nset.seed(253)\nfor_step_mod <- train(\n    nrgy ~ .,\n    data = top_spotify_new,\n    method = 'leapForward',\n    tuneGrid = data.frame(nvmax = 1:10),\n    trControl = trainControl(method = 'cv',number = 9),\n    metric = 'MAE',\n    na.action = na.omit\n)\n\n\n\nLASSO Model\nFit the LASSO model:\n\n\nset.seed(253)\nlasso_mod <- train(\n    nrgy ~ .,\n    data = top_spotify_new,\n    method = \"glmnet\",\n    tuneGrid = data.frame(alpha = 1, lambda = seq(0, 10, length.out = 100)),\n    trControl = trainControl(method = \"cv\", number = 9, selectionFunction = 'oneSE'),\n    metric = \"MAE\",\n    na.action = na.omit)\n\n\n\nCompare performances of different models:\nEstimate test performance of the models from these different methods. Report and interpret (with units) these estimates along with a measure of uncertainty in the estimate (SD is most readily available from caret).\nExamine OLS model outputs:\n\n\nsummary(OLS_cv)\n\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.223  -8.030   1.092   8.540  31.239 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.347e+03  4.228e+02   5.552 4.26e-08 ***\nyear        -1.119e+00  2.097e-01  -5.337 1.34e-07 ***\nacous       -4.123e-01  2.611e-02 -15.790  < 2e-16 ***\nbpm         -3.427e-03  2.178e-02  -0.157  0.87504    \npop         -4.448e-02  3.713e-02  -1.198  0.23131    \ndnce         1.833e-03  4.190e-02   0.044  0.96511    \nlive         1.239e-01  4.012e-02   3.087  0.00212 ** \nspch         2.046e-01  6.916e-02   2.958  0.00322 ** \ndur         -7.281e-02  1.568e-02  -4.644 4.20e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.51 on 593 degrees of freedom\nMultiple R-squared:  0.402, Adjusted R-squared:  0.3939 \nF-statistic: 49.83 on 8 and 593 DF,  p-value: < 2.2e-16\n\nOLS_cv$results\n\n\n  intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD   MAESD\n1      TRUE 12.62445 0.3880631 10.13206 1.140868  0.1381483 0.83748\n\nOn average, we’re off in top song energy predictions by about 10.13206 points.\nResidual plot for OLS model:\n\n\nOLS_mod_output <- broom::augment(OLS, newdata = top_spotify_new)\n\nggplot(OLS_mod_output, aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\")\n\n\n\n\nExamine Backward Stepwise Selection model output:\n\n\nsummary(back_step_mod)\n\n\nSubset selection object\n10 Variables  (and intercept)\n      Forced in Forced out\nyear      FALSE      FALSE\nbpm       FALSE      FALSE\ndnce      FALSE      FALSE\ndB        FALSE      FALSE\nlive      FALSE      FALSE\nval       FALSE      FALSE\ndur       FALSE      FALSE\nacous     FALSE      FALSE\nspch      FALSE      FALSE\npop       FALSE      FALSE\n1 subsets of each size up to 9\nSelection Algorithm: backward\n         year bpm dnce dB  live val dur acous spch pop\n1  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"   \" \"  \" \"\n2  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \"*\"   \" \"  \" \"\n3  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \"*\"   \"*\"  \" \"\n4  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \"*\" \" \" \"*\"   \"*\"  \" \"\n5  ( 1 ) \" \"  \" \" \" \"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \" \"\n6  ( 1 ) \" \"  \" \" \"*\"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \" \"\n7  ( 1 ) \" \"  \" \" \"*\"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \"*\"\n8  ( 1 ) \"*\"  \" \" \"*\"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \"*\"\n9  ( 1 ) \"*\"  \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\" \"*\"   \"*\"  \"*\"\n\nplot(back_step_mod)\n\n\n\nback_step_mod$bestTune\n\n\n  nvmax\n9     9\n\nWe chose the model with 6 predictors. Although all 10 yields better model metrics, we believe that we will run into problems of over-fitting.\nOn average, we’re off in top song energy predictions by about 8.286 percentage points, if we use the model with 6 predictors.\n\n\ncoef(back_step_mod$finalModel, id = 6)\n\n\n(Intercept)        dnce          dB        live         val \n 97.4672886  -0.1228588   4.5587752   0.1128596   0.1264183 \n      acous        spch \n -0.2930557   0.2006552 \n\nback_step_mod$results %>% filter(nvmax==6)\n\n\n  nvmax     RMSE  Rsquared      MAE    RMSESD RsquaredSD    MAESD\n1     6 10.18581 0.5910122 8.286481 0.9244708  0.1070956 0.825095\n\nback_step_mod_eq <- lm(nrgy ~ dnce + dB + live + val + acous + spch, data =top_spotify_new)\n\n\n\nResidual plot for Backward Step-wise Selection model:\n\n\nback_step_mod_out <- top_spotify_new %>%\n    mutate(\n        fitted = predict(back_step_mod_eq, newdata = top_spotify_new),\n        resid = nrgy - fitted\n    )\n\nggplot(back_step_mod_out, aes(x = fitted, y = resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\") +\n    theme_classic() +\n    labs(x = \"Fitted values\", y = \"Residuals\")\n\n\n\n\nFor Loop looking at all predictors in the Backward Stepwise Selection model:\n\n\npredictors <- setdiff(colnames(top_spotify_new), c(\"year\",\"bpm\",\"nrgy\",\"dur\",\"spch\"))\nfor (pred in predictors) {\n    p <- ggplot(back_step_mod_out, aes(x = .data[[pred]], y = resid)) +\n        geom_point() +\n        geom_smooth() +\n        geom_hline(yintercept = 0, color = \"red\") +\n        theme_classic() +\n        labs(x = pred, y = \"Residuals\")\n    print(p)\n}\n\n\n\n\nExamine Forward Stepwise Selection model output:\n\n\nsummary(for_step_mod)\n\n\nSubset selection object\n10 Variables  (and intercept)\n      Forced in Forced out\nyear      FALSE      FALSE\nbpm       FALSE      FALSE\ndnce      FALSE      FALSE\ndB        FALSE      FALSE\nlive      FALSE      FALSE\nval       FALSE      FALSE\ndur       FALSE      FALSE\nacous     FALSE      FALSE\nspch      FALSE      FALSE\npop       FALSE      FALSE\n1 subsets of each size up to 9\nSelection Algorithm: forward\n         year bpm dnce dB  live val dur acous spch pop\n1  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"   \" \"  \" \"\n2  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \"*\"   \" \"  \" \"\n3  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \"*\"   \"*\"  \" \"\n4  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \"*\" \" \" \"*\"   \"*\"  \" \"\n5  ( 1 ) \" \"  \" \" \" \"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \" \"\n6  ( 1 ) \" \"  \" \" \"*\"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \" \"\n7  ( 1 ) \" \"  \" \" \"*\"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \"*\"\n8  ( 1 ) \"*\"  \" \" \"*\"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \"*\"\n9  ( 1 ) \"*\"  \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\" \"*\"   \"*\"  \"*\"\n\nplot(for_step_mod)\n\n\n\nfor_step_mod$results\n\n\n   nvmax      RMSE  Rsquared      MAE    RMSESD RsquaredSD     MAESD\n1      1 11.993192 0.4404595 9.440092 1.1663410 0.08326330 0.7602086\n2      2 10.477550 0.5683678 8.533998 0.6892741 0.09392739 0.6832649\n3      3 10.421451 0.5699856 8.442041 0.8481219 0.10845490 0.7918324\n4      4 10.333804 0.5765861 8.412541 0.8391433 0.11095779 0.7986408\n5      5 10.260625 0.5846211 8.383712 0.8495003 0.11238655 0.7669223\n6      6 10.142928 0.5941810 8.277152 0.8950607 0.10739226 0.8186073\n7      7  9.950075 0.6082421 8.067531 0.9987578 0.11136222 0.8886527\n8      8  9.940663 0.6092391 8.072114 0.9905555 0.10712761 0.9066449\n9      9  9.902507 0.6120420 8.049632 0.9827727 0.10630862 0.9021970\n10    10  9.915936 0.6110377 8.062302 0.9829506 0.10702524 0.9087888\n\nUsing Forward selection, we chose the model with 5 predictors. The five predictors being acous, dB, live, spch, and val.\nOn average, we’re off in top song energy predictions by about 8.383712 percentage points.\n\n\ncoef(for_step_mod$finalModel, id = 5)\n\n\n(Intercept)          dB        live         val       acous \n91.50592402  4.65576857  0.11831750  0.09041016 -0.28031999 \n       spch \n 0.22151861 \n\nfor_step_mod$results %>% filter(nvmax==5)\n\n\n  nvmax     RMSE  Rsquared      MAE    RMSESD RsquaredSD     MAESD\n1     5 10.26063 0.5846211 8.383712 0.8495003  0.1123866 0.7669223\n\nfor_step_mod_eq <- lm(nrgy ~ acous + dB + val + spch + live, data =top_spotify_new)\n\n\n\nResidual plot for Forward Stepwise Selection model:\n\n\nfor_step_mod_out <- top_spotify_new %>%\n    mutate(\n        fitted = predict(for_step_mod_eq, newdata = top_spotify_new),\n        resid = nrgy - fitted\n    )\n\nggplot(for_step_mod_out, aes(x = fitted, y = resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\") +\n    theme_classic() +\n    labs(x = \"Fitted values\", y = \"Residuals\")\n\n\n\n\nFor Loop looking at all predictors in the Forward Stepwise Selection model:\n\n\npredictors <- setdiff(colnames(top_spotify_new), c(\"year\",\"bpm\",\"nrgy\",\"dur\",\"pop\", \"dnce\"))\nfor (pred in predictors) {\n    p <- ggplot(for_step_mod_out, aes(x = .data[[pred]], y = resid)) +\n        geom_point() +\n        geom_smooth() +\n        geom_hline(yintercept = 0, color = \"red\") +\n        theme_classic() +\n        labs(x = pred, y = \"Residuals\")\n    print(p)\n}\n\n\n\n\nExamine LASSO model output:\n\n\nplot(lasso_mod$finalModel, xvar = \"lambda\", label = TRUE, col = rainbow(20))\n\n\n\nlasso_mod$bestTune\n\n\n   alpha   lambda\n11     1 1.010101\n\n# lasso_mod$results\nrownames(lasso_mod$finalModel$beta)[c(4,8)]\n\n\n[1] \"dB\"    \"acous\"\n\nWe chose a lambda value of 1.010101, dB and acous seem to be two of the strongest/persistent predictors when it comes to energy-level.\n\n\ncoef(lasso_mod$finalModel, 1.010101)\n\n\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept) 427.22094252\nyear         -0.16551620\nbpm           .         \ndnce          .         \ndB            4.25384058\nlive          0.05257947\nval           0.06575686\ndur           .         \nacous        -0.25154488\nspch          0.10397394\npop          -0.02164891\n\nlasso_mod$results[11,]\n\n\n   alpha   lambda     RMSE Rsquared     MAE    RMSESD RsquaredSD\n11     1 1.010101 10.17634 0.597547 8.32769 0.8552498  0.1057273\n       MAESD\n11 0.7932597\n\nOn average, we’re off in top song energy predictions by about 8.32769 percentage points using LASSO with a lambda of 1.010101.\nResidual plot for LASSO model:\n\n\nlasso_mod_out <- top_spotify_new %>%\n    mutate(\n        fitted = predict(lasso_mod, newdata = top_spotify_new),\n        resid = nrgy - fitted)\nggplot(lasso_mod_out, aes(x = fitted, y = resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\") +\n    theme_classic() +\n    labs(x = \"Fitted values\", y = \"Residuals\")\n\n\n\n\nThere does not seem to be any noticable patterns of over and underpredicting here!\nFor Loop for LASSO model:\n\n\npredictors <- setdiff(colnames(top_spotify_new), \"Top Spotify\")\nfor (pred in predictors) {\n    p <- ggplot(lasso_mod_out, aes(x = .data[[pred]], y = resid)) +\n        geom_point() +\n        geom_smooth() +\n        geom_hline(yintercept = 0, color = \"red\") +\n        theme_classic() +\n        labs(x = pred, y = \"Residuals\")\n    print(p)\n}\n\n\n\n\nCompare estimated test performance across methods. Which method(s) might you prefer?\nModel\nTraining MAE\nMAESD\nOLS Model\n10.13206\n0.83748\nBackward\n8.286481\n0.825095\nForward\n8.383712\n0.7669223\nLASSO\n8.32769\n0.7932597\nComparing the four models, LASSO and Backward Stepwise Selection models seem to be yielding the best results as the predictions for top song energy level are closest to the test value.\nCompare insights from variable importance analyses from the different methods (stepwise and LASSO, but not OLS). Are there variables for which the methods reach consensus? What insights are expected? Surprising?\nAcross all models, the “top” 3 predictors for song energy level are acous (Acousticness- the higher the value the more acoustic the song is), dB(Loudness, the higher the value, the louder the song), and val(Valence, the higher the value, the more positive mood for the song). This is mostly consistent with our expectation, as when the song is louder, and more positive, the song has a higher energy level. It is a bit surprising that when a song is more acoustic, it is less energetic.\nNote that if some (but not all) of the indicator terms for a categorical predictor are selected in the final models, the whole predictor should be treated as selected.\nInvestigation 2: Accounting for nonlinearity\nUpdate your stepwise selection model(s) and LASSO model to use natural splines for the quantitative predictors.\nYou’ll need to update the model formula from y ~ . to something like y ~ cat_var1 + ns(quant_var1, df) + ....\nIt’s recommended to use few knots (e.g., 2 knots = 3 degrees of freedom).\nNote that ns(x,3) replaces x with 3 transformations of x. Keep this in mind when setting nvmax in stepwise selection.\n\n\nggplot(top_spotify_new, aes(x = val, y = nrgy)) +\n    geom_point(alpha = 0.25) +\n    geom_smooth(color = \"blue\", se = FALSE) +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\nggplot(top_spotify_new, aes(x = dB, y = nrgy)) +\n    geom_point(alpha = 0.25) +\n    geom_smooth(color = \"blue\", se = FALSE) +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\nggplot(top_spotify_new, aes(x = acous, y = nrgy)) +\n    geom_point(alpha = 0.25) +\n    geom_smooth(color = \"blue\", se = FALSE) +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\nggplot(top_spotify_new, aes(x = year, y = nrgy)) +\n    geom_point(alpha = 0.25) +\n    geom_smooth(color = \"blue\", se = FALSE) +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\nBackward Stepwise Selection model with natural splines\nUpdate the Backward Stepwise Selection model to use natural splines for the quantitative predictors:\n\n\nset.seed(253)\nback_spline_mod <- train(\n    nrgy ~ ns(acous, 3) + ns(pop, 3) + ns(dnce, 3) + ns(live, 3) + ns(dB, 3) + ns(val, 3),\n    data = top_spotify_new,\n    method = \"leapBackward\",\n    trControl = trainControl(method = \"cv\", number = 9, selectionFunction = \"oneSE\"),\n    metric = \"MAE\",\n    na.action = na.omit\n)\n\n\n\nForward Stepwise Selection model with natural splines\nUpdate the Forward Stepwise Selection model to use natural splines for the quantitative predictors:\n\n\nset.seed(253)\nfor_spline_mod <- train(\n    nrgy ~ ns(year, 3) + ns(acous, 3) + ns(bpm, 3) + ns(pop, 3) + ns(dnce, 3) + ns(live, 3) + ns(spch, 3) + ns(dur, 3),\n    data = top_spotify_new,\n    method = \"leapForward\",\n    trControl = trainControl(method = \"cv\", number = 9, selectionFunction = \"oneSE\"),\n    metric = \"MAE\",\n    na.action = na.omit\n)\n\n\n\nLASSO model with natural splines\nUpdate the LASSO model to use natural splines for the quantitative predictors:\n\n\nset.seed(253)\nLASSO_spline_mod <- train(\n    nrgy ~ ns(year, 3) + ns(acous, 3) + ns(bpm, 3) + ns(pop, 3) + ns(dnce, 3) + ns(live, 3) + ns(spch, 3) + ns(dur, 3),\n    data = top_spotify_new,\n    method = \"glmnet\",\n    tuneGrid = data.frame(alpha = 1, lambda = seq(0, 10, length.out = 100)),\n    trControl = trainControl(method = \"cv\", number = 9, selectionFunction = \"oneSE\"),\n    metric = \"MAE\",\n    na.action = na.omit\n)\n\n\n\nCompare insights from variable importance analyses\nCompare insights from variable importance analyses here and the corresponding results from Investigation 1. Now after having accounted for nonlinearity, have the most relevant predictors changed?\nNote that if some (but not all) of the spline terms are selected in the final models, the whole predictor should be treated as selected.\nExamine Backward Stepwise Selection model with natural splines output:\n\n\nsummary(back_spline_mod)\n\n\nSubset selection object\n18 Variables  (and intercept)\n              Forced in Forced out\nns(acous, 3)1     FALSE      FALSE\nns(acous, 3)2     FALSE      FALSE\nns(acous, 3)3     FALSE      FALSE\nns(pop, 3)1       FALSE      FALSE\nns(pop, 3)2       FALSE      FALSE\nns(pop, 3)3       FALSE      FALSE\nns(dnce, 3)1      FALSE      FALSE\nns(dnce, 3)2      FALSE      FALSE\nns(dnce, 3)3      FALSE      FALSE\nns(live, 3)1      FALSE      FALSE\nns(live, 3)2      FALSE      FALSE\nns(live, 3)3      FALSE      FALSE\nns(dB, 3)1        FALSE      FALSE\nns(dB, 3)2        FALSE      FALSE\nns(dB, 3)3        FALSE      FALSE\nns(val, 3)1       FALSE      FALSE\nns(val, 3)2       FALSE      FALSE\nns(val, 3)3       FALSE      FALSE\n1 subsets of each size up to 4\nSelection Algorithm: backward\n         ns(acous, 3)1 ns(acous, 3)2 ns(acous, 3)3 ns(pop, 3)1\n1  ( 1 ) \" \"           \" \"           \" \"           \" \"        \n2  ( 1 ) \" \"           \" \"           \"*\"           \" \"        \n3  ( 1 ) \" \"           \"*\"           \"*\"           \" \"        \n4  ( 1 ) \" \"           \"*\"           \"*\"           \" \"        \n         ns(pop, 3)2 ns(pop, 3)3 ns(dnce, 3)1 ns(dnce, 3)2\n1  ( 1 ) \" \"         \" \"         \" \"          \" \"         \n2  ( 1 ) \" \"         \" \"         \" \"          \" \"         \n3  ( 1 ) \" \"         \" \"         \" \"          \" \"         \n4  ( 1 ) \" \"         \" \"         \" \"          \" \"         \n         ns(dnce, 3)3 ns(live, 3)1 ns(live, 3)2 ns(live, 3)3\n1  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n2  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n3  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n4  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n         ns(dB, 3)1 ns(dB, 3)2 ns(dB, 3)3 ns(val, 3)1 ns(val, 3)2\n1  ( 1 ) \" \"        \" \"        \"*\"        \" \"         \" \"        \n2  ( 1 ) \" \"        \" \"        \"*\"        \" \"         \" \"        \n3  ( 1 ) \" \"        \" \"        \"*\"        \" \"         \" \"        \n4  ( 1 ) \"*\"        \" \"        \"*\"        \" \"         \" \"        \n         ns(val, 3)3\n1  ( 1 ) \" \"        \n2  ( 1 ) \" \"        \n3  ( 1 ) \" \"        \n4  ( 1 ) \" \"        \n\nplot(back_spline_mod)\n\n\n\nback_spline_mod$bestTune\n\n\n  nvmax\n3     4\n\nback_spline_mod$results\n\n\n  nvmax     RMSE  Rsquared       MAE    RMSESD RsquaredSD     MAESD\n1     2 12.94073 0.3520225 10.551403 1.0723596 0.09809830 0.7221742\n2     3 11.21314 0.5103530  9.152245 0.9208166 0.07104034 0.7258275\n3     4 10.28440 0.5900269  8.392090 0.8083434 0.09138549 0.7327469\n\nAccording to the Backward Stepwise Selection model with natural splines, the top predictors for song energy level are acous and dB.\nExamine Forward Stepwise Selection model output:\n\n\nsummary(for_spline_mod)\n\n\nSubset selection object\n24 Variables  (and intercept)\n              Forced in Forced out\nns(year, 3)1      FALSE      FALSE\nns(year, 3)2      FALSE      FALSE\nns(year, 3)3      FALSE      FALSE\nns(acous, 3)1     FALSE      FALSE\nns(acous, 3)2     FALSE      FALSE\nns(acous, 3)3     FALSE      FALSE\nns(bpm, 3)1       FALSE      FALSE\nns(bpm, 3)2       FALSE      FALSE\nns(bpm, 3)3       FALSE      FALSE\nns(pop, 3)1       FALSE      FALSE\nns(pop, 3)2       FALSE      FALSE\nns(pop, 3)3       FALSE      FALSE\nns(dnce, 3)1      FALSE      FALSE\nns(dnce, 3)2      FALSE      FALSE\nns(dnce, 3)3      FALSE      FALSE\nns(live, 3)1      FALSE      FALSE\nns(live, 3)2      FALSE      FALSE\nns(live, 3)3      FALSE      FALSE\nns(spch, 3)1      FALSE      FALSE\nns(spch, 3)2      FALSE      FALSE\nns(spch, 3)3      FALSE      FALSE\nns(dur, 3)1       FALSE      FALSE\nns(dur, 3)2       FALSE      FALSE\nns(dur, 3)3       FALSE      FALSE\n1 subsets of each size up to 3\nSelection Algorithm: forward\n         ns(year, 3)1 ns(year, 3)2 ns(year, 3)3 ns(acous, 3)1\n1  ( 1 ) \" \"          \" \"          \" \"          \" \"          \n2  ( 1 ) \" \"          \" \"          \" \"          \" \"          \n3  ( 1 ) \" \"          \" \"          \" \"          \" \"          \n         ns(acous, 3)2 ns(acous, 3)3 ns(bpm, 3)1 ns(bpm, 3)2\n1  ( 1 ) \" \"           \"*\"           \" \"         \" \"        \n2  ( 1 ) \"*\"           \"*\"           \" \"         \" \"        \n3  ( 1 ) \"*\"           \"*\"           \" \"         \" \"        \n         ns(bpm, 3)3 ns(pop, 3)1 ns(pop, 3)2 ns(pop, 3)3 ns(dnce, 3)1\n1  ( 1 ) \" \"         \" \"         \" \"         \" \"         \" \"         \n2  ( 1 ) \" \"         \" \"         \" \"         \" \"         \" \"         \n3  ( 1 ) \" \"         \" \"         \" \"         \" \"         \" \"         \n         ns(dnce, 3)2 ns(dnce, 3)3 ns(live, 3)1 ns(live, 3)2\n1  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n2  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n3  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n         ns(live, 3)3 ns(spch, 3)1 ns(spch, 3)2 ns(spch, 3)3\n1  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n2  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n3  ( 1 ) \" \"          \" \"          \"*\"          \" \"         \n         ns(dur, 3)1 ns(dur, 3)2 ns(dur, 3)3\n1  ( 1 ) \" \"         \" \"         \" \"        \n2  ( 1 ) \" \"         \" \"         \" \"        \n3  ( 1 ) \" \"         \" \"         \" \"        \n\nplot(for_spline_mod)\n\n\n\nfor_spline_mod$bestTune\n\n\n  nvmax\n2     3\n\nfor_spline_mod$results\n\n\n  nvmax     RMSE  Rsquared      MAE    RMSESD RsquaredSD     MAESD\n1     2 12.86781 0.3498257 10.55154 0.9235936 0.10765745 0.6993457\n2     3 12.92971 0.3449085 10.51615 0.9716964 0.09693563 0.6348611\n3     4 12.69644 0.3685762 10.30346 0.8366573 0.10280846 0.6553263\n\nAccording to the Forward Stepwise Selection model with natural splines, the top predictor for song energy level is acous.\nExamine LASSO model with natural splines output:\n\n\nsummary(LASSO_spline_mod)\n\n\n            Length Class      Mode     \na0            72   -none-     numeric  \nbeta        1728   dgCMatrix  S4       \ndf            72   -none-     numeric  \ndim            2   -none-     numeric  \nlambda        72   -none-     numeric  \ndev.ratio     72   -none-     numeric  \nnulldev        1   -none-     numeric  \nnpasses        1   -none-     numeric  \njerr           1   -none-     numeric  \noffset         1   -none-     logical  \ncall           5   -none-     call     \nnobs           1   -none-     numeric  \nlambdaOpt      1   -none-     numeric  \nxNames        24   -none-     character\nproblemType    1   -none-     character\ntuneValue      2   data.frame list     \nobsLevels      1   -none-     logical  \nparam          0   -none-     list     \n\nplot(LASSO_spline_mod)\n\n\n\nLASSO_spline_mod$bestTune\n\n\n  alpha    lambda\n6     1 0.5050505\n\n# LASSO_spline_mod$results\n\n\n\nThe lambda value provided by the LASSO model with splines is 0.5050505.\nGAM with LOESS terms\nFit a GAM using LOESS terms using the set of variables deemed to be most relevant based on your investigations so far.\nHow does test performance of the GAM compare to other models you explored?\nDo you gain any insights from the GAM output plots for each predictor?\n\n\nset.seed(253)\ngam_mod <- train(\n    nrgy ~ acous + val + dB,\n    data = top_spotify_new,\n    method = \"gamLoess\",\n    tuneGrid = data.frame(degree = 1, span = seq(0.1, 0.9, by = 0.1)),\n    trControl = trainControl(method = \"cv\", number = 9, selectionFunction = \"best\"),\n    metric = \"MAE\",\n    na.action = na.omit\n)\n\n\n\nExamine GAM with LOESS output:\n\n\ngam_mod$results[3,]\n\n\n  degree span     RMSE  Rsquared      MAE   RMSESD RsquaredSD\n3      1  0.3 10.20914 0.5906928 8.252745 0.892152  0.1020878\n      MAESD\n3 0.8103724\n\n\n\nplot(gam_mod)\n\n\n\n#Metrics for the best model \ngam_mod$results %>%\n    filter(span==gam_mod$bestTune$span)\n\n\n  degree span     RMSE  Rsquared      MAE   RMSESD RsquaredSD\n1      1  0.3 10.20914 0.5906928 8.252745 0.892152  0.1020878\n      MAESD\n1 0.8103724\n\n#Graphing Each Predictor \npar(mfrow = c(3,4)) # Sets up a grid of plots\nplot(gam_mod$finalModel, se = TRUE) # Dashed lines are +/- 2 SEs\n\n\n\n\nGAM with a span of 0.3 offers a MAE of 8.252745, indicating that we our predictions for top song energy level would be off by 8.368585 percentage points in this case. This result is actually better than all four previous models fitted in the first section.\nSummarize investigations\nDecide on an overall best model based on your investigations so far. To do this, make clear your analysis goals. Predictive accuracy? Interpretability? A combination of both?\nOverall, based on the output given by all of the models we fitted above, it seems that a GAM with LOESS model achieves the lowest MAE for our dataset. For our analysis, since we want to correctly predict the energy level of a popular song, we care about the predictive accuracy of the model. We are also interested in knowing what contributes to an energetic song, thus interpretability is also essential for the model. Therefore splines doesn’t seem the most straightforward choice for us, whereas either GAM with LOESS or LASSO seems like a better option.\nSocietal impact\nAre there any harms that may come from your analyses and/or how the data were collected? What cautions do you want to keep in mind when communicating your work?\nOur models takes a harmless look at the deciding elements of an energetic song, as under the environment of a global pandemic where social interactions are limited, it is important to look for means to maintain a positive mood, and it seems that listening to uplifting pop music is a favorable way to do so. Given our dataset, though, since the source is Spotify and Billboard, our scope of pop music is limited and may result in a certain pattern in our predictions. We want to caution that good music choice should by no means be limited, and it should always be optimal to listen to whatever one’s heart desires.\nClassification analysis (Methods)\nWe used logistic regression and random forest for building classification models.\nLogistic Regression\nWe converted the predictor “pop” to categorical, assigning the observations with value above 75 to be top songs.\n\n\ntop_spotify_new$IsPop <- \"NO\"\ntop_spotify_new$IsPop[top_spotify_new$pop >= 75] <- \"YES\"\ntable(top_spotify_new$IsPop)\ntable(top_spotify_new$pop)\ntop_spotify_new$IsPop <- factor(top_spotify_new$IsPop)\n\n\n\nWe then fit the logistic regression model predicting whether a given song is a popular song with all other predictors. We selected the metrics Accuracy so that the model we fit would prioritize making the most accurate predictions.\n\n\nset.seed(253)\nlogistic_mod <- train(\n    IsPop ~ .-pop,\n    data = top_spotify_new,\n    method = \"glm\",\n    family = \"binomial\",\n    trControl = trainControl(method = \"cv\", number = 10),\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\nsummary(logistic_mod$results)\n\n\n  parameter            Accuracy          Kappa       \n Length:1           Min.   :0.7124   Min.   :0.2109  \n Class :character   1st Qu.:0.7124   1st Qu.:0.2109  \n Mode  :character   Median :0.7124   Median :0.2109  \n                    Mean   :0.7124   Mean   :0.2109  \n                    3rd Qu.:0.7124   3rd Qu.:0.2109  \n                    Max.   :0.7124   Max.   :0.2109  \n   AccuracySD         KappaSD       \n Min.   :0.02915   Min.   :0.07924  \n 1st Qu.:0.02915   1st Qu.:0.07924  \n Median :0.02915   Median :0.07924  \n Mean   :0.02915   Mean   :0.07924  \n 3rd Qu.:0.02915   3rd Qu.:0.07924  \n Max.   :0.02915   Max.   :0.07924  \n\ncoefficients(logistic_mod$finalModel) %>% exp()\n\n\n  (Intercept)          year           bpm          nrgy          dnce \n1.508016e-192  1.246311e+00  1.000621e+00  9.717299e-01  1.007890e+00 \n           dB          live           val           dur         acous \n 1.147383e+00  9.915256e-01  1.006670e+00  9.968091e-01  1.001887e+00 \n         spch \n 9.881283e-01 \n\nWe also fit the LASSO logistic regression, gaining insight about variable importance.\n\n\ntwoClassSummaryCustom <- function (data, lev = NULL, model = NULL) {\n    if (length(lev) > 2) {\n        stop(paste(\"Your outcome has\", length(lev), \"levels. The twoClassSummary() function isn't appropriate.\"))\n    }\n    caret:::requireNamespaceQuietStop(\"pROC\")\n    if (!all(levels(data[, \"pred\"]) == lev)) {\n        stop(\"levels of observed and predicted data do not match\")\n    }\n    rocObject <- try(pROC::roc(data$obs, data[, lev[1]], direction = \">\", \n        quiet = TRUE), silent = TRUE)\n    rocAUC <- if (inherits(rocObject, \"try-error\")) \n        NA\n    else rocObject$auc\n    out <- c(rocAUC, sensitivity(data[, \"pred\"], data[, \"obs\"], \n        lev[1]), specificity(data[, \"pred\"], data[, \"obs\"], lev[2]))\n    out2 <- postResample(data[, \"pred\"], data[, \"obs\"])\n    out <- c(out, out2[1])\n    names(out) <- c(\"AUC\", \"Sens\", \"Spec\", \"Accuracy\")\n    out\n}\nset.seed(253)\nlasso_logistic_mod <- train(\n    IsPop ~ .-pop,\n    data = top_spotify_new,\n    method = \"glmnet\",\n    family = \"binomial\",\n    tuneGrid = data.frame(alpha = 1, lambda = seq(0, 1, length.out = 100)),\n    trControl = trainControl(method = \"cv\", number = 10, selectionFunction = \"oneSE\", classProbs = TRUE, summaryFunction = twoClassSummaryCustom),\n    metric = \"AUC\",\n    na.action = na.omit\n)\n\nlasso_logistic_mod\n\n\nglmnet \n\n602 samples\n 11 predictor\n  2 classes: 'NO', 'YES' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 542, 542, 542, 542, 541, 543, ... \nResampling results across tuning parameters:\n\n  lambda      AUC        Sens       Spec        Accuracy \n  0.00000000  0.6749598  0.9271196  0.25380117  0.7157748\n  0.01010101  0.6752695  0.9513937  0.20087719  0.7158850\n  0.02020202  0.6746745  0.9610918  0.12690058  0.6991901\n  0.03030303  0.6774243  0.9951220  0.06374269  0.7027174\n  0.04040404  0.6853205  0.9975610  0.02105263  0.6909943\n  0.05050505  0.6883566  1.0000000  0.00000000  0.6860489\n  0.06060606  0.6818036  1.0000000  0.00000000  0.6860489\n  0.07070707  0.6777676  1.0000000  0.00000000  0.6860489\n  0.08080808  0.6753102  1.0000000  0.00000000  0.6860489\n  0.09090909  0.6753102  1.0000000  0.00000000  0.6860489\n  0.10101010  0.6753102  1.0000000  0.00000000  0.6860489\n  0.11111111  0.6753102  1.0000000  0.00000000  0.6860489\n  0.12121212  0.6463629  1.0000000  0.00000000  0.6860489\n  0.13131313  0.5172520  1.0000000  0.00000000  0.6860489\n  0.14141414  0.5000000  1.0000000  0.00000000  0.6860489\n  0.15151515  0.5000000  1.0000000  0.00000000  0.6860489\n  0.16161616  0.5000000  1.0000000  0.00000000  0.6860489\n  0.17171717  0.5000000  1.0000000  0.00000000  0.6860489\n  0.18181818  0.5000000  1.0000000  0.00000000  0.6860489\n  0.19191919  0.5000000  1.0000000  0.00000000  0.6860489\n  0.20202020  0.5000000  1.0000000  0.00000000  0.6860489\n  0.21212121  0.5000000  1.0000000  0.00000000  0.6860489\n  0.22222222  0.5000000  1.0000000  0.00000000  0.6860489\n  0.23232323  0.5000000  1.0000000  0.00000000  0.6860489\n  0.24242424  0.5000000  1.0000000  0.00000000  0.6860489\n  0.25252525  0.5000000  1.0000000  0.00000000  0.6860489\n  0.26262626  0.5000000  1.0000000  0.00000000  0.6860489\n  0.27272727  0.5000000  1.0000000  0.00000000  0.6860489\n  0.28282828  0.5000000  1.0000000  0.00000000  0.6860489\n  0.29292929  0.5000000  1.0000000  0.00000000  0.6860489\n  0.30303030  0.5000000  1.0000000  0.00000000  0.6860489\n  0.31313131  0.5000000  1.0000000  0.00000000  0.6860489\n  0.32323232  0.5000000  1.0000000  0.00000000  0.6860489\n  0.33333333  0.5000000  1.0000000  0.00000000  0.6860489\n  0.34343434  0.5000000  1.0000000  0.00000000  0.6860489\n  0.35353535  0.5000000  1.0000000  0.00000000  0.6860489\n  0.36363636  0.5000000  1.0000000  0.00000000  0.6860489\n  0.37373737  0.5000000  1.0000000  0.00000000  0.6860489\n  0.38383838  0.5000000  1.0000000  0.00000000  0.6860489\n  0.39393939  0.5000000  1.0000000  0.00000000  0.6860489\n  0.40404040  0.5000000  1.0000000  0.00000000  0.6860489\n  0.41414141  0.5000000  1.0000000  0.00000000  0.6860489\n  0.42424242  0.5000000  1.0000000  0.00000000  0.6860489\n  0.43434343  0.5000000  1.0000000  0.00000000  0.6860489\n  0.44444444  0.5000000  1.0000000  0.00000000  0.6860489\n  0.45454545  0.5000000  1.0000000  0.00000000  0.6860489\n  0.46464646  0.5000000  1.0000000  0.00000000  0.6860489\n  0.47474747  0.5000000  1.0000000  0.00000000  0.6860489\n  0.48484848  0.5000000  1.0000000  0.00000000  0.6860489\n  0.49494949  0.5000000  1.0000000  0.00000000  0.6860489\n  0.50505051  0.5000000  1.0000000  0.00000000  0.6860489\n  0.51515152  0.5000000  1.0000000  0.00000000  0.6860489\n  0.52525253  0.5000000  1.0000000  0.00000000  0.6860489\n  0.53535354  0.5000000  1.0000000  0.00000000  0.6860489\n  0.54545455  0.5000000  1.0000000  0.00000000  0.6860489\n  0.55555556  0.5000000  1.0000000  0.00000000  0.6860489\n  0.56565657  0.5000000  1.0000000  0.00000000  0.6860489\n  0.57575758  0.5000000  1.0000000  0.00000000  0.6860489\n  0.58585859  0.5000000  1.0000000  0.00000000  0.6860489\n  0.59595960  0.5000000  1.0000000  0.00000000  0.6860489\n  0.60606061  0.5000000  1.0000000  0.00000000  0.6860489\n  0.61616162  0.5000000  1.0000000  0.00000000  0.6860489\n  0.62626263  0.5000000  1.0000000  0.00000000  0.6860489\n  0.63636364  0.5000000  1.0000000  0.00000000  0.6860489\n  0.64646465  0.5000000  1.0000000  0.00000000  0.6860489\n  0.65656566  0.5000000  1.0000000  0.00000000  0.6860489\n  0.66666667  0.5000000  1.0000000  0.00000000  0.6860489\n  0.67676768  0.5000000  1.0000000  0.00000000  0.6860489\n  0.68686869  0.5000000  1.0000000  0.00000000  0.6860489\n  0.69696970  0.5000000  1.0000000  0.00000000  0.6860489\n  0.70707071  0.5000000  1.0000000  0.00000000  0.6860489\n  0.71717172  0.5000000  1.0000000  0.00000000  0.6860489\n  0.72727273  0.5000000  1.0000000  0.00000000  0.6860489\n  0.73737374  0.5000000  1.0000000  0.00000000  0.6860489\n  0.74747475  0.5000000  1.0000000  0.00000000  0.6860489\n  0.75757576  0.5000000  1.0000000  0.00000000  0.6860489\n  0.76767677  0.5000000  1.0000000  0.00000000  0.6860489\n  0.77777778  0.5000000  1.0000000  0.00000000  0.6860489\n  0.78787879  0.5000000  1.0000000  0.00000000  0.6860489\n  0.79797980  0.5000000  1.0000000  0.00000000  0.6860489\n  0.80808081  0.5000000  1.0000000  0.00000000  0.6860489\n  0.81818182  0.5000000  1.0000000  0.00000000  0.6860489\n  0.82828283  0.5000000  1.0000000  0.00000000  0.6860489\n  0.83838384  0.5000000  1.0000000  0.00000000  0.6860489\n  0.84848485  0.5000000  1.0000000  0.00000000  0.6860489\n  0.85858586  0.5000000  1.0000000  0.00000000  0.6860489\n  0.86868687  0.5000000  1.0000000  0.00000000  0.6860489\n  0.87878788  0.5000000  1.0000000  0.00000000  0.6860489\n  0.88888889  0.5000000  1.0000000  0.00000000  0.6860489\n  0.89898990  0.5000000  1.0000000  0.00000000  0.6860489\n  0.90909091  0.5000000  1.0000000  0.00000000  0.6860489\n  0.91919192  0.5000000  1.0000000  0.00000000  0.6860489\n  0.92929293  0.5000000  1.0000000  0.00000000  0.6860489\n  0.93939394  0.5000000  1.0000000  0.00000000  0.6860489\n  0.94949495  0.5000000  1.0000000  0.00000000  0.6860489\n  0.95959596  0.5000000  1.0000000  0.00000000  0.6860489\n  0.96969697  0.5000000  1.0000000  0.00000000  0.6860489\n  0.97979798  0.5000000  1.0000000  0.00000000  0.6860489\n  0.98989899  0.5000000  1.0000000  0.00000000  0.6860489\n  1.00000000  0.5000000  1.0000000  0.00000000  0.6860489\n\nTuning parameter 'alpha' was held constant at a value of 1\nAUC was used to select the optimal model using  the one SE rule.\nThe final values used for the model were alpha = 1 and lambda\n = 0.1111111.\n\nplot(lasso_logistic_mod)\n\n\n\n\n\n\nlasso_logistic_mod$bestTune\n\n\n   alpha    lambda\n12     1 0.1111111\n\nlasso_logistic_mod$results\n\n\n    alpha     lambda       AUC      Sens       Spec  Accuracy\n1       1 0.00000000 0.6749598 0.9271196 0.25380117 0.7157748\n2       1 0.01010101 0.6752695 0.9513937 0.20087719 0.7158850\n3       1 0.02020202 0.6746745 0.9610918 0.12690058 0.6991901\n4       1 0.03030303 0.6774243 0.9951220 0.06374269 0.7027174\n5       1 0.04040404 0.6853205 0.9975610 0.02105263 0.6909943\n6       1 0.05050505 0.6883566 1.0000000 0.00000000 0.6860489\n7       1 0.06060606 0.6818036 1.0000000 0.00000000 0.6860489\n8       1 0.07070707 0.6777676 1.0000000 0.00000000 0.6860489\n9       1 0.08080808 0.6753102 1.0000000 0.00000000 0.6860489\n10      1 0.09090909 0.6753102 1.0000000 0.00000000 0.6860489\n11      1 0.10101010 0.6753102 1.0000000 0.00000000 0.6860489\n12      1 0.11111111 0.6753102 1.0000000 0.00000000 0.6860489\n13      1 0.12121212 0.6463629 1.0000000 0.00000000 0.6860489\n14      1 0.13131313 0.5172520 1.0000000 0.00000000 0.6860489\n15      1 0.14141414 0.5000000 1.0000000 0.00000000 0.6860489\n16      1 0.15151515 0.5000000 1.0000000 0.00000000 0.6860489\n17      1 0.16161616 0.5000000 1.0000000 0.00000000 0.6860489\n18      1 0.17171717 0.5000000 1.0000000 0.00000000 0.6860489\n19      1 0.18181818 0.5000000 1.0000000 0.00000000 0.6860489\n20      1 0.19191919 0.5000000 1.0000000 0.00000000 0.6860489\n21      1 0.20202020 0.5000000 1.0000000 0.00000000 0.6860489\n22      1 0.21212121 0.5000000 1.0000000 0.00000000 0.6860489\n23      1 0.22222222 0.5000000 1.0000000 0.00000000 0.6860489\n24      1 0.23232323 0.5000000 1.0000000 0.00000000 0.6860489\n25      1 0.24242424 0.5000000 1.0000000 0.00000000 0.6860489\n26      1 0.25252525 0.5000000 1.0000000 0.00000000 0.6860489\n27      1 0.26262626 0.5000000 1.0000000 0.00000000 0.6860489\n28      1 0.27272727 0.5000000 1.0000000 0.00000000 0.6860489\n29      1 0.28282828 0.5000000 1.0000000 0.00000000 0.6860489\n30      1 0.29292929 0.5000000 1.0000000 0.00000000 0.6860489\n31      1 0.30303030 0.5000000 1.0000000 0.00000000 0.6860489\n32      1 0.31313131 0.5000000 1.0000000 0.00000000 0.6860489\n33      1 0.32323232 0.5000000 1.0000000 0.00000000 0.6860489\n34      1 0.33333333 0.5000000 1.0000000 0.00000000 0.6860489\n35      1 0.34343434 0.5000000 1.0000000 0.00000000 0.6860489\n36      1 0.35353535 0.5000000 1.0000000 0.00000000 0.6860489\n37      1 0.36363636 0.5000000 1.0000000 0.00000000 0.6860489\n38      1 0.37373737 0.5000000 1.0000000 0.00000000 0.6860489\n39      1 0.38383838 0.5000000 1.0000000 0.00000000 0.6860489\n40      1 0.39393939 0.5000000 1.0000000 0.00000000 0.6860489\n41      1 0.40404040 0.5000000 1.0000000 0.00000000 0.6860489\n42      1 0.41414141 0.5000000 1.0000000 0.00000000 0.6860489\n43      1 0.42424242 0.5000000 1.0000000 0.00000000 0.6860489\n44      1 0.43434343 0.5000000 1.0000000 0.00000000 0.6860489\n45      1 0.44444444 0.5000000 1.0000000 0.00000000 0.6860489\n46      1 0.45454545 0.5000000 1.0000000 0.00000000 0.6860489\n47      1 0.46464646 0.5000000 1.0000000 0.00000000 0.6860489\n48      1 0.47474747 0.5000000 1.0000000 0.00000000 0.6860489\n49      1 0.48484848 0.5000000 1.0000000 0.00000000 0.6860489\n50      1 0.49494949 0.5000000 1.0000000 0.00000000 0.6860489\n51      1 0.50505051 0.5000000 1.0000000 0.00000000 0.6860489\n52      1 0.51515152 0.5000000 1.0000000 0.00000000 0.6860489\n53      1 0.52525253 0.5000000 1.0000000 0.00000000 0.6860489\n54      1 0.53535354 0.5000000 1.0000000 0.00000000 0.6860489\n55      1 0.54545455 0.5000000 1.0000000 0.00000000 0.6860489\n56      1 0.55555556 0.5000000 1.0000000 0.00000000 0.6860489\n57      1 0.56565657 0.5000000 1.0000000 0.00000000 0.6860489\n58      1 0.57575758 0.5000000 1.0000000 0.00000000 0.6860489\n59      1 0.58585859 0.5000000 1.0000000 0.00000000 0.6860489\n60      1 0.59595960 0.5000000 1.0000000 0.00000000 0.6860489\n61      1 0.60606061 0.5000000 1.0000000 0.00000000 0.6860489\n62      1 0.61616162 0.5000000 1.0000000 0.00000000 0.6860489\n63      1 0.62626263 0.5000000 1.0000000 0.00000000 0.6860489\n64      1 0.63636364 0.5000000 1.0000000 0.00000000 0.6860489\n65      1 0.64646465 0.5000000 1.0000000 0.00000000 0.6860489\n66      1 0.65656566 0.5000000 1.0000000 0.00000000 0.6860489\n67      1 0.66666667 0.5000000 1.0000000 0.00000000 0.6860489\n68      1 0.67676768 0.5000000 1.0000000 0.00000000 0.6860489\n69      1 0.68686869 0.5000000 1.0000000 0.00000000 0.6860489\n70      1 0.69696970 0.5000000 1.0000000 0.00000000 0.6860489\n71      1 0.70707071 0.5000000 1.0000000 0.00000000 0.6860489\n72      1 0.71717172 0.5000000 1.0000000 0.00000000 0.6860489\n73      1 0.72727273 0.5000000 1.0000000 0.00000000 0.6860489\n74      1 0.73737374 0.5000000 1.0000000 0.00000000 0.6860489\n75      1 0.74747475 0.5000000 1.0000000 0.00000000 0.6860489\n76      1 0.75757576 0.5000000 1.0000000 0.00000000 0.6860489\n77      1 0.76767677 0.5000000 1.0000000 0.00000000 0.6860489\n78      1 0.77777778 0.5000000 1.0000000 0.00000000 0.6860489\n79      1 0.78787879 0.5000000 1.0000000 0.00000000 0.6860489\n80      1 0.79797980 0.5000000 1.0000000 0.00000000 0.6860489\n81      1 0.80808081 0.5000000 1.0000000 0.00000000 0.6860489\n82      1 0.81818182 0.5000000 1.0000000 0.00000000 0.6860489\n83      1 0.82828283 0.5000000 1.0000000 0.00000000 0.6860489\n84      1 0.83838384 0.5000000 1.0000000 0.00000000 0.6860489\n85      1 0.84848485 0.5000000 1.0000000 0.00000000 0.6860489\n86      1 0.85858586 0.5000000 1.0000000 0.00000000 0.6860489\n87      1 0.86868687 0.5000000 1.0000000 0.00000000 0.6860489\n88      1 0.87878788 0.5000000 1.0000000 0.00000000 0.6860489\n89      1 0.88888889 0.5000000 1.0000000 0.00000000 0.6860489\n90      1 0.89898990 0.5000000 1.0000000 0.00000000 0.6860489\n91      1 0.90909091 0.5000000 1.0000000 0.00000000 0.6860489\n92      1 0.91919192 0.5000000 1.0000000 0.00000000 0.6860489\n93      1 0.92929293 0.5000000 1.0000000 0.00000000 0.6860489\n94      1 0.93939394 0.5000000 1.0000000 0.00000000 0.6860489\n95      1 0.94949495 0.5000000 1.0000000 0.00000000 0.6860489\n96      1 0.95959596 0.5000000 1.0000000 0.00000000 0.6860489\n97      1 0.96969697 0.5000000 1.0000000 0.00000000 0.6860489\n98      1 0.97979798 0.5000000 1.0000000 0.00000000 0.6860489\n99      1 0.98989899 0.5000000 1.0000000 0.00000000 0.6860489\n100     1 1.00000000 0.5000000 1.0000000 0.00000000 0.6860489\n         AUCSD      SensSD     SpecSD  AccuracySD\n1   0.06339949 0.032813112 0.07294721 0.031497291\n2   0.06638225 0.025894083 0.10432083 0.038351085\n3   0.07601837 0.034839979 0.06639423 0.037447157\n4   0.08173310 0.010283830 0.04221072 0.015083279\n5   0.08661335 0.007712872 0.02717883 0.009331956\n6   0.08938083 0.000000000 0.00000000 0.003961554\n7   0.09092336 0.000000000 0.00000000 0.003961554\n8   0.09085215 0.000000000 0.00000000 0.003961554\n9   0.08967735 0.000000000 0.00000000 0.003961554\n10  0.08967735 0.000000000 0.00000000 0.003961554\n11  0.08967735 0.000000000 0.00000000 0.003961554\n12  0.08967735 0.000000000 0.00000000 0.003961554\n13  0.09527686 0.000000000 0.00000000 0.003961554\n14  0.03073130 0.000000000 0.00000000 0.003961554\n15  0.00000000 0.000000000 0.00000000 0.003961554\n16  0.00000000 0.000000000 0.00000000 0.003961554\n17  0.00000000 0.000000000 0.00000000 0.003961554\n18  0.00000000 0.000000000 0.00000000 0.003961554\n19  0.00000000 0.000000000 0.00000000 0.003961554\n20  0.00000000 0.000000000 0.00000000 0.003961554\n21  0.00000000 0.000000000 0.00000000 0.003961554\n22  0.00000000 0.000000000 0.00000000 0.003961554\n23  0.00000000 0.000000000 0.00000000 0.003961554\n24  0.00000000 0.000000000 0.00000000 0.003961554\n25  0.00000000 0.000000000 0.00000000 0.003961554\n26  0.00000000 0.000000000 0.00000000 0.003961554\n27  0.00000000 0.000000000 0.00000000 0.003961554\n28  0.00000000 0.000000000 0.00000000 0.003961554\n29  0.00000000 0.000000000 0.00000000 0.003961554\n30  0.00000000 0.000000000 0.00000000 0.003961554\n31  0.00000000 0.000000000 0.00000000 0.003961554\n32  0.00000000 0.000000000 0.00000000 0.003961554\n33  0.00000000 0.000000000 0.00000000 0.003961554\n34  0.00000000 0.000000000 0.00000000 0.003961554\n35  0.00000000 0.000000000 0.00000000 0.003961554\n36  0.00000000 0.000000000 0.00000000 0.003961554\n37  0.00000000 0.000000000 0.00000000 0.003961554\n38  0.00000000 0.000000000 0.00000000 0.003961554\n39  0.00000000 0.000000000 0.00000000 0.003961554\n40  0.00000000 0.000000000 0.00000000 0.003961554\n41  0.00000000 0.000000000 0.00000000 0.003961554\n42  0.00000000 0.000000000 0.00000000 0.003961554\n43  0.00000000 0.000000000 0.00000000 0.003961554\n44  0.00000000 0.000000000 0.00000000 0.003961554\n45  0.00000000 0.000000000 0.00000000 0.003961554\n46  0.00000000 0.000000000 0.00000000 0.003961554\n47  0.00000000 0.000000000 0.00000000 0.003961554\n48  0.00000000 0.000000000 0.00000000 0.003961554\n49  0.00000000 0.000000000 0.00000000 0.003961554\n50  0.00000000 0.000000000 0.00000000 0.003961554\n51  0.00000000 0.000000000 0.00000000 0.003961554\n52  0.00000000 0.000000000 0.00000000 0.003961554\n53  0.00000000 0.000000000 0.00000000 0.003961554\n54  0.00000000 0.000000000 0.00000000 0.003961554\n55  0.00000000 0.000000000 0.00000000 0.003961554\n56  0.00000000 0.000000000 0.00000000 0.003961554\n57  0.00000000 0.000000000 0.00000000 0.003961554\n58  0.00000000 0.000000000 0.00000000 0.003961554\n59  0.00000000 0.000000000 0.00000000 0.003961554\n60  0.00000000 0.000000000 0.00000000 0.003961554\n61  0.00000000 0.000000000 0.00000000 0.003961554\n62  0.00000000 0.000000000 0.00000000 0.003961554\n63  0.00000000 0.000000000 0.00000000 0.003961554\n64  0.00000000 0.000000000 0.00000000 0.003961554\n65  0.00000000 0.000000000 0.00000000 0.003961554\n66  0.00000000 0.000000000 0.00000000 0.003961554\n67  0.00000000 0.000000000 0.00000000 0.003961554\n68  0.00000000 0.000000000 0.00000000 0.003961554\n69  0.00000000 0.000000000 0.00000000 0.003961554\n70  0.00000000 0.000000000 0.00000000 0.003961554\n71  0.00000000 0.000000000 0.00000000 0.003961554\n72  0.00000000 0.000000000 0.00000000 0.003961554\n73  0.00000000 0.000000000 0.00000000 0.003961554\n74  0.00000000 0.000000000 0.00000000 0.003961554\n75  0.00000000 0.000000000 0.00000000 0.003961554\n76  0.00000000 0.000000000 0.00000000 0.003961554\n77  0.00000000 0.000000000 0.00000000 0.003961554\n78  0.00000000 0.000000000 0.00000000 0.003961554\n79  0.00000000 0.000000000 0.00000000 0.003961554\n80  0.00000000 0.000000000 0.00000000 0.003961554\n81  0.00000000 0.000000000 0.00000000 0.003961554\n82  0.00000000 0.000000000 0.00000000 0.003961554\n83  0.00000000 0.000000000 0.00000000 0.003961554\n84  0.00000000 0.000000000 0.00000000 0.003961554\n85  0.00000000 0.000000000 0.00000000 0.003961554\n86  0.00000000 0.000000000 0.00000000 0.003961554\n87  0.00000000 0.000000000 0.00000000 0.003961554\n88  0.00000000 0.000000000 0.00000000 0.003961554\n89  0.00000000 0.000000000 0.00000000 0.003961554\n90  0.00000000 0.000000000 0.00000000 0.003961554\n91  0.00000000 0.000000000 0.00000000 0.003961554\n92  0.00000000 0.000000000 0.00000000 0.003961554\n93  0.00000000 0.000000000 0.00000000 0.003961554\n94  0.00000000 0.000000000 0.00000000 0.003961554\n95  0.00000000 0.000000000 0.00000000 0.003961554\n96  0.00000000 0.000000000 0.00000000 0.003961554\n97  0.00000000 0.000000000 0.00000000 0.003961554\n98  0.00000000 0.000000000 0.00000000 0.003961554\n99  0.00000000 0.000000000 0.00000000 0.003961554\n100 0.00000000 0.000000000 0.00000000 0.003961554\n\nlasso_logistic_mod$results %>%\n    filter(lambda==lasso_logistic_mod$bestTune$lambda)\n\n\n  alpha    lambda       AUC Sens Spec  Accuracy      AUCSD SensSD\n1     1 0.1111111 0.6753102    1    0 0.6860489 0.08967735      0\n  SpecSD  AccuracySD\n1      0 0.003961554\n\nplot(lasso_logistic_mod$finalModel, xvar = \"lambda\", label = TRUE, col = rainbow(20), ylim = c(-0.5,7))\n\n\n\nrownames(lasso_logistic_mod$finalModel$beta)[c(5,3,1)]\n\n\n[1] \"dB\"   \"nrgy\" \"year\"\n\nTrees and Random Forest\nWe fit trees and random forest to make predictions as well, using all other predictors to predict whether an observation is a popular song or not. The metrics we selected is Accuracy, so that the model would prioritize making accurate predictions.\n\n\nset.seed(253)\ntree_mod <- train(\n    IsPop ~ .-pop,\n    data = top_spotify_new,\n    method = \"rpart\",\n    tuneGrid = data.frame(cp = seq(0, 0.5, length.out = 50)),\n    trControl = trainControl(method = \"cv\", number = 10, selectionFunction = \"oneSE\"),\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\n\nplot(tree_mod)\n\n\n\ntree_mod$results %>%\n    filter(cp==tree_mod$bestTune$cp)\n\n\n         cp  Accuracy    Kappa AccuracySD KappaSD\n1 0.1020408 0.7143012 0.181339 0.03461561 0.13422\n\n\n\nrf_mod <- train(\n    IsPop ~ .-pop,\n    data = top_spotify_new,\n    method = \"rf\",\n    tuneGrid = data.frame(mtry = c(2,3,4,5,6,7,8)),\n    trControl = trainControl(method = \"oob\", selectionFunction = \"best\"),\n    metric = \"Accuracy\",\n    ntree = 750, # To force fitting 1000 trees (can help with stability of results)\n    na.action = na.omit\n)\nplot(rf_mod)\n\n\n\nrf_mod$results\n\n\n   Accuracy     Kappa mtry\n1 0.7524917 0.3347524    2\n2 0.7425249 0.3147939    3\n3 0.7342193 0.3007419    4\n4 0.7392027 0.3082283    5\n5 0.7342193 0.2961637    6\n6 0.7358804 0.2947911    7\n7 0.7375415 0.3026788    8\n\nrf_mod$finalModel\n\n\n\nCall:\n randomForest(x = x, y = y, ntree = 750, mtry = min(param$mtry,      ncol(x))) \n               Type of random forest: classification\n                     Number of trees: 750\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 26.08%\nConfusion matrix:\n     NO YES class.error\nNO  384  29  0.07021792\nYES 128  61  0.67724868\n\nOur model is better at identifying songs that are not popular. In our context, it helps us avoid bad songs, which is preferable than the more lenient alternative which is more likely to falsely categorize a song as popular.\n\n\nvar_imp_rf <- randomForest::importance(rf_mod$finalModel)\n\n# Sort by importance with dplyr's arrange()\nvar_imp_rf <- data.frame(\n        predictor = rownames(var_imp_rf),\n        MeanDecreaseGini = var_imp_rf[,\"MeanDecreaseGini\"]\n    ) %>%\n    arrange(desc(MeanDecreaseGini))\n\n# Top 10\nhead(var_imp_rf, 10)\n\n\n      predictor MeanDecreaseGini\nyear       year         34.01757\nval         val         29.91566\ndur         dur         29.62221\nnrgy       nrgy         29.08262\nbpm         bpm         27.71691\ndnce       dnce         26.28274\nacous     acous         24.00443\nlive       live         23.57061\nspch       spch         18.39002\ndB           dB         15.18356\n\nIt seems that the most important predictor, given contributions to decreasing the Gini index, is year, which is pretty interesting. This tells us that knowing what year the song is released would offer us much insight into whether the song is likely to be popular.\nClassification Analysis (Results- Variable Importance)\nFor our logistic regression model, we utilized a LASSO logistic regression to gain insight on variable importance. The results demonstrate that dnce, and bpm are the most important variables for predicting the popularity of a song. These results are sensible because it is plausible that more upbeat songs that you can dance to will be valuable traits that may lead a song to be more popular.\nIn our random forest model, it shows that year is by far the most important variable for predicting song popularity, which also corresponds to the most important variable as determined by the variable importance measure of a single decision tree. This is because it lowers to Gini index the most on average for all the trees. Year is not very insightful, though, because it is possible that the popular songs featured in this dataset came more from particular years than others. It doesn’t really help us predict the future popularity of a song. More interestingly, the energy displayed by a song has the second most meaningful mean decrease in the Gini index. Again, this is sensible because the goal of a song often times is to portray energy to its listener, so it makes sense that songs that accomplish this goal would be more popular. 22\nClassification analysis (Summary)\nCompare models\nCompare the 2 different classification models tried in light of evaluation metrics, variable importance, and data context:\nWe have compared a logistic regression and a decision tree model. To complement the models, we have ran a LASSO logistic regression and a random forest. We are trying to predict whether a song will be relatively popular. We have created our own binary variable with a threshold of > 75 in pop to be considered popular (IsPop = YES). In all models, the most important variable seemed to be year. In this context, songs released in a certain year seem to be the most popular. Other important variables were energy levels and dance-ibility, both of which intuitively make sense as they would be more commonly enjoyed among music listeners.\nEvaluation metrics\nDisplay evaluation metrics for different models in a clean, organized way. This display should include both the estimated metric as well as its standard deviation. (This won’t be available from OOB error estimation. If using OOB, don’t worry about reporting the SD.):\nLogistic Regression Accuracy: 0.7124414 Logistic Regression Accuracy SD: 0.0291491\nLasso Logistic Regression Accuracy: 0.6860489 Lasso Logistic Regression Accuracy SD: 0.003961554 Lasso Logistic Regression AUC: 0.6715804 Lasso Logistic Regression AUC SD: 0.07519223\nDecision Tree Accuracy: 0.7143012 Decision Tree Accuracy SD: 0.03461561\nRandom Forest Accuracy: 0.7524917 Random Forest Confusion Matrix:\nPREDICTED\n  NO YES class.error\nNO 380 33 0.07990315 YES 122 67 0.64550265\nNIR: (413)/(413+189) = 68.6% The NIR is calculated using the whole training set.\nBroadly summarize conclusions from looking at these evaluation metrics and their measures of uncertainty:\nWe can see that the model that gives us the least amount of variance is lasso logistic regression. However, we note that we are not trying to build the model with the smallest variance, as the variance is just a way to look at the uncertainty of in estimation of test performance, which is not so high when using lasso logistic regression. Random Forest seems to have the highest accuracy. With an OOB estimate error rate of 25.75%, this is reflective of our accuracy.\nOverall most preferable model\nThe overall most preferable model would be our random forest model. The accuracy in this model far outweighs all other models at an accuracy of 75.25% Random forests also provide out of bag error estimations, which give us an accountable measurement of error in our model.\nInterpretion of evaluation metric(s) for the final model in context. Does the model show an acceptable amount of error:\nWith an overall accuracy of 75.2%, and a NIR of 68.6%, we believe this model shows an acceptable amount of error.\nIf using OOB error estimation, display the test (OOB) confusion matrix, and use it to interpret the strengths and weaknesses of the final model:\nWe can see that we have a sensitivity of (67)/(67+112) = 37.43% and a specificity of (380)/(380+33) = 92.01%. Our model is good at correctly predicting songs that won’t be as popular. However, our model is bad at correctly predicting songs that won’t be popular.\n\n\nPopularSongs <- top_spotify_new[top_spotify_new$pop >= 75, ]\ntable(PopularSongs$year)\n\n\n\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 \n   9   10   10   16   12   24   23   25   32   28 \n\ntable(top_spotify_new$year)\n\n\n\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 \n  51   53   35   71   58   95   79   65   64   31 \n\nWe can see that although years are fairly balanced, the years when looking at popular songs seem to be skewed towards later years. This means makes sense as the more “popular” songs seem to be the more recent ones.\n\n\n\n",
    "preview": "https://storage.googleapis.com/pr-newsroom-wp/1/2018/11/Spotify_Logo_CMYK_Green.png",
    "last_modified": "2021-12-01T23:32:47-06:00",
    "input_file": {}
  }
]
