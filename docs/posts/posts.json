[
  {
    "path": "posts/EconHonorsProject/",
    "title": "Economics Independent Research",
    "description": "Honors Thesis, published and won best undergraduate paper award at Minnesota Economics Association 2022 Conference!",
    "author": [
      {
        "name": "Nicholas Di",
        "url": {}
      }
    ],
    "date": "2022-05-15",
    "categories": [],
    "contents": "\nAbstract\nDespite their importance in the social safety net, Unemployment\nInsurance (UI) benefits are expected to increase unemployment duration.\nI find that males, on average, face a greater drop in unemployment than\nfemales when (UI) is no longer offered in their respective state. Male’s\nunemployment rate dropped more by a magnitude of 0.7 percent compared to\nfemale’s which consists of about 11.5 percent of male unemployment\nduring UI. Females who were married, were in lower family income\nbrackets, or had children saw smaller changes in unemployment when UI\nprograms were exhausted.\nLink\nto Award\nLink\nTo Paper As of now in Macalester Commons, but will be published at\nUT Austin’s Jounral: The Developing Economist.\n\n\n\n",
    "preview": "https://media.istockphoto.com/id/878268796/photo/unemployment-claim-form-on-an-office-table.jpg?s=612x612&w=0&k=20&c=mGQYtO9B-Gi5B4MQPHLfgvYT3KE9HNjtlVhejOCUDr8=",
    "last_modified": "2023-10-17T23:36:44-05:00",
    "input_file": "EconHonorsProject.knit.md"
  },
  {
    "path": "posts/Correlated_data_capstone/",
    "title": "Spatial Analysis of Elevated Blood Lead Levels in Twin Cities Area",
    "description": "This project was conducted for the capstone course: STAT 452 Correlated Data. We analyzed elevated blood lead levels in Minneapolis/Saint Paul area through spatial data!",
    "author": [
      {
        "name": "Nicholas Di and Erin Franke",
        "url": {}
      }
    ],
    "date": "2022-05-11",
    "categories": [],
    "contents": "\n\n\n\nIntroduction\nWhen raising a child, parents go through lots of stress to keep their\nchildren safe and healthy. From using car seats to getting children\nvaccinated to working on speech and mobility development and beyond,\nthere is a lot to think about. But one aspect that may be overlooked in\nproviding safe and healthy environment for a child is lead. Lead in\npaint, soil, air, or water is invisible to the naked eye and has no\nsmell (“Prevent Children’s\nExposure to Lead” 2021). However, children can be\nexposed to lead in a variety of manners, including swallowing house dust\nor soil contaminated by lead paint or drinking water delivered through\nlead-based pipes, faucets, and plumbing fixtures. Exposure to this\nhidden element can seriously harm a child’s health, including damage to\nthe child’s brain and nervous system, slowed growth and development, as\nwell as learning, hearing, speech, and behavior problems (“Prevent Children’s\nExposure to Lead” 2021). If exposed to especially high\nlevels of lead, children can face a brain condition known as\nencephalopathy, severe neurological damage, comas, and even death (“Annual Elevated Blood Lead Levels”\n2020). Thus, without a question it is crucial to keep lead\nexposure to a minimum when raising a child.\nIn this project, we analyzed elevated blood lead levels in the\n7 county Twin Cities metropolitan area using public\ndata provided by the Minnesota Department of Health\nover the period of 2015-2019 (Health, n.d.). To protect the privacy of\nindividuals, the smallest granularity we were able to obtain this data\nwas on the census tract level, meaning for each of the 691 census tracts\nin the Twin Cities metropolitan area we obtained information on how many\nchildren were tested and how many of those tests resulted in elevated\nblood lead levels. To have elevated blood lead levels\n(EBLLs) means that a child has a confirmed result at or\nabove 5 micrograms of lead per deciliter of blood (mcg/dL)\n(“Annual Elevated\nBlood Lead Levels” 2020). Children under 6 years of\nage are tested. The Minnesota Department of Health identifies children\nliving in the Minneapolis and Saint Paul city limits as children at a\nhigher risk for lead exposure and recommends these children to receive\nblood lead testing at 1 and 2 years of age. This recommendation is\nwarranted given that in 2019, between 1-2% of children in Minneapolis or\nSt. Paul had EBLLs, which is double the statewide average and higher\nthan any other region of Minnesota (“Annual Elevated Blood Lead Levels”\n2020). Interestingly, the MDH has found children living in\nthe Metro area but not living in the cities of Minneapolis or St. Paul\nare at a lower risk of lead exposure than the Greater Minnesota\n(non-Metro) are. Only about 0.3% of these children have elevated blood\nlead levels whereas about 0.8% of children living in MN outside the\nmetro area have elevated blood lead levels. As a result, to best explore\nthis contrast between Minneapolis-Saint Paul and the suburban region,\nthis project will solely focus on EBLL data from the 7 county Twin\nCities metro area. This region is shown in navy on the road map of\nMinnesota below.\n\n\n\nResearch Goal\nKeeping the health consequences of lead exposure to children in the\nfront of our minds, our research focuses on investigating what is\ncorrelated with a census tract having a noticeably high proportion of\nchildren testing with elevated blood lead levels. We defined a tract to\nbe a “high lead tract” if at least 1% of the tests in the tract resulted\nin elevated blood lead levels (meaning 5+ mcg lead/dL). This left us\nwith 106 “high lead” tracts and 585 “safe” tracts. The location of these\n“high lead” tracts in the Twin Cities metropolitan area can be seen\nbelow in red. It is clear that the majority of them fall in the\nMinneapolis-Saint Paul city limits.\n\n\n\nThe reason why this research question is important is because\nunderstanding what is correlated with tracts having high lead levels can\nhelp the Minnesota Department of Health, organizations, and families\nprotect children from lead exposure. For example, it wouldn’t be\nunreasonable to expect tracts with older homes to have higher lead\nlevels, as these homes are more likely to have been built when science\ndid not know the harms of lead pipes and paint. On March 28, 2022, Saint\nPaul Mayor Melvin Carter announced a $14.5 million American Rescue Plan\ninvestment to remove thousands of lead pipes across the city (n.d.). If home age appears a strong\nindicator of high lead levels, identifying tracts with old homes, high\nlead levels, and lots of young children can alert the city to replace\ntheir pipes first. In our research we also might search for a\nrelationship between testing, income, and lead levels. If we are to find\ncertain income groups getting tested more or less than others holding\nother variables constant, we can shed light on that and advocate for\nresources to get specific tracts the testing they need and deserve given\ntheir exposure.\nTo help us understand what is correlated with a tract being “high\nlead”, we will need more than just the information provided by the MDH\nof tract lead levels. Using the tidycensus (Walker and Herman 2022) package in R, we\ncan access a plethora of information on each census tract including its\nestimated mean age, mean income, population, proportion of family\nhouseholds, home age, and so much more. We begin by exploring the\nrelationship between many of these variables and testing as well as\nEBLLs.\nExploratory Data Analysis\nEstimated Home Age and EBLLs\nOne of the first variables we decided to explore was estimated home\nage. Using the tidycensus package, we were able to access the number of\nresidences in each census tract built prior to 1950, between 1950-1959,\n1960-1969, etc. We made these variables proportions by dividing the\nnumber of homes built in each time period by the total number of homes\nin the census tract. Most revealing was the proportion of homes built\nprior to 1950 - as seen in the map below, the Minneapolis-Saint Paul\ncity limits are largely composed of these older homes while the tracts\non the outskirts of the city have few very homes built before 1950.\n\n\n\nGiven this visualization and our knowledge of history, it is clear\nthat home age likely plays a strong role in lead exposure in children.\nBut it can’t be the only factor. In the map below, we again identify\ntracts with at least 1% of tests registering with elevated blood lead\nlevels. These tracts are colored red and pink, though in the pink\ntracts less than 25% of homes were built before 1950. We see these\npink tracts generally are located outside the MSP city limits in more\nrecently developed suburban areas.\n\n\n\nComparing these pink tracts we have denoted as “high lead” tracts\nthat contain less than 25% of homes built before 1950 to tracts we have\ndenoted as having safe lead levels, there are a few things to notice.\nOur first thought was that perhaps these pink tracts were still\nsignificantly older than the safe lead level tracks and were just built\nlargely in the 1960s and 70s. Lead-based paint and lead-contaminated\ndust are the most common sources of lead poisoning, and paint containing\nlead was not banned in the United States until 1978 (“Common Sources of Lead Poisoning,”\nn.d.). Therefore, any home built prior to 1978 could\ncertainly serve as an exposure threat to children. It ended up that on\naverage 56.1% percent of the homes in the pink tracts were built before\n1979 compared to 54.8% of homes in the safe lead tracts. With such a\nsmall difference, there has to be something else correlated with a\nhigher proportion of tests with EBLLs in particular tracts. Looking into\nother variables, we found the pink high lead tracts have a slightly\nhigher population density at about 2 people/1000 \\(\\text{m}^2\\) than the safe lead tracts at\n1.4 people/1000 \\(\\text{m}^2\\).\nAdditionally, these pink high lead tracts have an estimated median\nincome of $63,431, whereas the safe lead tracts have an estimated median\nincome of almost $87,661. Lead exposure can also come through occupation\n(people exposed to lead through jobs in fields such as auto repair,\nmining, pipe fitting, battery manufacturing, painting, and construction\ncan bring it home on their clothing), soil, pottery, herbal and folk\nremedies, the ingredient tamarind in candy, and cosmetics (“Lead Poisoning” 2022).\nGiven the significant difference in median income between the pink high\nlead tracts and the safe lead tracts, it is possible that residents from\nthe pink high lead tracts live a different lifestyle than residents in\nthe safe lead tracts that causes them to be exposed to lead at a higher\nrate. Exactly how this lead exposure is happening is a mystery that we\ncannot currently solve given the data we have, but the identification of\nthese somewhat unexpected “high lead” tracts is crucial as it can help\ndirect resources and information toward these tracts in order to reduce\nlead exposure.\n\n\n\nWho is getting tested?\nA large factor of obtaining high lead percentages is related to how\noften an area is tested. As more tests are issued, it is more likely we\nwill observe a high lead percentage since the census tract is taking\nprecautions and responding to factors that already cause EBLLs. It is\ninteresting that there is a population within “high lead” census tracts\nof tracts with newer homes that aren’t getting tested at a high rate. We\ndefined a “high rate” of testing to be when the number of tests in the\ntract is less than the estimated number of children who are 0 to 5 age.\nThese specific census tracts are shown in yellow on the map below and\nseem to be located outside of the cities. We are unsure exactly why this\nis, but perhaps it is a result of poor news and communication in terms\nof safely precautions for high lead levels.\n\n\n\nDoing some further investigation using our binary variables\nindicative of high lead and high testing as well as an estimated tract\nhome age variable, we note some interesting patterns. As we noted with\nthe map above, there are several tracts with on average newer homes that\nhave over 1% of tests with EBLLs but are not getting tested at a high\nrate. Again, this may be the result of ignorance or delayed news.\nAdditionally, among the census tracts that have a high testing ratio,\ntracts denoted as “high lead” tend to have a significantly higher\nestimated home age, which is intuitive as older homes tend to have lead\npipes. Furthermore, looking at the relationship between estimated tract\nmedian income and testing, we see higher income census tracts are\ngetting tested less compared to lower income census tracts.\nThis is intuitive as lower income census tracts may be more risk in\nterms of living in older houses and thus face higher lead exposure.\n\n\n\nModeling\nIn the upcoming section, we will be modeling the binary outcome of\nwhether a census tract is considered to be “high lead” or not. As a\nreminder, we denoted a tract as “high lead” if over 1% of tests\ncontained elevated blood lead levels. The majority of these tracts are\nlocated in the Twin Cities.\nLasso\nWe use LASSO logistic regression to distinguish important variables\nin predicting census tracts with high lead concentrations. After tuning\nfor the best penalty, we discovered that income, proportions of homes\nbuilt before 1950, testing ratio (number of tests/child aged 0 to 5),\nnumber of tests total, and median age of the census tract are important\nvariables in modeling the variance of high lead levels. However, it is\ndifficult to account for spatial correlation using LASSO, so we will not\nbe interpreting the output and standard errors. Instead, we will take\nthe important variables LASSO identified and fit a random effect model.\nThe one exception to this is we will drop variable indicating the total\nnumber of tests and soley use testing ratio, as it is better indicative\nof whether the number of tests a tract receives is appropriate for their\npopulation. This random effect model will account for spatial\ncorrelation. Spatial correlation is very important in our study as\ncensus tracts that are close together will share many similar\ncharacteristics in regards to income, community, and more. Leaving this\nunaccounted for will result in correlated residuals.\nMatern Random Effect Models\nEssentially, a Matern random effects model takes into account the\ncorrelation between points via the euclidean distance between\ncoordinates. Our random effect model accounts for spatial correlation by\nincorporating the X and Y coordinates of the centroid, or center, of\neach census tract. We are able to do so by creating a numeric factor\nrepresenting the coordinates of sampled locations. We fit a constant nu\n(smoothness parameter) for easier computational purposes. We use a nu\nvalue of 0.5, which means the Matern correlation is equivalent to\nspatial exponential decay. Because we have the matern correlation\ncoefficient, we do assume isotropic, meaning that the covariance\nfunction depends only on the computational distance.\nWe use this model as it is an alternative way to account for spatial\ncorrelation, by imposing a correlation structure on the random effect so\nthat each census tract are spatially correlated. In absence of the\nrandom effect, neighboring census tracts will have spatially correlated\nresiduals. When two regions are farther away, we expect the correlation\nbetween them to get lower. Rho is a measure of range correlation,\ntherefore a higher value of rho implies more spatial correlation being\ncaptured by the model.\nWe fit two different models, one with our designated important\nvariables from LASSO and another with an interaction between the\nproportion of homes built before 1950 and a categorical income variable.\nThis interaction suggests that income plays a different role among high\nlead levels conditioned on proportion of hold homes. Perhaps if we are\nat a high income level and have high proportion of old homes, we may see\nreduced probability of high lead levels due to the ability to\nrenovate.\n\n\n\nNow that we have our two models, we can evaluate them. We decided to\nuse a threshold of 70% to predict if a census tract is to be considered\nin the high lead category or not. This means that if the logistic\nregression gives us a predicted probability of .70 or higher, we will\nmake a hard prediction that the census area is high lead. We chose a\nthreshold of .70 as it is gives us the best sensitivity. In the context\nof EBLL, a threshold of 0.70 gives us the most accuracy in correctly\ndetermining a census tract with high lead levels.\nThe signs of all coefficients make sense. As income increases, the\nodds ratio decreases by 0.852 for every 1000 dollar increase in median\nincome holding other variables constant. This is in check with our\nunderstanding as the more income a household has, the more likely they\nwill be able to remodel and replace lead pipes. Additionally, the more\nincome a census tract has, the more newer houses we may see. The\nproportion of homes built before 1950 is the most statistically\nsignificant coefficient. As per the first model, a percentage increase\nin a proportion of homes built before 1950 will increase odds ratio of a\ntract being high lead by 1.25 holding other variables constant. Finally,\nthe coefficient on the test ratio variable is also positive, indicating\nan increase in odds of a census tract being high lead as their test\nratio increases. This is intuitive as tracts that are testing more are\nlikely doing so because they face higher exposure.\nOur interaction terms in the second model were all non-significant.\nMeaning, under the model, category of income did not impact high lead\ndifferently despite being conditioned on the proportion of houses built\nbefore 1950. Although all statistically insignificant, income classes\nthat suffered the most from greater houses built before 1950 were the\ncensus tracts with lowest median income.\nWhy does our prediction have 100% accuracy? Spatial random effect\nwill help improve the prediction because it is using neighboring\ninformation to account for that spatial correlation, doing so more in\nthe mean structure and actually change the prediction, conditioned on\nrandom effects and getting more precise and improved conditions, rather\nthan marginal mean prediction. Hence why we have a 100% prediction\naccuracy for both models, because of the random effect that is able to\ncapture variations that are unobservant.\n\n\n\n\n\n\nModeling\nthe percent of children by census tract with EBLLs\nThus far, we have developed a model to predict whether or not a\ncensus tract will have at least 1% of tests return with an indication of\nEBLLs. But its important to acknowledge that not all census tracts that\nwe have denoted as “high lead” have the same proportion of tests\nindicating EBLLs. For the 106 “high lead” tracts, the distribution of\nthe proportion of tests indicating EBLLs is shown below.\n\n\n\nIn order to better understand this distribution and what is\ncorrelated with certain tracts having a higher percentage of tests with\nEBLLs than others, we will build a model for this percentage using\nsolely the 106 “high lead” tracts. Similar to our logistic model\nbuilding process to predict whether or not a tract is “high lead”, we\nwill begin with a LASSO regression model. Variables that remain in the\nmodel after the shrinkage process can be thought of as most important at\nhelping us identify why certain tracts have a higher percentage of tests\nwith EBLLs than others.\nUsing 10-fold cross validation on our 106 census tracts, the LASSO\nmodeling process identified tract population, the proportion of homes\nbuilt between 1950 and 1969, the proportion of homes built before 1950,\nand the estimated mean receipt of supplemental security income (SSI) for\nhouseholds with children under 18 as the most important predictors of\npercentage of tests with EBLLs. Interestingly, population and amount of\nSSI both showed a negative relationship with percentage of tests with\nEBLLs, meaning more highly populated tracts tend to have a lower\nproportion of tests with EBLLs holding other variables constant.\nAdditionally, tracts receiving more SSI per household tend to have a\nlower proportion of tests with EBLLs holding other variables constant.\nThese relationships are shown in the plots below.\n\n\n\nThe reasoning for this phenomena could be that such higher populated\nand impoverished tracts are viewed “higher risk” for lead exposure and\nhave received greater resources to prevent it thus far.\nNow that we have our model, we can evaluate it. The model appears\nsolid with a RMSE of about 1.5%, meaning on average our prediction of a\ntract’s proportion of tests with EBLLs was either too high or too low by\nabout 1.5%. While this is amount of error is relatively small, our model\nmust also have residuals that do not have spatial autocorrelation. As we\nhave discussed, spatial autocorrelation means residuals in one census\ntract are related to the residuals in the census tracts around it, which\nis problematic because we violate the assumption of independence of\nresiduals and jeopardize the validity of hypothesis tests. We can test\nfor spatial autocorrelation with something called the Moran’s I test. In\norder to run the Moran’s I test, we must decide in what way we want to\ndefine census tracts as “close”. In other words, we must define a\nneighborhood structure. There are many options when\ndefining a neighborhood structure. We can define tracts as neighbors if\nthey touch at all, even just at one point such as a corner. This is\ncalled the Queen neighborhood structure. Another option is the Rook\nneighborhood structure, which defines tracts as neighbors if they share\nan edge (more than just a corner). Neighbors can also be defined using\ndistance. The KNN method calculates the distance between the centers (or\ncentroids) of each census tract, and then defines a neighborhood based\non K nearest tracts, distanced based on the centers (Heggeseth\n2022). Because we are only looking at census tracts with high\nlead levels, some tracts do not touch and thus we will use the KNN\nstructure with 4 neighbors. 4 neighbors gives a nice balance between not\nhaving too many neighbors (which makes census tracts almost always\ncorrelated) and not having too few neighbors, making it harder to pick\nup on spatial correlation. The KNN(4) structure is shown below.\n\n\n\n\n\n\nUsing the Moran’s I test with the KNN(4) structure shown above, there\nis very strong evidence to reject our null hypothesis of no spatial\ncorrelation between neighboring tracts. We thus conclude that census\ntracts closer together tend to have similar percentages of tests with\nEBLLs than census tracts further apart. Given this, we will need to use\na model that accounts for this spatial autocorrelation. Two models that\ncan potentially accomplish this are the simultaneous\nautoregressive model (SAR) and the conditional\nautoregressive model (CAR). These models are fit in a similar\nway to an ordinary least squares model as we predict percent of tests\nwith EBLLs using our selected variables, however, we add a component to\nthe model that allows us to use surrounding neighborhood values at\nvarying weights to estimate percentage of tests with EBLLs for each\ntract. After fitting both a CAR and SAR model using the four variables\nselected by LASSO and the KNN(4) neighborhood structure, we compared\nthem using BIC and the Moran’s I test. From the Moran’s I test we\nlearned the SAR model yielded strong evidence in support of independent\nresiduals. This evidence was significantly weaker for the CAR model,\nimplying remaining spatial autocorrelation in the residuals. The BIC (a\ncriterion used for model selection) was also superior for the SAR model\nin comparison to the CAR model, and thus we decided to proceed with the\nSAR structure. While we tested multiple other SAR models with different\ncombinations of explanatory variables, the model with the four variables\nselected by LASS0 proved our best model with the lowest average\nprediction error (about 1.4%).\n\n\n\nWhile the average predictor error of our model is relatively small at\n1.4%, one obvious downfall of this model is that it did not predict any\ncensus tract to have a percent of tests with EBLLs above 5.6%, as seen\nbelow. In reality - as shown in the dotplot earlier in this modeling\nsection - seven tracts had a percent of tests with EBLLs over 6% and two\ntracts had levels over 10%. Thus, our model does not quite capture as\nlarge of a distribution in tract percentages as well as we might have\nliked.\n\n\n\nDespite this, our model does indeed do a good job of not\nsystematically over or under-predicting particular areas of the Twin\nCities metropolitan area. We see that tracts both inside and outside\ncity limits have a mix of positive and negative residuals and there are\nseveral areas where percent of tests with EBLLs are over predicted in\none tract and under predicted in its neighboring tract. Given the strong\nevidence that spatial autocorrelation was accounted for from the Moran’s\nI test, this is not surprising.\n\n\n\nThe biggest takeaway from our model is what we can learn about lead\nexposure patterns using it. Takeaways are generally similar to the LASSO\nregression model we fit, but we now have more certainty in our\ncoefficient estimates and their significance given we are not breaking\nthe assumption of independent residuals. The two significant\ncoefficients on the \\(\\alpha=0.05\\)\nlevel in our model are tract population and the proportion of homes\nbuilt from 1950 to 1969 in each tract. With regard to population, we\nestimate for every additional 1000 people residing in a tract that the\nproportion of tests with EBLLs falls on average 0.4%, holding other\nvariables constant. Given that census tracts are intended to have\nsimilar populations (ideally ~4000 people), this might not seem\npractically significant at first. However, the 106 “high lead” tracts\nhave populations ranging from about 2,000 to over 10,000 people per\ntract, with the majority falling in the 3000 to 6000 range. Thus,\ncomparing a 6,000 resident to 3,000 resident tract, we’d expect the\n6,000 resident tract to have a percent of tests with EBLLs about 1.2%\nlower than that of the 3,000 resident tract, which is a considerable\ndifference. When looking at our second significant variable, we learn\nthat with every 10% increase in the proportion of homes built between\n1950 and 1969 we can expect the percent of tests with EBLLs to decrease\nabout 0.4%, holding other variables constant. This relationship is shown\nin the graph below on the left and is rather interesting when contrasted\nto the graph on the right, which displays proportion of homes built\nbefore 1950 versus percent of tests with EBLLs for “high lead” tracts.\nThe key takeaway here is that as tracts tend to have more homes built\nbetween 1950-1969, their percent of tests with EBLLs tends to\nfall, while as tracts tend to have more homes built prior to\n1950 their percent of tests with EBLLs tends to rise.\nGiven that lead paint was not banned in the United States until 1978,\nthis contrasting relationship is surprising and implies lead paint is\nnot the sole factor causing tracts to have a high percent of tests with\nEBLLs.\n\n\n\nTo learn a little more about what might be happening, we created the\nfollowing graph which shows the remaining home age distribution for high\nlead tracts based on the proportion of homes built from 1950-1969. We\nsee that tracts with very few homes (less than 20%) built from 1950-1969\nare composed on average by over 50% of homes built before 1950. These\ntracts also have the smallest proportion of homes built from 1970-1989.\nAs the proportion of home built 1950-1969 increases, the proportion of\nhomes built before 1950 in the tract decreases and the proportion of\nhomes built 1970-1989 increases. This implies overall higher average\nhome age and helps to explain why we see that relationship we see in our\nmodel.\n\n\n\nThe SSI and proportion of homes built before 1950 variables are both\ninsignificant in this model, though have coefficient directions that\nmake intuitive sense given what we have discussed thus far. Holding\nother variables constant, as the proportion of homes in a tract built\nprior to 1950 increases, the percent of tests with EBLLs in that tract\nincreases. Additionally, as discussed when interpreting the LASSO model,\ntracts receiving more SSI tend to have a lower percentage of tests with\nEBLLs holding other variables constant.\nLimitations\nOne of the main limitations in our analysis was data. While we are\nincredibly thankful to have access to public lead data and demographics\non the census tract level, we had been hoping to complete a\nspatial-temporal analysis looking at the percent of tests with EBLLs in\neach tract each year dating back to the early 2000s. Unfortunately, the\nMinnesota Department of Health did not have this data on hand. An\nadditional goal of our was to look at building-specific data available\nthrough ArcGIS on lead piping for the St. Paul Regional Water Services\n(SPRWS) area and incorporate it into our analysis. However, we ran out\nof time to learn how to web scrape this and so this will be a task for\nthe near future. One other limitation related to the data for this\nproject is that many of the variables we used are estimates. For\nexample, mean tract age is estimated from ACS and census data. Home\nvalues come from government valuations which is done for tax purposes.\nThe fact that there is likely a fair amount of error in these estimates\nshould be taken into account when interpreting model coefficients. Also\nrelated to the topic of modeling is that fact that no neighborhood\nstructure we choose is going to be perfect. For example, we chose to use\nthe KNN(4) neighborhood structure for our models, which defines four\nneighbors for every tract using distances between tract centroids.\nHowever, it is possible that tracts could be similar in other ways.\nTracts close in distance could be incredibly different due to a highway\nrunning between them, while tracts further apart but both bordering the\nriver could actually be more similar.\nFurthermore, our models will not be the best to predict new data\noutside of our dataset. If we have a new census tract added, it will be\ndifficult to account for the spatial correlation. The new observation\nmay be farther away in distance that it will become independent and we\nwill not gain the extra predictive ability. However, we can use the\ndistributions from the matern random effect models to attempt in making\na prediction for a new area. Unfortunately, we are not able to explain\neverything.\nConclusions\nThroughout our research report, we focused on what seems to be\ncorrelated to high lead levels. We anlyzed data from the\n7-county Twin Cities metropolitan area using public\ndata provided by the Minnesota Department of Health\nover the period of 2015-2019 (Health, n.d.). We fit LASSO models to\npick out “important” variables and utilize different spatial correlation\nregressions to obtain accurate standard errors on the coefficients.\nOverall, the age of homes in the tract and median income of census\ntracts seem to be the most important factors when looking at the\nvariation in high lead levels. This is intuitive as old houses,\nmentioned earlier, tend to have older pipes, more dust, paint chips, all\nof which have a causal effect leading to high lead levels. Among houses\nwith high lead levels, as tract population increases and proportions of\nhomes built between 1950-1969 increases we see that the percent of tests\nreturning EBLLs decreases holding other variables constant.\nFurthermore among census tracts with high lead exposure, there is a\nspecific subgroup of tracts that do not test often (test less than once\nper child) and have a new home age. This is potentially dangerous as\nfamilies living in these census tracts may go on about their routine\nthinking living in a newer household is safe when in reality there may\nbe other factors that contribute to high lead exposure. This is\nespecially concerning as testing rates tend to be lower and the percent\nof tests with EBLLs are higher in these census tracts. For future\nresearch, it will be worthwhile to investigate which census tracts are\ngetting tested more often than others and look into other observable\nfactors that may capture the culture within a census tract with regards\nto lead levels.\nAcknowledgements\nWe thank the Minnesota Department of Health and creators of the\ntidycensus package for providing publicly available data that made our\nwork possible. We also give a big thank you to our professor Brianna\nHeggeseth for teaching us the mapping and modeling techniques used in\nthis analysis, as well as for providing support and resources throughout\nthis project.\nReferences\n\n\n\nn.d. Saint Paul Minnesota. https://www.stpaul.gov/news/saint-paul-announces-145-million-investment-replace-lead-pipes.\n\n\n“Annual Elevated Blood Lead Levels.” 2020. Childhood\nLead Exposure: Annual Blood Lead Levels - MN Data. https://data.web.health.state.mn.us/lead_annual_level.\n\n\n“Common Sources of Lead Poisoning.” n.d. Washington\nState Department of Health. https://doh.wa.gov/you-and-your-family/healthy-home/home-contaminants/lead/common-sources-lead-poisoning.\n\n\nHealth, Minnesota Department of. n.d. Childhood Lead Exposure Map:\nMNPH Data Access - MN Dept. Of Health. https://mndatamaps.web.health.state.mn.us/interactive/leadtract.html.\n\n\nHeggeseth, Brianna. 2022. “Correlated Data Notes.”\nBrianna C. Heggeseth. https://bcheggeseth.github.io/CorrelatedData/index.html.\n\n\n“Lead Poisoning.” 2022. Mayo Clinic. Mayo\nFoundation for Medical Education; Research. https://www.mayoclinic.org/diseases-conditions/lead-poisoning/symptoms-causes/syc-20354717.\n\n\n“Prevent Children’s Exposure to Lead.” 2021. Centers\nfor Disease Control and Prevention. Centers for Disease Control;\nPrevention. https://www.cdc.gov/nceh/features/leadpoisoning/index.html.\n\n\nWalker, Kyle, and Matt Herman. 2022. Tidycensus: Load US Census\nBoundary and Attribute Data as ’Tidyverse’ and ’Sf’-Ready Data\nFrames. https://walker-data.com/tidycensus/.\n\n\n\n\n",
    "preview": "https://assets.nrdc.org/sites/default/files/styles/full_content--retina/public/media-uploads/breakingnews_newarkwater_lawsuit_12716367_1076318232400357_1222393660715498598_o_main.jpg?itok=33uB_PJ0",
    "last_modified": "2022-05-31T14:02:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/LassoProject/",
    "title": "Lasso Regression & Simulation",
    "description": "This project was conducted for the capstone course: STAT 455 Mathematical Statistics. We wrote a report and created a simulation to visualize the bias variance tradeoff with penalized regressions!",
    "author": [
      {
        "name": "Nicholas Di, Ellery Island, and Will Orser",
        "url": {}
      }
    ],
    "date": "2022-05-05",
    "categories": [],
    "contents": "\nIntroduction\nLasso, an abbreviation for “least absolute shrinkage and selection\noperator”, was developed independently in the field of geophysics in\n1986 (“Lasso (statistics)”). The technique was rediscovered, named, and\npopularized by statistician Robert Tibshirani in 1996, in his paper\n“Regression Shrinkage and Selection via the Lasso”. The topic of lasso\nstood out to our group as an option for the final project because we\nhave all had experiences applying the technique in our Machine Learning\ncourses. Lasso is also connected to the section of our Mathematical\nStatistics course devoted to linear models. In particular, lasso was\ndeveloped as a method to overcome certain complaints that data analysts\nhad with ordinary least squares (OLS) regression models, namely,\nprediction accuracy and interpretation. OLS estimates often have low\nbias but high variance, meaning that prediction accuracy can sometimes\nbe improved by shrinking or setting to zero some regression\ncoefficients. Further, OLS models typically contain a large number of\npredictors; we often would like to narrow this down to a smaller subset\nthat exhibits the strongest effects (Tibshirani, n.d.).\nLasso falls under the category of penalized or regularized regression\nmethods. Penalized regression methods keep all the predictor variables\nin a model but constrain or regularize their regression coefficients by\nshrinking them towards zero. In certain cases, if the amount of\nshrinkage is large enough, these methods can also serve as variable\nselection techniques by shrinking some coefficients to zero (Gunes 2015). This is the case with\nlasso, which provides both variable selection and regularization to\nenhance the prediction accuracy and the interpretability of the\nresulting statistical model. Lasso was originally developed for use on\nlinear regression models, but is easily extended to other statistical\nmodels including generalized linear models, generalized estimating\nequations, and proportional hazards models (“Lasso (statistics)”). In\nterms of real world applications, lasso is commonly used to handle\ngenetic data because the number of potential predictors is often large\nrelative to the number of observations and there is often little prior\nknowledge to inform variable selection (Ranstam and Cook, n.d.).\nThe sources we explored to learn about lasso in greater depth were\n“LASSO regression”, a brief overview of the technique written by J.\nRanstam and J.A. Cook, Tibshirani’s paper mentioned above, and the\nchapter on lasso in An Introduction to Statistical Learning (ISLR; a\nstatistics textbook commonly used in Machine Learning courses) by Gareth\nJames et al. \nRanstam and Cook provide a nice introductory look into lasso,\nexplaining the motivation behind the method (standard regression models\noften overfit the data and overestimate the model’s predictive power), a\ngeneral description of how lasso works including the role of\ncross-validation in selecting the tuning parameter \\(\\lambda\\), and some of the limitations of\nthe method.\nTibshirani’s paper proposes a new method for estimation in linear\nmodels (“the lasso”), explains the mathematical derivation of this\nmethod, and presents the results of various simulation studies,\ncomparing the novel method to more established methods of variable\nselection and regularization, subset selection and ridge regression.\nTibshirani concludes by examining the relative merits of the three\nmethods in different scenarios, stating that lasso performs best in\nsituations where the predictors represent a small to medium number of\nmoderate-sized effects.\nISLR provided us with the most comprehensive (and understandable)\nlook into lasso. ISLR explains the mathematics involved in lasso and\nprovides an in-depth comparison to ridge regression at the mathematical,\ngeometrical, and functional levels. The textbook concludes that neither\nmethod will universally dominate the other, but that lasso tends to\nperform better in situations where only a relatively small number of\npredictors have substantial coefficients, while ridge regression tends\nto perform better when the response variable is a function of many\npredictors, all with coefficients of relatively equal size. Finally,\nISLR proved extremely useful to us because it included various graphs\nand visualizations that illustrate how and why lasso works the way it\ndoes.\nIn the background section of this report, we will describe the\nmathematical underpinnings of the lasso, ridge regression and OLS\nregression. This will include notation, an explanation of the “penalty\nterm” used in lasso and ridge regression, and alternate interpretations\nof how lasso and ridge regression work. In the main results section, we\nderive the estimators for OLS and ridge regression and create a\nsimulation to understand the lasso estimators. We will introduce the\nset-up for a simulation experiment using R that demonstrates the merits\nand drawbacks of using lasso in comparison to OLS regression. Then, we\nwill compare relevant aspects of the models: regression coefficients,\nerror metrics, and the bias and variance of model predictions. The\ndiscussion section summarizes the main takeaways of our research.\nBackground\nOverfitting and the\nBias-Variance Tradeoff\nWhen models are created, a specific set of data is used to ‘train’\nthem. From this training data, all the coefficients and other parameters\nof the model are determined. Even though a model is trained on a very\nspecific set of data, it is often applied to other data sets. A model\nthat is ‘overfit’ to the training data will make accurate predictions\nfor the training data, but will make significantly less accurate\npredictions when applied to different data. Overfitting occurs when the\nmodel is too sensitive to the training data and ends up picking up on,\nand modeling, random quirks of this subset of data. We wish to avoid\noverfitting our models to ensure that they are able to make accurate\npredictions on unknown data (Gareth James and Tibshirani 2013).\nTwo important properties of a model and its parameters are bias and\nvariance. Bias is the difference between the average value that the\nmodel predicts and the true average; we want our model to be pinpointing\nthe correct average, but this is often extremely challenging to do\nbecause models are simplifications of more complicated phenomena.\nVariance describes how much the estimates of a model would change if the\nmodel was fit using a different dataset. We do not want our model\nestimates to fluctuate widely when different data is used; this is an\nindication that the model is not capturing trends common to all the\ndata. Overfit models tend to have low bias, but high variance – they are\nable to very accurately capture the trends of the training data, but\nthey do not generalize well to other data. Ideally, we would like to\nminimize both bias and variance, but it turns out that these two\nproperties are interrelated. Decreasing bias tends to increase variance\nand decreasing variance tends to increase bias. When constructing a\nmodel, the goal is balance between bias and variance effectively to\nyield an accurate, yet more general model (Gareth James and Tibshirani 2013).\nVariable Selection\nWhenever we are trying to model data with many possible predictors,\nwe want to determine which variables are important for predicting the\noutcome variable. We could include every predictor but often this yields\na complicated and less meaningful model. Variable selection is the\nability of some models to choose which variables are irrelevant to the\nmodel and which variables help predict the outcome variable. Models\naccomplish variable selection by setting a variable’s coefficient equal\nto 0. Variable selection is an extremely useful ability of some models,\nespecially when data context cannot inform variable selection (Gareth James and Tibshirani 2013).\nOrdinary Least Squares\nEstimation\nIn ordinary least squares estimation (OLS), we attempt to find a\nlinear model that best fits the data. Our model is a polynomial \\(\\hat{y} = \\beta_0 +\\beta_1x_1 + \\beta_2x_2 +\n\\space ... \\space + \\beta_nx_n\\) with unknown coefficients \\(\\beta_0, \\space \\beta_1, \\space \\beta_2, \\space\n.., \\space \\beta_n\\). In the method of least squares, we find the\nvalues of these coefficients that minimize the distance between the true\n\\(y\\) values and the predicted \\(y\\) values \\(\\hat{y}\\). We define this distance as a\nresidual: \\(y_i- \\hat{y}\\). To get an\noverall estimate of the prediction error of our model, we compute the\nresidual for each observation, square the residuals and sum these values\n(Gareth James and Tibshirani 2013). We\ncan write this as:\n\\[\n\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n (y_i - [\\beta_0\n+\\beta_1x_1 + \\space ...  \\space + \\beta_nx_n])^2 \\\\\n= \\sum_{i=1}^n ( y_i +\\beta_0 - \\sum_{j=1}^p \\beta_jx_{ij} )^2\n\\] We can summarize the least squares method as: \\[\n\\text{argmin}_{\\beta_0,..., \\beta_n}\\sum_{i=1}^n ( y_i +\\beta_0 -\n\\sum_{j=1}^p \\beta_jx_{ij} )^2\n\\] Instead of using standard mathematical notation, we can write\nlinear models and the least squares method in matrix notation. In matrix\nnotation, a linear model is written as:\n\\[\\mathbf{y} =\n\\mathbf{X}\\boldsymbol\\beta  + \\boldsymbol\\epsilon, \\text{ where }\nE[\\boldsymbol\\epsilon] = \\mathbf{0}\\].\n\\(\\mathbf{y}\\) is the vector of\noutcomes, \\(\\boldsymbol\\beta\\) is the\nvector of covariates, and \\(\\mathbf{X}\\) is the matrix of covariates:\n\\[\\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\\n\\vdots \\\\ y_n \\end{pmatrix}; \\space\\boldsymbol\\beta = \\begin{pmatrix}\n\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix}; \\space \\mathbf{X}\n= \\begin{pmatrix} 1 & x_{11} & \\cdots & x_{p1} \\\\ 1 &\nx_{12} & \\cdots & x_{p2} \\\\ \\vdots & \\vdots & \\ddots\n& \\vdots \\\\ 1 & x_{1n} & \\cdots & x_{pn}\n\\end{pmatrix}.\\] The least squares estimation method then\nbecomes:\n\\[\\text{argmin}_{\\boldsymbol\\beta}\n(\\mathbf{y} - \\mathbf{X}\\boldsymbol\\beta)^\\top(\\mathbf{y} -\n\\mathbf{X}\\boldsymbol\\beta)\\].\nProblems with\nOrdinary Least Squares Estimation\nOLS models are incredibly useful and form the basis of many other\nmodels, but they have problems that other models can address. OLS models\ntend to overfit the data, leading to highly variable predictions when\nthey are applied to new data. They have high variance, especially when\nmaking predictions on the extreme, and thus do not generalize to new\ncontexts. Additionally, they cannot perform variable selection, making\nthe models challenging to interpret when there are a large number of\npredictors. Furthermore, OLS models struggle when predictors are\ncorrelated (Gareth James and Tibshirani\n2013). Because of these problems, OLS models are not\nappropriate in many circumstances, even when a linear model is a good\noption.\nLasso\nLasso is an adjustment to the linear regression framework. In a lasso\nmodel, the goal is the same as for OLS model: minimize the RSS. However,\nwe add an additional penalty term, shown in red below, that limits the\nvalues of the coefficients (Gareth James and Tibshirani 2013).\nSpecifically, lasso is defined as:\n\\[\\text{argmin}_{\\beta_j}\\sum_{i=1}^n (\ny_i +\\beta_0 - \\sum_{j=1}^p \\beta_jx_{ij} )^2 + \\color{red}{\\lambda\n\\sum_{j=1}^p |\\beta_j|}\\]\nWhen minimizing this quantity as a whole, we are minimizing each\ncomponent – both the RSS and the penalty term. Minimizing the penalty\nterm, for a given \\(\\lambda\\), has the\neffect of reducing the values of the coefficients towards zero (Gareth James and Tibshirani 2013). The\nconstant \\(\\lambda\\) allows us to\ncontrol how much the coefficients are shrunk towards zero and is thus\nconsidered a tuning parameter for lasso models. Large \\(\\lambda\\) values weight the penalty term\nheavily, so the coefficient values must be very small to minimize the\noverall function. Small \\(\\lambda\\)\nvalues reduce the importance of the penalty term allowing the\ncoefficients to be larger. In the extreme, if \\(\\lambda\\) is infinitely large, the\ncoefficients would all become zero; if \\(\\lambda\\) is zero, the coefficients would\nbe the OLS solution (Gareth James and Tibshirani\n2013). We discuss how to choose \\(\\lambda\\) in the next section.\nThere is an alternate formulation of lasso that reveals how it is a\nconstrained optimization problem. In this formulation, we define lasso\nas: \\[\n\\text{argmin}_{\\beta_j}\\sum_{i=1}^n ( y_i +\\beta_0 - \\sum_{j=1}^p\n\\beta_jx_{ij} )^2  \\text{; subject to }  \\sum_{j=1}^p |\\beta_j| \\le s.\n\\] In this formulation it is clear that the goal remains to\nminimize the RSS; however, the values of the coefficients are subjected\nto an additional constraint. Instead of using the tuning parameter \\(\\lambda\\), the tuning parameter \\(s\\) is used. For large values of \\(s\\), the coefficients are unconstrained and\ncan have large values. Small values of \\(s\\) impose a tight constraint on the\ncoefficients, forcing them to be small (Gareth James and Tibshirani 2013). With\nthis formulation of lasso, we can visualize the relationship between the\nRSS and the constraint in a two predictors setting. With two predictors,\nthe constraint region is defined as \\(|\\beta_1| + |\\beta_2| \\le s\\); this is a\ndiamond with height \\(s\\). In the graph\nbelow, the blue diamond is the constraint region, the red ellipses\nrepresent contour lines of the RSS, and \\(\\hat{\\beta}\\) is the OLS solution (the\nabsolute minimum of the RSS). In a lasso model, the goal is to find the\nsmallest RSS that is within the constraint region; in this graph, that\nis the point where the ellipses intersect the diamond at its top corner\n(Gareth James and Tibshirani 2013).\n\nSelecting the Tuning\nParameter\nThe tuning parameter is often selected using cross validation. With\ncross validation, the data are randomly divided into equally sized\ngroups called folds. In one iteration, k-1 folds are reserved for\ntraining the model and 1 fold is reserved for testing the model. The\nerror in the predictions generated by the model is computed for the test\nfold. This process is repeated until all the folds are used for testing.\nThen, the average test error is computed across all the folds. For\nselecting \\(\\lambda\\), we compute cross\nvalidated error metrics for many different values of \\(\\lambda\\) and choose a value of \\(\\lambda\\) that leads to low error (Gareth James and Tibshirani 2013).\nComparison to Ridge\nRegression\nRidge regression is another technique that modifies the OLS framework\nby constraining the values of the coefficients. Ridge regression is\ndefined as: \\[\\text{argmin}_{\\beta_j}\\sum_{i=1}^n ( y_i\n+\\beta_0 - \\sum_{j=1}^p \\beta_jx_{ij} )^2 + \\color{red}{\\lambda\n\\sum_{j=1}^p (\\beta_j)^2}\\]. We can see that ridge regression is\nnearly identical to lasso; the only difference is in the penalty term\n(shown above in red). Instead of taking the absolute value of the\ncoefficients, ridge regression squares the coefficients (James et al.,\n2013). We can consider the constrained optimization formulation of ridge\nregression, as we did for lasso: \\[\n\\text{argmin}_{\\beta_j}\\sum_{i=1}^n ( y_i +\\beta_0 - \\sum_{j=1}^p\n\\beta_jx_{ij} )^2  \\text{; subject to }  \\sum_{j=1}^p (\\beta_j)^2 \\le s.\n\\] With two predictors, the constraint region becomes a circle:\n\\(\\beta_1^2 + \\beta_2^2 \\le s^2\\)\n(James et al., 2013). We can construct a very similar graph to the one\nabove:\n By comparing these two graph, we can tell\nthat the only difference between lasso and ridge regression are their\nconstraint regions. In the next section, we discuss an important\nimplication of this difference.\nThe Constraint\nRegion and Variable Selection\nLasso’s constraint region allows it to perform variable selection,\nwhile ridge regression’s does not. In the two dimensional example,\nlasso’s constraint region is a diamond. In a diamond, the points that\nline farthest from the center, the points that are most likely to\nintersect with the RSS contours, are the corners. These corners lie on\nthe axes; if an RSS contour intersects the constraint region at a\ncorner, one coefficient will be set to 0. If a coefficient is set to 0,\nit is selected out of the model. For ridge regression’s circular\nconstraint region, all of the points on the perimeter lie equidistant to\nthe center – no point is more likely to intersect an RSS contour than\nany other point. So, the contours lines do not intersect at an axis for\nridge regression, making it impossible for this technique to perform\nvariable selection (Gareth James and Tibshirani\n2013).\nBenefits of Lasso and\nRidge Regression\nBoth lasso and ridge regression are able to make more accurate\npredictions than OLS in many contexts. Lasso and ridge regression are\noften more accurate than OLS because they sacrifice a small increase in\nbias for a significant reduction in variance. Both ridge regression and\nlasso perform well in a variety of contexts, but the variable selection\nproperty of lasso is a significant advantage. Lasso models have fewer\npredictors, making them easier to interpret. Ridge regression, because\nit includes every variable in the model, outperforms lasso when all of\nthe predictors are related to the outcome. On the other hand, lasso\noutperforms ridge regression when only a few of the predictors are\nrelated to the outcome (Gareth James and Tibshirani\n2013).\nIn the main results section, we will derived the variance of OLS and\nridge regression estimators and perform a simulation to examine bias and\nvariance in lasso estimators.\nMain Results\nDeriving\nOLS, Ridge Regression and Lasso Estimators\nOLS\nAs described above, the OLS problem can be written as \\(\\text{argmin}_{\\boldsymbol\\beta} (\\mathbf{y} -\n\\mathbf{X}\\boldsymbol\\beta)^\\top(\\mathbf{y} -\n\\mathbf{X}\\boldsymbol\\beta)\\).\nWe can derive the OLS estimate for \\(\\boldsymbol\\beta\\):\n\\[\\begin{aligned}\n\n&\\text{argmin}_{\\boldsymbol\\beta} (\\mathbf{y} -\n\\mathbf{X}\\boldsymbol\\beta)^\\top(\\mathbf{y} -\n\\mathbf{X}\\boldsymbol\\beta) \\\\\n\n&= \\frac{\\partial}{\\partial \\boldsymbol\\beta} (\\mathbf{y}^\\top\n\\mathbf{y} - \\mathbf{y}^\\top\\mathbf{X}\\boldsymbol\\beta  -\n\\boldsymbol\\beta^T\\mathbf{X}^Ty + \\boldsymbol\\beta^\\top \\mathbf{X}^\\top\n\\mathbf{X} \\boldsymbol\\beta) \\\\\n\n\n&= \\frac{\\partial}{\\partial \\boldsymbol\\beta} (\\mathbf{y}^\\top\n\\mathbf{y} - 2\\mathbf{y}^\\top\\mathbf{X}\\boldsymbol\\beta +\n\\boldsymbol\\beta^\\top \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol\\beta) \\\\\n\n&= -2\\mathbf{X}^\\top\\mathbf{y} + 2 \\mathbf{X}^\\top \\mathbf{X}\n\\boldsymbol\\beta \\\\\n\n0 &\\stackrel{set}{=} -2\\mathbf{X}^\\top\\mathbf{y} + 2 \\mathbf{X}^\\top\n\\mathbf{X} \\boldsymbol\\beta \\\\\n\n2 \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol\\beta &=\n2\\mathbf{X}^\\top\\mathbf{y} \\\\\n\n(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{X} \\boldsymbol\\beta\n&= (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^\\top\\mathbf{y} \\\\\n\n\\hat{\\boldsymbol\\beta}& = (\\mathbf{X}^T\\mathbf{X})^{-1}\n\\mathbf{X}^\\top\\mathbf{y}\n\n\\end{aligned}\\]\nRidge Regression\nIn ridge regression, the formula we are trying to minimize is \\(\\sum_{i=1}^n(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_j\nx_{ij})^2 + \\lambda\\sum_{j=1}^p \\beta_j^2\\). We can write this in\nmatrix notation as: \\((\\mathbf{y} -\n\\mathbf{X}\\boldsymbol\\beta)^\\top(\\mathbf{y} -\n\\mathbf{X}\\boldsymbol\\beta) + \\lambda\n\\boldsymbol\\beta^T\\boldsymbol\\beta\\). We can minimize this in\nmuch the same way as in OLS:\n\\[\\begin{aligned}\n&\\text{argmin}_{\\boldsymbol\\beta} (\\mathbf{y} -\n\\mathbf{X}\\boldsymbol\\beta)^\\top(\\mathbf{y} -\n\\mathbf{X}\\boldsymbol\\beta) + \\lambda \\boldsymbol\\beta^T\\boldsymbol\\beta\n\\\\\n&= \\frac{\\partial}{\\partial \\boldsymbol\\beta} (\\mathbf{y}^\\top\n\\mathbf{y} - 2\\mathbf{y}^\\top\\mathbf{X}\\boldsymbol\\beta +\n\\boldsymbol\\beta^\\top \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol\\beta +\n\\lambda \\boldsymbol\\beta^T\\boldsymbol\\beta) \\\\\n&= -2\\mathbf{X}^\\top\\mathbf{y} + 2 \\mathbf{X}^\\top \\mathbf{X}\n\\boldsymbol\\beta + 2\\lambda\\boldsymbol\\beta \\\\\n0 &\\stackrel{set}{=} -2\\mathbf{X}^\\top\\mathbf{y} + 2 \\mathbf{X}^\\top\n\\mathbf{X} \\boldsymbol\\beta + 2\\lambda\\boldsymbol\\beta\\\\\n\\mathbf{X}^\\top \\mathbf{X} \\boldsymbol\\beta + \\lambda\\boldsymbol\\beta\n&= \\mathbf{X}^\\top\\mathbf{y} \\\\\n(\\mathbf{X}^\\top \\mathbf{X} + \\lambda\\mathbf{I}) \\boldsymbol\\beta &=\n\\mathbf{X}^\\top\\mathbf{y} \\\\\n(\\mathbf{X}^\\top \\mathbf{X} + \\lambda\\mathbf{I}) (\\mathbf{X}^\\top\n\\mathbf{X} + \\lambda\\mathbf{I}) ^{-1}\\boldsymbol\\beta &=\n\\mathbf{X}^\\top\\mathbf{y}(\\mathbf{X}^\\top \\mathbf{X} +\n\\lambda\\mathbf{I}) ^{-1}\\\\\n\\boldsymbol\\beta &= \\mathbf{X}^\\top\\mathbf{y}(\\mathbf{X}^\\top\n\\mathbf{X} + \\lambda\\mathbf{I}) ^{-1}\\\\\n\\end{aligned}\\]\nConsidering a Simple Case\nWe can consider a simple case: \\(\\mathbf{X}\\) is a diagonal matrix with 1’s\non the diagonals and 0’s on all the off diagonals, the number of\npredictors equals the number of cases, and we force the intercept to go\nthrough the origin. This case allows us simplify our OLS and ridge\nregression estimators. For OLS, the solution is \\(\\boldsymbol\\beta = \\mathbf{y}\\) and for\nridge regression the solution becomes \\(\\boldsymbol\\beta =\n\\frac{\\mathbf{y}}{1+\\lambda}\\). Applying this simple case to find\nthe estimators is helpful particularly for Lasso. Unlike OLS and Ridge\nRegression, there is no closed form solution for \\(\\boldsymbol\\beta\\) for Lasso. To derive any\nestimators for Lasso, we must consider this simple case.\nLasso Estimators in a Simple\nCase\nFor lasso, we can not find a general closed form solution for \\(\\boldsymbol\\beta\\), so we will derive the\nlasso estimates for \\(\\boldsymbol\\beta\\) for the simple case\ndescribed above. We will not use matrix notation in order to easily\napply the assumptions of our simple case.\nRemember that we can write the general form of lasso as:\n\\[\\begin{aligned}\n\n\\text{argmin}_{\\beta}\\sum_{i=1}^n(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_j\nx_{ij})^2 + \\lambda\\sum_{j=1}^p |\\beta_j|\n\n\\end{aligned}\\]\nIf we apply our simplifying assumptions, we can write:\n\\[\\begin{aligned}\n\n\\text{argmin}_{\\beta}\\sum_{j=1}^p(y_i - \\beta_1)^2 + \\lambda|\\beta_1|\n\n\\end{aligned}\\]\nWith these assumptions, we can find a closed form solution for \\(\\beta\\):\n\\[\\begin{aligned}\n\n&\\text{argmin}_{\\beta}(y_i - \\beta_1)^2 + \\lambda|\\beta_1| \\\\\n\n&= \\frac{\\partial}{\\partial \\beta} \\left( (y_j - \\beta_1)^2 +\n\\lambda|\\beta_1| \\right) \\\\\n\n&= \\frac{\\partial}{\\partial \\beta} \\left( y_j^2 - 2y_j\\beta_1 +\n\\beta_1^2 + \\lambda|\\beta_1| \\right) \\\\\n\n&=  - 2y_j + 2\\beta_1 + \\lambda sign(\\beta_1) \\\\\n\n\\end{aligned}\\]\nTo solve for \\(\\beta_1\\), we must\nconsider different regions: (1) when \\(\\beta_1\n< 0\\), (2) when \\(\\beta_1 >\n0\\) and (3) when \\(\\beta_1 =\n0\\).\nwhen \\(\\beta_1 < 0\\) or when\n\\(y_j < - \\lambda/2\\):\n\\[\\begin{aligned}\n\n0 &\\stackrel{set}{=} - 2y_j + 2\\beta_1 - \\lambda \\\\\n\n\\beta_1 &= y_j + \\lambda/2 \\\\\n\n\\end{aligned}\\]\nwhen \\(\\beta_1 > 0\\) or when\n\\(y_j > \\lambda/2\\):\n\\[\\begin{aligned}\n\n0 &\\stackrel{set}{=} - 2y_j + 2\\beta_1 + \\lambda \\\\\n\n\\beta_1 &= y_j - \\lambda/2 \\\\\n\n\\end{aligned}\\]\nwhen \\(\\beta_1 = 0\\):\n\\[\\begin{aligned}\n\n\\text{when } \\beta_1 = 0 \\text{ or when } |y_i| \\le \\lambda/2 : \\\\\n0\n\n\\end{aligned}\\]\nVisualizing the Simple\nCase Estimators\nThe graph below shows the simple case coefficient estimates for OLS,\nridge regression and lasso as a function of the data \\(y_j\\). We can see from that graph, and from\nthe equations derived above, that ridge regression scales the\ncoefficient estimates by the same factor, \\(1/(1+\\lambda)\\), regardless of the value of\n\\(y_j\\). Since it is impossible to\ndivide a non-zero number by any value and get 0, ridge regression cannot\nset any coefficient to zero unless it is already 0. However, lasso\nperforms shrinkage in a different way, allowing some coefficients to be\n0. Lasso changes the values of the coefficients by adding or subtracting\n\\(\\lambda/2\\), depending on the\ncorresponding \\(y_j\\). If \\(y_j\\) is inside the region \\((-\\lambda/2, \\lambda/2)\\), the coefficient\nis shrunk to 0.\n\n\n\nDeriving\nBias and Variance of OLS and Ridge Regression Estimators\nOLS\nBias\nWe will assume that \\(\\mathbf{y} =\n\\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\\) and that \\(E[\\boldsymbol\\epsilon] = \\mathbf{0}\\). We\ncan show that the least squares estimator \\(\\hat{\\boldsymbol\\beta} =\n(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^\\top\\mathbf{y}\\) is an\nunbiased estimator of \\(\\boldsymbol\\beta\\):\n\\[\\begin{aligned}\n\nE[\\hat{\\boldsymbol\\beta}_{OLS}] &= E[(\\mathbf{X}^T\\mathbf{X})^{-1}\n\\mathbf{X}^\\top\\mathbf{y}]\\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^\\top E[\\mathbf{y}],\n\\text{ since X is fixed} \\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^\\top\nE[\\mathbf{X}\\boldsymbol\\beta  + \\boldsymbol\\epsilon], \\text{ by\nassumption}\\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^\\top\n(\\mathbf{X}\\boldsymbol\\beta  + E[\\boldsymbol\\epsilon])\\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^\\top\n(\\mathbf{X}\\boldsymbol\\beta  + 0), \\text{ by assumption}\\\\\n&=(\\mathbf{X}^T\\mathbf{X})^{-1}\n(\\mathbf{X}^\\top\\mathbf{X})\\boldsymbol\\beta \\\\\n&= \\boldsymbol\\beta\n\n\\end{aligned}\\]\nVariance\nWe will assume that \\(\\mathbf{y} =\n\\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\\), \\(E[\\boldsymbol\\epsilon] = \\mathbf{0}\\) and\nthat \\(Var[\\boldsymbol\\epsilon] = \\sigma^2\n\\mathbf{I}\\). We can show that the variance of the least squares\nestimator \\(\\hat{\\boldsymbol\\beta} =\n(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^\\top\\mathbf{y}\\) is\n\\(Var[\\hat{\\boldsymbol\\beta}] =\n\\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\\):\n\\[\\begin{aligned}\n\nVar[\\hat{\\boldsymbol\\beta}_{OLS}] &=\nVar[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^\\top\\mathbf{y}]\\\\\n\n&= (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^\\top\nVar[\\mathbf{y}]((\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^\\top)^\\top,\n\\text{ since } Var(\\mathbf{Ax}) =\n\\mathbf{A}Var(\\mathbf{x})\\mathbf{A}^\\top \\\\\n\n&= (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^\\top Var[\\mathbf{y}]\n\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}, \\text{ since }\n(\\mathbf{AB})^\\top = \\mathbf{B}^\\top\\mathbf{A}^\\top \\text{ and }\n(\\mathbf{A}^{-1})^\\top = (\\mathbf{A}^\\top)^{-1} \\\\\n\n&= (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^\\top\nVar[\\mathbf{X}\\boldsymbol\\beta  + \\boldsymbol\\epsilon]\n\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}, \\text{ by assumption}\\\\\n\n&= (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^\\top\nVar[\\boldsymbol\\epsilon] \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}, \\text{\nsince } \\mathbf{X} \\text{ and } \\boldsymbol{\\beta} \\text{ are fixed}\\\\\n\n&= (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^\\top\n(\\sigma^2\\mathbf{I}) \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}, \\text{ by\nassumption} \\\\\n\n&= \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} (\\mathbf{X}^\\top\n\\mathbf{X})(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\n&= \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\n\\end{aligned}\\]\nRidge Regression\nBias\nWe will assume that \\(\\mathbf{y} =\n\\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\\) and that \\(E[\\boldsymbol\\epsilon] = \\mathbf{0}\\). We\ncan show that the ridge regression estimator \\(\\boldsymbol\\beta = (\\mathbf{X}^\\top \\mathbf{X} +\n\\lambda\\mathbf{I}) ^{-1}\\mathbf{X}^\\top\\mathbf{y}\\) is a biased\nestimator of \\(\\boldsymbol\\beta\\)\n(Taboga):\n\\[\\begin{aligned}\n\nE[\\hat{\\boldsymbol\\beta}_{ridge}] &= E[(\\mathbf{X}^\\top \\mathbf{X} +\n\\lambda\\mathbf{I}) ^{-1}\\mathbf{X}^\\top\\mathbf{y}]\\\\\n\n&= E[(\\mathbf{X}^\\top \\mathbf{X} + \\lambda\\mathbf{I})\n^{-1}\\mathbf{X}^\\top (\\mathbf{X}\\boldsymbol\\beta +\n\\boldsymbol\\epsilon)], \\text{ by assumption} \\\\\n\n&= E[(\\mathbf{X}^\\top \\mathbf{X} + \\lambda\\mathbf{I})\n^{-1}\\mathbf{X}^\\top (\\mathbf{X}\\boldsymbol\\beta) + (\\mathbf{X}^\\top\n\\mathbf{X} + \\lambda\\mathbf{I}) ^{-1}\\mathbf{X}^\\top\n(\\boldsymbol\\epsilon)] \\\\\n\n&= E[(\\mathbf{X}^\\top \\mathbf{X} + \\lambda\\mathbf{I})\n^{-1}\\mathbf{X}^\\top (\\mathbf{X}\\boldsymbol\\beta)] + E[(\\mathbf{X}^\\top\n\\mathbf{X} + \\lambda\\mathbf{I}) ^{-1}\\mathbf{X}^\\top\n(\\boldsymbol\\epsilon)] \\\\\n\n&= (\\mathbf{X}^\\top \\mathbf{X} + \\lambda\\mathbf{I})\n^{-1}\\mathbf{X}^\\top (\\mathbf{X}\\boldsymbol\\beta) + (\\mathbf{X}^\\top\n\\mathbf{X} + \\lambda\\mathbf{I}) ^{-1}\\mathbf{X}^\\top\nE[(\\boldsymbol\\epsilon)], \\text{ since } \\mathbf{X} \\text{ and }\n\\boldsymbol{\\beta} \\text{ are fixed} \\\\\n\n&= (\\mathbf{X}^\\top \\mathbf{X} + \\lambda\\mathbf{I})\n^{-1}\\mathbf{X}^\\top (\\mathbf{X}\\boldsymbol\\beta) + (\\mathbf{X}^\\top\n\\mathbf{X} + \\lambda\\mathbf{I}) ^{-1}\\mathbf{X}^\\top (0), \\text{ by\nassumption } \\\\\n\n&= (\\mathbf{X}^\\top \\mathbf{X} + \\lambda\\mathbf{I})\n^{-1}\\mathbf{X}^\\top \\mathbf{X}\\boldsymbol\\beta  \\\\\n\n\\end{aligned}\\]\nSince \\(E[\\hat{\\boldsymbol\\beta}_{ridge}] =\n(\\mathbf{X}^\\top \\mathbf{X} + \\lambda\\mathbf{I}) ^{-1}\\mathbf{X}^\\top\n\\mathbf{X}\\boldsymbol\\beta\\), the ridge regression estimator for\n\\(\\boldsymbol{\\beta}\\) will always be\nbiased, unless \\(\\lambda = 0\\). If\n\\(\\lambda = 0\\), the ridge regression\nestimator is equal to the OLS estimator, which we showed above is\nunbiased.\nVariance\nWe will assume that \\(\\mathbf{y} =\n\\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\\), \\(E[\\boldsymbol\\epsilon] = \\mathbf{0}\\) and\nthat \\(Var[\\boldsymbol\\epsilon] = \\sigma^2\n\\mathbf{I}\\). We can show that the variance of the ridge\nregression estimator is \\(\\sigma^2(\\mathbf{X}^\\top \\mathbf{X} +\n\\lambda\\mathbf{I}) ^{-1}\\mathbf{X}^\\top \\mathbf{X} (\\mathbf{X}^\\top\n\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\) (Taboga, n.d.):\n\\[\\begin{aligned}\nVar[\\hat{\\boldsymbol\\beta}_{ridge}] &= Var((\\mathbf{X}^\\top\n\\mathbf{X} + \\lambda\\mathbf{I}) ^{-1}\\mathbf{X}^\\top\\mathbf{y})\\\\\n\n&= (\\mathbf{X}^\\top \\mathbf{X} + \\lambda\\mathbf{I})\n^{-1}\\mathbf{X}^\\top Var(\\mathbf{y}) ((\\mathbf{X}^\\top \\mathbf{X} +\n\\lambda\\mathbf{I}) ^{-1}\\mathbf{X}^\\top)^\\top, \\text{ since }\nVar(\\mathbf{Ax}) = \\mathbf{A}Var(\\mathbf{x})\\mathbf{A}^\\top \\\\\n\n&= (\\mathbf{X}^\\top \\mathbf{X} + \\lambda\\mathbf{I})\n^{-1}\\mathbf{X}^\\top Var(\\mathbf{X}\\boldsymbol\\beta +\n\\boldsymbol\\epsilon) ((\\mathbf{X}^\\top \\mathbf{X} + \\lambda\\mathbf{I})\n^{-1}\\mathbf{X}^\\top)^\\top, \\text{ by assumption } \\\\\n\n&= (\\mathbf{X}^\\top \\mathbf{X} + \\lambda\\mathbf{I})\n^{-1}\\mathbf{X}^\\top (Var(\\mathbf{X}\\boldsymbol\\beta) +\nVar(\\boldsymbol\\epsilon)) ((\\mathbf{X}^\\top \\mathbf{X} +\n\\lambda\\mathbf{I}) ^{-1}\\mathbf{X}^\\top)^\\top \\\\\n\n&= (\\mathbf{X}^\\top \\mathbf{X} + \\lambda\\mathbf{I})\n^{-1}\\mathbf{X}^\\top Var(\\boldsymbol\\epsilon) ((\\mathbf{X}^\\top\n\\mathbf{X} + \\lambda\\mathbf{I}) ^{-1}\\mathbf{X}^\\top)^\\top, \\text{ since\n} \\mathbf{X} \\text{ and } \\boldsymbol{\\beta} \\text{ are fixed} \\\\\n\n&= (\\mathbf{X}^\\top \\mathbf{X} + \\lambda\\mathbf{I})\n^{-1}\\mathbf{X}^\\top (\\sigma^2\\mathbf{I}) ((\\mathbf{X}^\\top \\mathbf{X} +\n\\lambda\\mathbf{I}) ^{-1}\\mathbf{X}^\\top)^\\top, \\text{ by assumption\n}  \\\\\n\n&= \\sigma^2(\\mathbf{X}^\\top \\mathbf{X} + \\lambda\\mathbf{I})\n^{-1}\\mathbf{X}^\\top) ((\\mathbf{X}^\\top \\mathbf{X} + \\lambda\\mathbf{I})\n^{-1}\\mathbf{X}^\\top)^\\top \\\\\n\n&= \\sigma^2(\\mathbf{X}^\\top \\mathbf{X} + \\lambda\\mathbf{I})\n^{-1}\\mathbf{X}^\\top \\mathbf{X} ((\\mathbf{X}^\\top \\mathbf{X} +\n\\lambda\\mathbf{I}) ^{-1})^\\top \\\\\n\n& = \\sigma^2(\\mathbf{X}^\\top \\mathbf{X} + \\lambda\\mathbf{I})\n^{-1}\\mathbf{X}^\\top \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X} +\n\\lambda\\mathbf{I})^{-1} \\\\\n\n\\end{aligned}\\]\nWe can show that the variance of the ridge regression estimator is\nequal to the variance of the OLS estimator when \\(\\lambda = 0\\):\n\\[\\begin{aligned}\n\nVar[\\hat{\\boldsymbol\\beta}_{ridge}] \\text{ when } \\lambda = 0: \\\\\n&= \\sigma^2(\\mathbf{X}^\\top \\mathbf{X} + 0\\mathbf{I})\n^{-1}\\mathbf{X}^\\top \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X} +\n0\\mathbf{I})^{-1} \\\\\n&= \\sigma^2(\\mathbf{X}^\\top \\mathbf{X}) ^{-1}\\mathbf{X}^\\top\n\\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\\\\n&= \\sigma^2(\\mathbf{X}^\\top \\mathbf{X}) ^{-1} =\nVar[\\hat{\\boldsymbol\\beta}_{OLS}]\n\n\\end{aligned}\\]\nImportantly, the variance of the ridge regression estimator is always\nsmaller than the variance of the OLS estimator when \\(\\lambda>0\\). To see that this is true,\nwe can consider the case when \\(\\mathbf{X}\\) is a 1 by 1 matrix with value\n1 ([1]) and \\(\\lambda = 1\\):\n\\[\\begin{aligned}\n\nVar[\\hat{\\boldsymbol\\beta}_{ridge}] &= \\sigma^2(\\mathbf{X}^\\top\n\\mathbf{X} + \\lambda\\mathbf{I}) ^{-1}\\mathbf{X}^\\top \\mathbf{X}\n(\\mathbf{X}^\\top \\mathbf{X} + \\lambda\\mathbf{I})^{-1} \\\\\n\n&= \\sigma^2(1 *1 + 1) ^{-1}1*1 (1*1 + 1)^{-1} \\\\\n\n&= \\sigma^2(2) ^{-1}(2)^{-1} \\\\\n\n&= \\frac{\\sigma^2}{4}\n\n\\end{aligned}\\]\n\\[\\begin{aligned}\n\nVar[\\hat{\\boldsymbol\\beta}_{OLS}] &=\n\\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\n&= \\sigma^2(1 *1) ^{-1} \\\\\n\n&= \\frac{\\sigma^2}{1} = \\sigma^2\n\n\\end{aligned}\\]\nFrom this simple case, we can see that \\(Var[\\hat{\\boldsymbol\\beta}_{ridge}]\\) is\nsmaller than \\(Var[\\hat{\\boldsymbol\\beta}_{OLS}]\\). This\nholds true for all cases when \\(\\lambda>0\\), but the proof of that is\nbeyond the scope of this project (Taboga, n.d.).\nLasso\nLasso, unlike OLS and ridge regression, does not have closed form\nsolutions for the bias and variance of its estimator. To examine the\nbias and variance of lasso estimators, we constructed a simulation and\nwe discuss the results of the simulation in the next section.\nSimulation\nFor the simulation, we generated a dataset of 9 variables, 3 of which\nare highly correlated with one another. The 9th variable is the \\(y\\) variable that we will be trying to\npredict. This outcome variable is a linear combination of 2 correlated\nvariables, 3 independent variables, and some noninformative variables.\nWe also added some measurement error to \\(y\\). The true form of \\(y\\) is as follows: \\(y = 0v_1 +2v_2 +2v_3+ 5v_4 +5v_5 +5v_6 + 3v_7 +\n0v_8+\\text{rnorm}(0,6)\\). The rnorm adds measurement noise to\nmodel. First, we fit an OLS model to the data, and then we fit a lasso\nregression model. We compare the coefficient estimates for both the OLS\nmodel and the lasso model to the true coefficient estimates. We also\nexamine the bias and variance of the estimates from both models.\n\n\n\n\n\n\nCoefficient Estimates\n\n# A tibble: 9 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  -0.673    0.378      -1.78  7.46e- 2\n2 V1            0.0184   0.0476      0.386 7.00e- 1\n3 V2            2.13     0.108      19.8   7.76e-86\n4 V3            1.86     0.0905     20.5   1.02e-91\n5 V4            5.06     0.0301    168.    0       \n6 V5            5.01     0.00845   593.    0       \n7 V6            4.99     0.00597   837.    0       \n8 V7            3.01     0.0204    148.    0       \n9 V8           -0.0122   0.0302     -0.405 6.86e- 1\n\nThe table above provides the coefficient estimates and their standard\nerrors for the linear model. For the correlated variables, (\\(v_1, v_2, v_3\\)), the standard errors are\nhigher than for the noncorrelated variables because the linear model\nstruggles to deal with multicollinearity. The linear model can\ndistinguish between variables with true non-zero coefficients and\nnoninformative variables, but it did not set the coefficients of the\nnoninformative variables exactly to 0.\n\n\n\n\n# A tibble: 1 × 2\n  penalty .config               \n    <dbl> <chr>                 \n1     0.1 Preprocessor1_Model001\n\n\n# A tibble: 9 × 3\n  term        estimate penalty\n  <chr>          <dbl>   <dbl>\n1 (Intercept)     2.72   0.351\n2 V1              0      0.351\n3 V2              2.11   0.351\n4 V3              1.74   0.351\n5 V4              4.88   0.351\n6 V5              4.96   0.351\n7 V6              4.96   0.351\n8 V7              2.89   0.351\n9 V8              0      0.351\n\nThe table above displays the coefficient estimates generated by the\nlasso model. Unlike the OLS model, lasso was able to set the\ncoefficient’s of the noninformative variables exactly to 0.\nModel Accuracy\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 mae     standard    4.81    10  0.0352 Preprocessor1_Model1\n2 rmse    standard    6.02    10  0.0451 Preprocessor1_Model1\n\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 mae     standard    4.85    10  0.0342 Preprocessor1_Model1\n2 rmse    standard    6.08    10  0.0457 Preprocessor1_Model1\n\nThe two tables above show the overall accuracy of the two models\naccording to two different error metrics MAE and RMSE. Comparing the\nmodels’ accuracy reveals that the lasso model is slightly less accurate\nthan the OLS model; however, this difference is very small. Although the\nlasso model is less accurate, it’s ability to set coefficients to 0 and\nthus perform variable selection is a significant advantage over the OLS\nmodel. In the next section, we will examine how the lasso coefficient\nestimates change as we alter the value of the tuning parameter \\(\\lambda\\).\nChanging the Tuning\nParameter\n\n\n\nThis graph depicts what happens to the coefficient estimates as \\(\\lambda\\) increases. As \\(\\lambda\\) reaches 50, all of the\ncoefficients are set to 0. However, the coefficients are not set to 0 at\nthe same time. Both the coefficients of \\(v_1\\) and \\(v_8\\), the noninformative variables, were\nset to 0 with a very small \\(\\lambda\\).\nThe most important variable (because of its large variance), \\(v_6\\), is set to 0 only for very large\nvalues of \\(\\lambda\\).\nThe Bias and\nVariance of the Coefficient Estimates\nTo get estimates for the bias and variance of the coefficient\nestimate for both models, we sampled 100 different datasets of\ncoefficient values from the larger dataset generated in the\nbeginning.\n\n              Estimate Std. Error    t value     Pr(>|t|)\n(Intercept) -4.4961576 4.06832662 -1.1051614 2.720031e-01\nV1           0.1536838 0.48783342  0.3150333 7.534576e-01\nV2           1.9078509 1.26265671  1.5109814 1.342580e-01\nV3           2.0410304 0.98715975  2.0675787 4.151895e-02\nV4           5.1588137 0.34129732 15.1153069 1.502605e-26\nV5           4.9461718 0.07327357 67.5028130 1.595056e-79\nV6           5.0132068 0.05598747 89.5415868 1.594911e-90\nV7           3.2342516 0.19213361 16.8333459 1.076086e-29\nV8           0.3539362 0.31637034  1.1187402 2.661954e-01\n             (Intercept)        V1       V2      V3       V4       V5\nmy_estimates   -4.496158 0.1536838 1.907851 2.04103 5.158814 4.946172\n                   V6       V7        V8\nmy_estimates 5.013207 3.234252 0.3539362\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis graph visualizes how frequently \\(v_4\\) had a specific coefficient value for\neach model. By comparing the most commonly occuring coefficient value\nfor lasso and for OLS to the true value, we can tell that the lasso\ncoefficient is more biased than the OLS coefficient. However, the\nvariance of the lasso coefficient is far smaller than the variance for\nOLS coefficient.\n\n\n\nThis graph depicts the bias and variance for a noninformative\nvariable for both models. In this graph, the reduction in variance in\nthe lasso model is even more extreme than in the graph for the\ninformative variable. While both models seem to be relatively unbiased,\nthe lasso model’s small variance will yield more accurate predictions\noverall.\n\n\n\n\n           Bias_lm Variance_lm  Bias_lasso Variance_lasso\nV1_lm  0.066834354 0.253835703  0.01883497   0.0026286929\nV2_lm  0.055503772 1.288218122  0.09150388   0.0756530955\nV3_lm -0.121160270 0.902640797 -0.27157368   0.0691781158\nV4_lm  0.059540947 0.091731644 -0.11984989   0.0077073755\nV5_lm  0.013835419 0.007307476 -0.04399253   0.0006592307\nV6_lm -0.010163536 0.003733533 -0.04331724   0.0002988397\nV7_lm -0.002287099 0.042577869 -0.11252372   0.0043010517\nV8_lm  0.006798568 0.112422037  0.00087243   0.0002184819\n      Actual Value\nV1_lm            0\nV2_lm            2\nV3_lm            2\nV4_lm            5\nV5_lm            5\nV6_lm            5\nV7_lm            3\nV8_lm            0\n\nThis table shows the average bias and variance for each coefficient\nfor both the OLS and lasso model. Overall, the variances for the\ncoefficients in the lasso model are much smaller than the variances in\nthe OLS model, but the biases are larger for the lasso model\ncoefficients.\nDiscusion\nTo conclude our report, we will briefly discuss the relevance,\nlimitations, and applications of lasso regression. Lasso is relevant\nbecause of its ability to address the shortcomings of OLS regression\nmodels. Specifically, lasso is able to account for multicollinearity of\npredictor variables and correct for overfitting in situations with a\nlarge number of predictors. Furthermore, unlike some penalized\nregression methods (e.g., ridge regression) lasso has the ability to\nperform variable selection, by shrinking the regression coefficients of\ncertain predictors to zero, thus improving model interpretability.\nIn the main results section, we derived the estimators for OLS and\nridge regression and the bias and variance of these estimators.\nAdditionally, we included relevant outputs and visualizations from a\nsimulation experiment in which we compared the performance of lasso and\nOLS in modeling a fictitious dataset. There were two main takeaways from\nour simulation experiment. First, lasso, unlike OLS, performs variable\nselection by shrinking the coefficients of uninformative predictors to\nzero. In the coefficient output tables, we saw that lasso set the\ncoefficients of uninformative predictors (which we had given a true\nvalue of zero in the data creation stage) to zero, while OLS gave these\nvariables very small nonzero coefficient values. Thus, lasso helps to\nsimplify the model (and prevent overfitting) by eliminating predictors\nwith negligible effects on the output. The second main takeaway was that\nlasso, in comparison to OLS, provides an advantage in terms of the\nbias-variance tradeoff. The density plots from our simulations show how\nlasso returns predictor coefficient estimates that are slightly more\nbiased, but much less variable.\nIn spite of the results of our simulation, it is important to\nrecognize that lasso is not a cure-all for the issues of overfitting and\nmulticollinearity and does not remove the need to validate a model on a\ntest dataset. The primary limitation of lasso is that it trades off\npotential bias in estimating individual parameters for a better expected\noverall prediction. In other words, under the lasso approach, regression\ncoefficients may not be reliably interpreted in terms of independent\nrisk factors, as the model’s focus is on the best combined prediction,\nnot the accuracy of the estimation and interpretation of the\ncontribution of individual variables. Also, lasso may underperform in\ncomparison to ridge regression in situations where the predictor\nvariables account for a large number of small effects on the response\nvariable.\nIn the real world, lasso is commonly used to handle genetic data\nbecause the number of potential predictors is often large relative to\nthe number of observations and there is often little prior knowledge to\ninform variable selection (Ranstam & Cook 1). Lasso also has\napplications in economics and finance, helping to predict events like\ncorporate bankruptcy. Besides these specific fields of application,\nlasso is also implementable in any situation where multiple linear\nregression would apply. Multiple linear regression has wide-ranging\napplications, but to provide a specific example, it is often used in\nmedical research. Researchers may want to test whether there is a\nrelationship between various categorical variables (e.g., drug treatment\ngroup, patient sex), quantitative variables (e.g., patient age, cardiac\noutput), and a quantitative outcome (e.g., blood pressure). Multiple\nlinear regression allows researchers to test for this relationship, as\nwell as quantify its direction and strength. Lasso regression may come\ninto play in scenarios where multicollinearity exists (e.g., patient\nheight and weight), there are a large number of predictors (and it is\nlikely some are uninformative), and when it is important to have\nless-variable predictions for model coefficients.\nLink to Simulation Download\nHere\nReferences\n\n\n\nGareth James, Trevor Hastie, Daniela Witten, and Robert Tibshirani.\n2013. “Prevent Children’s Exposure to Lead.” An\nIntroduction to Statistical Learning. https://www.ime.unicamp.br/~dias/Intoduction%20to%20Statistical%20Learning.pdf.\n\n\nGunes, Funda. 2015. “Penalized Regression Methods for Linear\nModels in SAS/STAT.” Childhood Lead Exposure: Annual Blood\nLead Levels - MN Data. https://support.sas.com/rnd/app/stat/papers/2015/PenalizedRegression_LinearModels.pdf.\n\n\nRanstam, J., and J. A. Cook. n.d. “LASSO Regression.”\nBritish Journal of Surgery. https://bjssjournals.onlinelibrary.wiley.com/doi/10.1002/bjs.10895.\n\n\nTaboga, Marco. n.d. “Ridge Regression.” https://www.statlect.com/fundamentals-of-statistics/ridge-regression.\n\n\nTibshirani, Robert. n.d. “Regression Shrinkage and Selection via\nthe Lasso.” Journal of the Royal Statistical Society. https://www.jstor.org/stable/2346178.\n\n\n\n\n",
    "preview": "https://kirenz.com/post/2019-08-12-python-lasso-regression-auto/featured_hue76a1f1ce8c01e6382d5577465b6e9e2_44866_680x500_fill_q90_lanczos_smart1.jpg",
    "last_modified": "2022-05-31T14:16:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/S&P_Bayes_Project/",
    "title": "Bayes Hierarchical Models",
    "description": "This project was conducted for the capstone course: STAT 454 Bayesian Statistics. We utilized Bayes Methods to predict future company earnings!",
    "author": [
      {
        "name": "Nicholas Di, Nolan Meyer, and Duc Ngo",
        "url": {}
      }
    ],
    "date": "2021-12-11",
    "categories": [],
    "contents": "\nProject\nWelcome to our STAT 454: Bayesian Statistics capstone project. All of\nus group members have an interest in and connections to the financial\nworld, whether that be through our majors or internships, which led us\ntoward this topic. Financial information, like stock market prices, are\nknown to be notoriously hard to predict. We wanted to take a Bayesian\napproach to try and tackle a similar situation: predicting the future\nearnings of S&P 500 companies. In this project we seek to model\nfuture earnings using other financial information about a company, like\nprevious earnings and sales. We explore a few Bayesian hierarchical\nmodels, as well as a SARIMA model using the bayesforcast package to try\nand identify one that can provide insight and better predictions for\nfuture company’s earnings.\nProject\nLink\n\n\n\n",
    "preview": "https://wealthface.com/blog/wp-content/uploads/2021/05/SP500.jpg",
    "last_modified": "2022-05-31T14:19:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/YouTube_Viz/",
    "title": "YouTube Shiny App",
    "description": "Interactive visualization for youtube viewing data!",
    "author": [],
    "date": "2021-07-01",
    "categories": [],
    "contents": "\n\n\nlibrary(shiny)\nlibrary(tidyverse)\ndata <- read.csv(\"~/Documents/Intro To Data Science/USvideos.txt\")\ndata_cleaned<- data %>%\n  filter(category_id %in% c(\"1\", \"2\", \"10\", \"15\", \"17\", \"19\", \"22\", \"23\", \"24\", \"26\", \"27\", \"28\")) %>%\n  mutate(time_string = toString(publish_time)) %>%\n  mutate(day = substr(time_string, 9, 10)) %>%\n  mutate(likes = as.numeric(likes, na.rm = TRUE)) %>%\n  mutate(dislikes = as.numeric(dislikes, na.rm = TRUE))%>%\n  mutate(like_dislike_ratio = likes / (dislikes + 1))%>%\n  select(title, category_id, tags, views, likes, comment_count, dislikes, like_dislike_ratio)\n\ndata_cleaned <- data_cleaned %>%\n  mutate(category = ifelse(category_id == \"1\", \"Autos & Vehicles\", ifelse(category_id == \"2\", \"Music\", ifelse(category_id == \"10\", \"Comedy\", \n                                                                                                              ifelse(category_id == \"15\", \"Science & Technology\", ifelse(category_id == \"17\", \"Science & Technology\", \n                                                                                                                                                                         ifelse(category_id == \"17\", \"Movies\", ifelse(category_id == \"19\", \"Action/Adventure\", ifelse(category_id == \"22\", \"Documentary\",\n                                                                                                                                                                                                                                                                      ifelse(category_id == \"23\", \"Drama\", ifelse(category_id == \"24\", \"Family\", ifelse(category_id == \"26\", \"Horror\", \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ifelse(category_id == \"27\", \"Sci-Fi/Fantasy\", \"Thriller\")))))))))))))\ngraphdata <- data_cleaned %>%\n  mutate(category_id_c = as.character(category)) %>% \n  group_by(category_id_c) %>% \n  mutate(views = as.numeric(views, na.rm = TRUE)) %>%\n  mutate(comment_count = as.numeric(comment_count, na.rm = TRUE)) %>%\n  summarise(totallikes = sum(likes, na.rm = TRUE), totaldislikes = sum(dislikes, na.rm = TRUE), \n            totalviews = sum(views, na.rm = TRUE), totalcomment = sum(comment_count, na.rm = TRUE),\n            totalvideos = n(), likes_per_video = totallikes/totalvideos, \n            dislikes_per_video = totaldislikes/ totalvideos, views_per_video = totalviews/totalvideos,\n            comment_count_per_video = totalcomment/totalvideos, like_dislike_ratio_per_video = mean(like_dislike_ratio, na.rm = TRUE))\n\nui <- fluidPage(\n  \n  title = \"Youtube Recommendation\",\n  \n  sidebarLayout(\n    sidebarPanel(\n      conditionalPanel(\n        'input.dataset === \"Users Manual\"',\n        helpText(\"Here is the user's manual\")\n      ),\n      conditionalPanel(\n        'input.dataset === \"Likes\"',\n        selectInput(\"category1\",\n                    \"Category\",\n                    choices = list(\"Autos & Vehicles\" = \"1\", \"Music\" = \"2\", \"Comedy\" = \"10\", \"Science & Technology\" = \"15\", \"Movies\" = \"17\", \"Action/Adventure\" = \"19\", \n                                   \"Documentary\" = \"22\", \"Drama\" = \"23\", \"Family\" = \"24\", \"Horror\" = \"26\", \"Sci-Fi/Fantasy\" = \"27\", \"Thriller\" = \"28\")),\n        checkboxGroupInput(\"show_vars1\",\"Columns to show:\", \n                           list(\"Title\" = \"title\", \"Views\" = \"views\", \"Likes\" = \"likes\", \n                                \"Comment Count\" = \"comment_count\", \"Dislikes\" = \"dislikes\", \"Like Dislike Ratio\" = \"like_dislike_ratio\"), \n                           selected = list(\"title\", \"likes\"))\n      ),\n      conditionalPanel(\n        'input.dataset === \"Dislikes\"',\n        selectInput(\"category2\",\n                    \"Category\",\n                    choices = list(\"Autos & Vehicles\" = \"1\", \"Music\" = \"2\", \"Comedy\" = \"10\", \"Science & Technology\" = \"15\", \"Movies\" = \"17\", \"Action/Adventure\" = \"19\", \n                                   \"Documentary\" = \"22\", \"Drama\" = \"23\", \"Family\" = \"24\", \"Horror\" = \"26\", \"Sci-Fi/Fantasy\" = \"27\", \"Thriller\" = \"28\")),\n        checkboxGroupInput(\"show_vars2\",\"Columns to show:\", \n                          list(\"Title\" = \"title\", \"Views\" = \"views\", \"Likes\" = \"likes\", \n                               \"Comment Count\" = \"comment_count\", \"Dislikes\" = \"dislikes\", \"Like Dislike Ratio\" = \"like_dislike_ratio\"), \n                          selected = list(\"title\", \"dislikes\"))\n      ),\n      conditionalPanel(\n        'input.dataset === \"Comment Count\"',\n        selectInput(\"category3\",\n                    \"Category\",\n                    choices = list(\"Autos & Vehicles\" = \"1\", \"Music\" = \"2\", \"Comedy\" = \"10\", \"Science & Technology\" = \"15\", \"Movies\" = \"17\", \"Action/Adventure\" = \"19\", \n                                   \"Documentary\" = \"22\", \"Drama\" = \"23\", \"Family\" = \"24\", \"Horror\" = \"26\", \"Sci-Fi/Fantasy\" = \"27\", \"Thriller\" = \"28\")),\n        checkboxGroupInput(\"show_vars3\",\"Columns to show:\", \n                           list(\"Title\" = \"title\", \"Views\" = \"views\", \"Likes\" = \"likes\", \n                                \"Comment Count\" = \"comment_count\", \"Dislikes\" = \"dislikes\", \"Like Dislike Ratio\" = \"like_dislike_ratio\"), \n                           selected = list(\"title\", \"comment_count\"))\n      ),\n      conditionalPanel(\n        'input.dataset === \"Views\"',\n        selectInput(\"category4\",\n                    \"Category\",\n                    choices = list(\"Autos & Vehicles\" = \"1\", \"Music\" = \"2\", \"Comedy\" = \"10\", \"Science & Technology\" = \"15\", \"Movies\" = \"17\", \"Action/Adventure\" = \"19\", \n                                   \"Documentary\" = \"22\", \"Drama\" = \"23\", \"Family\" = \"24\", \"Horror\" = \"26\", \"Sci-Fi/Fantasy\" = \"27\", \"Thriller\" = \"28\")),\n        checkboxGroupInput(\"show_vars4\",\"Columns to show:\", \n                           list(\"Title\" = \"title\", \"Views\" = \"views\", \"Likes\" = \"likes\", \n                                \"Comment Count\" = \"comment_count\", \"Dislikes\" = \"dislikes\", \"Like Dislike Ratio\" = \"like_dislike_ratio\"), \n                           selected = list(\"title\", \"views\"))\n      ),\n      conditionalPanel(\n        'input.dataset === \"Like Dislike Ratio\"',\n        selectInput(\"category5\",\n                    \"Category\",\n                    choices = list(\"Autos & Vehicles\" = \"1\", \"Music\" = \"2\", \"Comedy\" = \"10\", \"Science & Technology\" = \"15\", \"Movies\" = \"17\", \"Action/Adventure\" = \"19\", \n                                   \"Documentary\" = \"22\", \"Drama\" = \"23\", \"Family\" = \"24\", \"Horror\" = \"26\", \"Sci-Fi/Fantasy\" = \"27\", \"Thriller\" = \"28\")),\n        checkboxGroupInput(\"show_vars5\",\"Columns to show:\", \n                           list(\"Title\" = \"title\", \"Views\" = \"views\", \"Likes\" = \"likes\", \n                                \"Comment Count\" = \"comment_count\", \"Dislikes\" = \"dislikes\", \"Like Dislike Ratio\" = \"like_dislike_ratio\"), \n                           selected = list(\"title\", \"like_dislike_ratio\"))\n      ),\n      conditionalPanel(\n        'input.dataset === \"Ranking Plot\"',\n        selectInput(\"Vars\",\n                    \"Ranking Categories of Video according to:\",\n                    choices = list(\"Views Per Video\" = \"views\", \"Likes Per Video\" = \"likes\",\n                                   \"Comment Count Per Video\" = \"comment_count\", \"Dislikes Per Video\" = \"dislikes\", \"Average Like Dislike Ratio\" = \"like_dislike_ratio\"))\n      ),\n      \n      conditionalPanel(\n        'input.dataset === \"Distribution of Likes by Categories\"',\n        checkboxGroupInput(\"Categoreis\",\"Categories to be included in the Visulization\",\n                           list(\"Autos & Vehicles\" = \"Autos & Vehicles\", \"Music\" = \"Music\", \"Comedy\" = \"Comedy\", \"Science & Technology\" = \"Science & Technology\", \"Movies\" = \"Movies\", \"Action/Adventure\" = \"Action/Adventure\",\n                                \"Documentary\" = \"Documentary\", \"Drama\" = \"Drama\", \"Family\" = \"Family\", \"Horror\" = \"Horror\", \"Sci-Fi/Fantasy\" = \"Sci-Fi/Fantasy\", \"Thriller\" = \"Thriller\"),\n                           selected = (\"Autos & Vehicles\")\n        )\n      )\n    ),\n    \n    mainPanel(\n      tabsetPanel(\n        id = 'dataset',\n        tabPanel(\"Users Manual\", tags$h2(\"User's Manual\"), htmlOutput(\"manual\")),\n        tabPanel(\"Likes\", DT::dataTableOutput(\"mytable_likes\")),\n        tabPanel(\"Dislikes\", DT::dataTableOutput(\"mytable_dislikes\")),\n        tabPanel(\"Comment Count\", DT::dataTableOutput(\"mytable_comment_count\")),\n        tabPanel(\"Views\", DT::dataTableOutput(\"mytable_views\")),\n        tabPanel(\"Like Dislike Ratio\", DT::dataTableOutput(\"mytable_like_dislike_ratio\")),\n        tabPanel(\"Ranking Plot\", plotOutput(outputId = \"Rankplot\")),\n        tabPanel(\"Distribution of Likes by Categories\", plotOutput(outputId = \"Boxplot\"))\n        \n      )\n    )\n  )\n)\n\n\n\nserver <- function(input, output) {\n    \n    output$mytable_likes <- DT::renderDataTable({\n      data_cleaned %>%\n        mutate(likes = as.numeric(likes)) %>%\n        filter(category_id == input$category1) %>%\n        arrange(desc(likes)) %>%\n        select(input$show_vars1)\n    })\n    \n    output$mytable_dislikes <- DT::renderDataTable({\n      data_cleaned %>%\n        mutate(dislikes = as.numeric(dislikes)) %>%\n        filter(category_id == input$category2) %>%\n        arrange(dislikes) %>%\n        select(input$show_vars2)\n    })\n    \n    output$mytable_comment_count <- DT::renderDataTable({\n      data_cleaned %>%\n        mutate(comment_count = as.numeric(comment_count)) %>%\n        filter(category_id == input$category3) %>%\n        arrange(desc(comment_count)) %>%\n        select(input$show_vars3)\n    })\n    \n    output$mytable_views <- DT::renderDataTable({\n      data_cleaned %>%\n        mutate(views = as.numeric(views)) %>%\n        filter(category_id == input$category4) %>%\n        arrange(desc(views)) %>%\n        select(input$show_vars4)\n    })\n    \n    output$mytable_like_dislike_ratio <- DT::renderDataTable({\n      data_cleaned %>%\n        mutate(like_dislike_ratio = as.numeric(like_dislike_ratio))%>%\n        filter(category_id == input$category5)%>%\n        arrange(desc(like_dislike_ratio)) %>%\n        select(input$show_vars5)\n    })\n    \n    output$manual <- renderText({\n      paste(\n        \"<p>Welcome to our shiny app! This is a fairly self-explanatory app. First off, start off my selecting the categories in which you would like to browse the data in. To select a category, clock on the drop-down box and scroll up or down until you see what you want to explore. Simply click on the option you’d like. We then select what results we would like to view. Check or uncheck what columns we would like to show in the results.<\/p>\n         <p> Then, we will be able to rank the videos by Likes, Dislikes, Comment Count, Views, and Like-Dislike Ratio. By selecting one, the app will bring you to a page where you can search by keywords and sort by likes and alphabetical order of the title. You may also choose to see how many entries you view per page. To go to the next page, click on the numbers or next.<\/p>\n        \"\n      )\n    })\n    \n    output$Rankplot <- renderPlot(\n      graphdata %>%\n        ggplot(aes(y = fct_reorder(category_id_c,\n                                   eval(as.name(paste(input$Vars,\"_per_video\", sep=\"\")))\n                                   ),\n                   x = eval(as.name(paste(input$Vars, \"_per_video\", sep=\"\"))), \n                   fill = category_id_c)) +\n        geom_bar(stat = 'identity') +\n        labs(y = \"Category of Music\",\n             title = paste(\"Ranking of Category by\",input$Vars),\n             x = \"\"\n             )+\n        theme(legend.position = \"none\")\n    )\n\n    output$Boxplot <- renderPlot(\n      data_cleaned %>%\n        filter(category == input$Categoreis) %>%\n        mutate(category_c = as.character(category)) %>%\n        group_by(category_c) %>%\n        summarize(category_c, likes) %>%\n        ggplot(aes(y = reorder(category_c,likes,median), x=likes, fill = category_c)) +\n        geom_boxplot()+\n        labs(y = \"Category ID\",\n             x = \"Likes\",\n             title = \"Distribution by Category\")+\n        theme_minimal()+\n        xlim(0,100000)+\n        theme(legend.position = \"none\")\n    )\n    \n}\n\nshinyApp(ui = ui, server = server)\n\n\nShiny applications not supported in static R Markdown documents\n\n\n\n\n",
    "preview": "https://cdn.mos.cms.futurecdn.net/8gzcr6RpGStvZFA2qRt4v6.jpg",
    "last_modified": "2022-05-23T13:52:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/Spotify_Project/",
    "title": "Machine Learning Semester Project",
    "description": "This project was conducted for the course: STAT 253 Statistical Machine Learning. We analyzed data from Spotify with the goal of predicting song popularity!",
    "author": [
      {
        "name": "Nicholas Di and Amy Xu",
        "url": {}
      }
    ],
    "date": "2020-05-01",
    "categories": [],
    "contents": "\nData Context\nWe selected the dataset “Top Spotify songs from 2010-2019 - BY YEAR”\nfrom Kaggle, which consists of the top songs by year in the world on\nSpotify, and the data is based on Billboard. The dataset contains 13\nvariables to be explored, including information about the songs such as\nthe song’s title, the song’s artist, the genre of the track, the year in\nwhich the song was in the Billboard rankings, its duration and\nacousticness. There are also variables describing the music, such as\nbeats per minute (which characterizes the tempo of the song), energy\n(the higher the value, the more energetic the song is), danceability\n(the higher the value, the easier it is to dance to this song), loudness\n(measured in dB), liveness (the higher the value, the more likely the\nsong is a live recording), valence (the higher the value, the more\npositive the mood is for the song), and speechiness (the higher the\nvalue, the more spoken words the song contains). The outcome variable in\nthis dataset is popularity, where the higher the value, the more popular\nthe song is.\nThe data were extracted from: http://organizeyourmusic.playlistmachinery.com/, and the\ndataset was constructed by Leonardo Henrique, updated in 2020. He was\ninterested in what we could know about the specific music genre based on\nthe popularity of the songs, and what elements would contribute to this\npopularity.\nResearch questions\nRegression: How can we predict the energy level of a song based on\nall other predictors?\nClassification: How can we predict whether the song is amongst the\nmost popular based on all other predictors?\n\n\n# Load data and required packages\nlibrary(caret)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ISLR)\nlibrary(splines)\nlibrary(caret)\nlibrary(stats)\nlibrary(lattice)\nlibrary(leaps)\nlibrary(gam)\ntop_spotify <- read_csv('https://www.dropbox.com/s/fi22whryueo4q85/top10s.csv?dl=1')\n\n\n\n\n\n# Any code to clean the data\ntop_spotify_new <- top_spotify %>% select(-artist,-'top genre',-title,-...1)\n#There seems to be an outlier \ntop_spotify_new <- filter(top_spotify_new, bpm > 1)\n\n\n\nInitial\ninvestigation 1: ignoring nonlinearity (for now)\nWe ordinary least squares (OLS) regression, forward and/or backward\nselection, and LASSO to build initial models for our quantitative\noutcome as a function of the predictors of interest.\nOLS Model\nFit the ordinary least squares (OLS) regression model:\n\n\nOLS <- lm(nrgy ~ year + acous + bpm + pop + dnce + live + spch + dur, data = top_spotify_new)\nsummary(OLS)\n\n\n\nCall:\nlm(formula = nrgy ~ year + acous + bpm + pop + dnce + live + \n    spch + dur, data = top_spotify_new)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.223  -8.030   1.092   8.540  31.239 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.347e+03  4.228e+02   5.552 4.26e-08 ***\nyear        -1.119e+00  2.097e-01  -5.337 1.34e-07 ***\nacous       -4.123e-01  2.611e-02 -15.790  < 2e-16 ***\nbpm         -3.427e-03  2.178e-02  -0.157  0.87504    \npop         -4.448e-02  3.713e-02  -1.198  0.23131    \ndnce         1.833e-03  4.190e-02   0.044  0.96511    \nlive         1.239e-01  4.012e-02   3.087  0.00212 ** \nspch         2.046e-01  6.916e-02   2.958  0.00322 ** \ndur         -7.281e-02  1.568e-02  -4.644 4.20e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.51 on 593 degrees of freedom\nMultiple R-squared:  0.402, Adjusted R-squared:  0.3939 \nF-statistic: 49.83 on 8 and 593 DF,  p-value: < 2.2e-16\n\nset.seed(253)\nOLS_cv <- train(\n    nrgy ~ year + acous + bpm + pop + dnce + live + spch + dur,\n    data = top_spotify_new,\n    method = \"lm\",\n    trControl = trainControl(method = \"cv\", number = 9),\n    na.action = na.omit)\n\n\n\nBackward Stepwise Selection\nModel\nFit the Backward Stepwise Selection model:\n\n\nfull_model <- lm(nrgy ~ ., data = top_spotify_new)\nsummary(full_model)\n\n\n\nCall:\nlm(formula = nrgy ~ ., data = top_spotify_new)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-28.7877  -6.2817   0.6035   6.8994  27.0338 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 812.704006 344.864487   2.357 0.018769 *  \nyear         -0.350636   0.171100  -2.049 0.040874 *  \nbpm          -0.004972   0.017149  -0.290 0.771957    \ndnce         -0.106738   0.037418  -2.853 0.004488 ** \ndB            4.479083   0.268874  16.659  < 2e-16 ***\nlive          0.103313   0.031586   3.271 0.001135 ** \nval           0.113503   0.022958   4.944 9.99e-07 ***\ndur          -0.017890   0.012827  -1.395 0.163629    \nacous        -0.289294   0.021547 -13.426  < 2e-16 ***\nspch          0.206303   0.055593   3.711 0.000226 ***\npop          -0.074613   0.029247  -2.551 0.010988 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.84 on 591 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.625 \nF-statistic: 101.2 on 10 and 591 DF,  p-value: < 2.2e-16\n\nset.seed(253)\nback_step_mod <- train(\n    nrgy ~ .,\n    data = top_spotify_new,\n    method = 'leapBackward',\n    tuneGrid = data.frame(nvmax = 1:10),\n    trControl = trainControl(method = 'cv',number = 9),\n    metric = 'MAE',\n    na.action = na.omit\n)\n\n\n\nForward Stepwise Selection\nModel\nFit the Forward Stepwise Selection model:\n\n\nset.seed(253)\nfor_step_mod <- train(\n    nrgy ~ .,\n    data = top_spotify_new,\n    method = 'leapForward',\n    tuneGrid = data.frame(nvmax = 1:10),\n    trControl = trainControl(method = 'cv',number = 9),\n    metric = 'MAE',\n    na.action = na.omit\n)\n\n\n\nLASSO Model\nFit the LASSO model:\n\n\nset.seed(253)\nlasso_mod <- train(\n    nrgy ~ .,\n    data = top_spotify_new,\n    method = \"glmnet\",\n    tuneGrid = data.frame(alpha = 1, lambda = seq(0, 10, length.out = 100)),\n    trControl = trainControl(method = \"cv\", number = 9, selectionFunction = 'oneSE'),\n    metric = \"MAE\",\n    na.action = na.omit)\n\n\n\nCompare performances\nof different models:\nEstimate test performance of the models from these different methods.\nReport and interpret (with units) these estimates along with a measure\nof uncertainty in the estimate (SD is most readily available from\ncaret).\nExamine OLS model outputs:\n\n\nsummary(OLS_cv)\n\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.223  -8.030   1.092   8.540  31.239 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.347e+03  4.228e+02   5.552 4.26e-08 ***\nyear        -1.119e+00  2.097e-01  -5.337 1.34e-07 ***\nacous       -4.123e-01  2.611e-02 -15.790  < 2e-16 ***\nbpm         -3.427e-03  2.178e-02  -0.157  0.87504    \npop         -4.448e-02  3.713e-02  -1.198  0.23131    \ndnce         1.833e-03  4.190e-02   0.044  0.96511    \nlive         1.239e-01  4.012e-02   3.087  0.00212 ** \nspch         2.046e-01  6.916e-02   2.958  0.00322 ** \ndur         -7.281e-02  1.568e-02  -4.644 4.20e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.51 on 593 degrees of freedom\nMultiple R-squared:  0.402, Adjusted R-squared:  0.3939 \nF-statistic: 49.83 on 8 and 593 DF,  p-value: < 2.2e-16\n\nOLS_cv$results\n\n\n  intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD   MAESD\n1      TRUE 12.62445 0.3880631 10.13206 1.140868  0.1381483 0.83748\n\nOn average, we’re off in top song energy predictions by about\n10.13206 points.\nResidual plot for OLS model:\n\n\nOLS_mod_output <- broom::augment(OLS, newdata = top_spotify_new)\n\nggplot(OLS_mod_output, aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\")\n\n\n\n\nExamine Backward Stepwise Selection model output:\n\n\nsummary(back_step_mod)\n\n\nSubset selection object\n10 Variables  (and intercept)\n      Forced in Forced out\nyear      FALSE      FALSE\nbpm       FALSE      FALSE\ndnce      FALSE      FALSE\ndB        FALSE      FALSE\nlive      FALSE      FALSE\nval       FALSE      FALSE\ndur       FALSE      FALSE\nacous     FALSE      FALSE\nspch      FALSE      FALSE\npop       FALSE      FALSE\n1 subsets of each size up to 9\nSelection Algorithm: backward\n         year bpm dnce dB  live val dur acous spch pop\n1  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"   \" \"  \" \"\n2  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \"*\"   \" \"  \" \"\n3  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \"*\"   \"*\"  \" \"\n4  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \"*\" \" \" \"*\"   \"*\"  \" \"\n5  ( 1 ) \" \"  \" \" \" \"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \" \"\n6  ( 1 ) \" \"  \" \" \"*\"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \" \"\n7  ( 1 ) \" \"  \" \" \"*\"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \"*\"\n8  ( 1 ) \"*\"  \" \" \"*\"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \"*\"\n9  ( 1 ) \"*\"  \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\" \"*\"   \"*\"  \"*\"\n\nplot(back_step_mod)\n\n\n\nback_step_mod$bestTune\n\n\n  nvmax\n9     9\n\nWe chose the model with 6 predictors. Although all 10 yields better\nmodel metrics, we believe that we will run into problems of\nover-fitting.\nOn average, we’re off in top song energy predictions by about 8.286\npercentage points, if we use the model with 6 predictors.\n\n\ncoef(back_step_mod$finalModel, id = 6)\n\n\n(Intercept)        dnce          dB        live         val \n 97.4672886  -0.1228588   4.5587752   0.1128596   0.1264183 \n      acous        spch \n -0.2930557   0.2006552 \n\nback_step_mod$results %>% filter(nvmax==6)\n\n\n  nvmax     RMSE  Rsquared      MAE    RMSESD RsquaredSD    MAESD\n1     6 10.18581 0.5910122 8.286481 0.9244708  0.1070956 0.825095\n\nback_step_mod_eq <- lm(nrgy ~ dnce + dB + live + val + acous + spch, data =top_spotify_new)\n\n\n\nResidual plot for Backward Step-wise Selection model:\n\n\nback_step_mod_out <- top_spotify_new %>%\n    mutate(\n        fitted = predict(back_step_mod_eq, newdata = top_spotify_new),\n        resid = nrgy - fitted\n    )\n\nggplot(back_step_mod_out, aes(x = fitted, y = resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\") +\n    theme_classic() +\n    labs(x = \"Fitted values\", y = \"Residuals\")\n\n\n\n\nFor Loop looking at all predictors in the Backward Stepwise Selection\nmodel:\n\n\npredictors <- setdiff(colnames(top_spotify_new), c(\"year\",\"bpm\",\"nrgy\",\"dur\",\"spch\"))\nfor (pred in predictors) {\n    p <- ggplot(back_step_mod_out, aes(x = .data[[pred]], y = resid)) +\n        geom_point() +\n        geom_smooth() +\n        geom_hline(yintercept = 0, color = \"red\") +\n        theme_classic() +\n        labs(x = pred, y = \"Residuals\")\n    print(p)\n}\n\n\n\n\nExamine Forward Stepwise Selection model output:\n\n\nsummary(for_step_mod)\n\n\nSubset selection object\n10 Variables  (and intercept)\n      Forced in Forced out\nyear      FALSE      FALSE\nbpm       FALSE      FALSE\ndnce      FALSE      FALSE\ndB        FALSE      FALSE\nlive      FALSE      FALSE\nval       FALSE      FALSE\ndur       FALSE      FALSE\nacous     FALSE      FALSE\nspch      FALSE      FALSE\npop       FALSE      FALSE\n1 subsets of each size up to 9\nSelection Algorithm: forward\n         year bpm dnce dB  live val dur acous spch pop\n1  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \" \"   \" \"  \" \"\n2  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \"*\"   \" \"  \" \"\n3  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \" \" \" \" \"*\"   \"*\"  \" \"\n4  ( 1 ) \" \"  \" \" \" \"  \"*\" \" \"  \"*\" \" \" \"*\"   \"*\"  \" \"\n5  ( 1 ) \" \"  \" \" \" \"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \" \"\n6  ( 1 ) \" \"  \" \" \"*\"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \" \"\n7  ( 1 ) \" \"  \" \" \"*\"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \"*\"\n8  ( 1 ) \"*\"  \" \" \"*\"  \"*\" \"*\"  \"*\" \" \" \"*\"   \"*\"  \"*\"\n9  ( 1 ) \"*\"  \" \" \"*\"  \"*\" \"*\"  \"*\" \"*\" \"*\"   \"*\"  \"*\"\n\nplot(for_step_mod)\n\n\n\nfor_step_mod$results\n\n\n   nvmax      RMSE  Rsquared      MAE    RMSESD RsquaredSD     MAESD\n1      1 11.993192 0.4404595 9.440092 1.1663410 0.08326330 0.7602086\n2      2 10.477550 0.5683678 8.533998 0.6892741 0.09392739 0.6832649\n3      3 10.421451 0.5699856 8.442041 0.8481219 0.10845490 0.7918324\n4      4 10.333804 0.5765861 8.412541 0.8391433 0.11095779 0.7986408\n5      5 10.260625 0.5846211 8.383712 0.8495003 0.11238655 0.7669223\n6      6 10.142928 0.5941810 8.277152 0.8950607 0.10739226 0.8186073\n7      7  9.950075 0.6082421 8.067531 0.9987578 0.11136222 0.8886527\n8      8  9.940663 0.6092391 8.072114 0.9905555 0.10712761 0.9066449\n9      9  9.902507 0.6120420 8.049632 0.9827727 0.10630862 0.9021970\n10    10  9.915936 0.6110377 8.062302 0.9829506 0.10702524 0.9087888\n\nUsing Forward selection, we chose the model with 5 predictors. The\nfive predictors being acous, dB, live, spch, and val.\nOn average, we’re off in top song energy predictions by about\n8.383712 percentage points.\n\n\ncoef(for_step_mod$finalModel, id = 5)\n\n\n(Intercept)          dB        live         val       acous \n91.50592402  4.65576857  0.11831750  0.09041016 -0.28031999 \n       spch \n 0.22151861 \n\nfor_step_mod$results %>% filter(nvmax==5)\n\n\n  nvmax     RMSE  Rsquared      MAE    RMSESD RsquaredSD     MAESD\n1     5 10.26063 0.5846211 8.383712 0.8495003  0.1123866 0.7669223\n\nfor_step_mod_eq <- lm(nrgy ~ acous + dB + val + spch + live, data =top_spotify_new)\n\n\n\nResidual plot for Forward Stepwise Selection model:\n\n\nfor_step_mod_out <- top_spotify_new %>%\n    mutate(\n        fitted = predict(for_step_mod_eq, newdata = top_spotify_new),\n        resid = nrgy - fitted\n    )\n\nggplot(for_step_mod_out, aes(x = fitted, y = resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\") +\n    theme_classic() +\n    labs(x = \"Fitted values\", y = \"Residuals\")\n\n\n\n\nFor Loop looking at all predictors in the Forward Stepwise Selection\nmodel:\n\n\npredictors <- setdiff(colnames(top_spotify_new), c(\"year\",\"bpm\",\"nrgy\",\"dur\",\"pop\", \"dnce\"))\nfor (pred in predictors) {\n    p <- ggplot(for_step_mod_out, aes(x = .data[[pred]], y = resid)) +\n        geom_point() +\n        geom_smooth() +\n        geom_hline(yintercept = 0, color = \"red\") +\n        theme_classic() +\n        labs(x = pred, y = \"Residuals\")\n    print(p)\n}\n\n\n\n\nExamine LASSO model output:\n\n\nplot(lasso_mod$finalModel, xvar = \"lambda\", label = TRUE, col = rainbow(20))\n\n\n\nlasso_mod$bestTune\n\n\n   alpha   lambda\n11     1 1.010101\n\n# lasso_mod$results\nrownames(lasso_mod$finalModel$beta)[c(4,8)]\n\n\n[1] \"dB\"    \"acous\"\n\nWe chose a lambda value of 1.010101, dB and acous seem to be two of\nthe strongest/persistent predictors when it comes to energy-level.\n\n\ncoef(lasso_mod$finalModel, 1.010101)\n\n\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept) 427.22094252\nyear         -0.16551620\nbpm           .         \ndnce          .         \ndB            4.25384058\nlive          0.05257947\nval           0.06575686\ndur           .         \nacous        -0.25154488\nspch          0.10397394\npop          -0.02164891\n\nlasso_mod$results[11,]\n\n\n   alpha   lambda     RMSE Rsquared     MAE    RMSESD RsquaredSD\n11     1 1.010101 10.17634 0.597547 8.32769 0.8552498  0.1057273\n       MAESD\n11 0.7932597\n\nOn average, we’re off in top song energy predictions by about 8.32769\npercentage points using LASSO with a lambda of 1.010101.\nResidual plot for LASSO model:\n\n\nlasso_mod_out <- top_spotify_new %>%\n    mutate(\n        fitted = predict(lasso_mod, newdata = top_spotify_new),\n        resid = nrgy - fitted)\nggplot(lasso_mod_out, aes(x = fitted, y = resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\") +\n    theme_classic() +\n    labs(x = \"Fitted values\", y = \"Residuals\")\n\n\n\n\nThere does not seem to be any noticable patterns of over and\nunderpredicting here!\nFor Loop for LASSO model:\n\n\npredictors <- setdiff(colnames(top_spotify_new), \"Top Spotify\")\nfor (pred in predictors) {\n    p <- ggplot(lasso_mod_out, aes(x = .data[[pred]], y = resid)) +\n        geom_point() +\n        geom_smooth() +\n        geom_hline(yintercept = 0, color = \"red\") +\n        theme_classic() +\n        labs(x = pred, y = \"Residuals\")\n    print(p)\n}\n\n\n\n\nModel\nTraining MAE\nMAESD\nOLS Model\n10.13206\n0.83748\nBackward\n8.286481\n0.825095\nForward\n8.383712\n0.7669223\nLASSO\n8.32769\n0.7932597\nComparing the four models, LASSO and Backward Stepwise Selection\nmodels seem to be yielding the best results as the predictions for top\nsong energy level are closest to the test value.\nCompare insights from variable importance analyses from the different\nmethods (stepwise and LASSO, but not OLS). Are there variables for which\nthe methods reach consensus? What insights are expected? Surprising?\nAcross all models, the “top” 3 predictors for song energy level are\nacous (Acousticness- the higher the value the more acoustic the song\nis), dB(Loudness, the higher the value, the louder the song), and\nval(Valence, the higher the value, the more positive mood for the song).\nThis is mostly consistent with our expectation, as when the song is\nlouder, and more positive, the song has a higher energy level. It is a\nbit surprising that when a song is more acoustic, it is less\nenergetic.\nInvestigation 2:\nAccounting for nonlinearity\nUpdate your stepwise selection model(s) and LASSO model to use\nnatural splines for the quantitative predictors:\n\n\nggplot(top_spotify_new, aes(x = val, y = nrgy)) +\n    geom_point(alpha = 0.25) +\n    geom_smooth(color = \"blue\", se = FALSE) +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\nggplot(top_spotify_new, aes(x = dB, y = nrgy)) +\n    geom_point(alpha = 0.25) +\n    geom_smooth(color = \"blue\", se = FALSE) +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\nggplot(top_spotify_new, aes(x = acous, y = nrgy)) +\n    geom_point(alpha = 0.25) +\n    geom_smooth(color = \"blue\", se = FALSE) +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\nggplot(top_spotify_new, aes(x = year, y = nrgy)) +\n    geom_point(alpha = 0.25) +\n    geom_smooth(color = \"blue\", se = FALSE) +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\nBackward\nStepwise Selection model with natural splines\nUpdate the Backward Stepwise Selection model to use natural splines\nfor the quantitative predictors:\n\n\nset.seed(253)\nback_spline_mod <- train(\n    nrgy ~ ns(acous, 3) + ns(pop, 3) + ns(dnce, 3) + ns(live, 3) + ns(dB, 3) + ns(val, 3),\n    data = top_spotify_new,\n    method = \"leapBackward\",\n    trControl = trainControl(method = \"cv\", number = 9, selectionFunction = \"oneSE\"),\n    metric = \"MAE\",\n    na.action = na.omit\n)\n\n\n\nForward\nStepwise Selection model with natural splines\nUpdate the Forward Stepwise Selection model to use natural splines\nfor the quantitative predictors:\n\n\nset.seed(253)\nfor_spline_mod <- train(\n    nrgy ~ ns(year, 3) + ns(acous, 3) + ns(bpm, 3) + ns(pop, 3) + ns(dnce, 3) + ns(live, 3) + ns(spch, 3) + ns(dur, 3),\n    data = top_spotify_new,\n    method = \"leapForward\",\n    trControl = trainControl(method = \"cv\", number = 9, selectionFunction = \"oneSE\"),\n    metric = \"MAE\",\n    na.action = na.omit\n)\n\n\n\nLASSO model with natural\nsplines\nUpdate the LASSO model to use natural splines for the quantitative\npredictors:\n\n\nset.seed(253)\nLASSO_spline_mod <- train(\n    nrgy ~ ns(year, 3) + ns(acous, 3) + ns(bpm, 3) + ns(pop, 3) + ns(dnce, 3) + ns(live, 3) + ns(spch, 3) + ns(dur, 3),\n    data = top_spotify_new,\n    method = \"glmnet\",\n    tuneGrid = data.frame(alpha = 1, lambda = seq(0, 10, length.out = 100)),\n    trControl = trainControl(method = \"cv\", number = 9, selectionFunction = \"oneSE\"),\n    metric = \"MAE\",\n    na.action = na.omit\n)\n\n\n\nCompare\ninsights from variable importance analyses\nCompare insights from variable importance analyses here and the\ncorresponding results from Investigation 1. Now after having accounted\nfor nonlinearity, have the most relevant predictors changed?\nNote that if some (but not all) of the spline terms are selected in\nthe final models, the whole predictor should be treated as\nselected.\nExamine Backward Stepwise Selection model with natural splines\noutput:\n\n\nsummary(back_spline_mod)\n\n\nSubset selection object\n18 Variables  (and intercept)\n              Forced in Forced out\nns(acous, 3)1     FALSE      FALSE\nns(acous, 3)2     FALSE      FALSE\nns(acous, 3)3     FALSE      FALSE\nns(pop, 3)1       FALSE      FALSE\nns(pop, 3)2       FALSE      FALSE\nns(pop, 3)3       FALSE      FALSE\nns(dnce, 3)1      FALSE      FALSE\nns(dnce, 3)2      FALSE      FALSE\nns(dnce, 3)3      FALSE      FALSE\nns(live, 3)1      FALSE      FALSE\nns(live, 3)2      FALSE      FALSE\nns(live, 3)3      FALSE      FALSE\nns(dB, 3)1        FALSE      FALSE\nns(dB, 3)2        FALSE      FALSE\nns(dB, 3)3        FALSE      FALSE\nns(val, 3)1       FALSE      FALSE\nns(val, 3)2       FALSE      FALSE\nns(val, 3)3       FALSE      FALSE\n1 subsets of each size up to 4\nSelection Algorithm: backward\n         ns(acous, 3)1 ns(acous, 3)2 ns(acous, 3)3 ns(pop, 3)1\n1  ( 1 ) \" \"           \" \"           \" \"           \" \"        \n2  ( 1 ) \" \"           \" \"           \"*\"           \" \"        \n3  ( 1 ) \" \"           \"*\"           \"*\"           \" \"        \n4  ( 1 ) \" \"           \"*\"           \"*\"           \" \"        \n         ns(pop, 3)2 ns(pop, 3)3 ns(dnce, 3)1 ns(dnce, 3)2\n1  ( 1 ) \" \"         \" \"         \" \"          \" \"         \n2  ( 1 ) \" \"         \" \"         \" \"          \" \"         \n3  ( 1 ) \" \"         \" \"         \" \"          \" \"         \n4  ( 1 ) \" \"         \" \"         \" \"          \" \"         \n         ns(dnce, 3)3 ns(live, 3)1 ns(live, 3)2 ns(live, 3)3\n1  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n2  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n3  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n4  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n         ns(dB, 3)1 ns(dB, 3)2 ns(dB, 3)3 ns(val, 3)1 ns(val, 3)2\n1  ( 1 ) \" \"        \" \"        \"*\"        \" \"         \" \"        \n2  ( 1 ) \" \"        \" \"        \"*\"        \" \"         \" \"        \n3  ( 1 ) \" \"        \" \"        \"*\"        \" \"         \" \"        \n4  ( 1 ) \"*\"        \" \"        \"*\"        \" \"         \" \"        \n         ns(val, 3)3\n1  ( 1 ) \" \"        \n2  ( 1 ) \" \"        \n3  ( 1 ) \" \"        \n4  ( 1 ) \" \"        \n\nplot(back_spline_mod)\n\n\n\nback_spline_mod$bestTune\n\n\n  nvmax\n3     4\n\nback_spline_mod$results\n\n\n  nvmax     RMSE  Rsquared       MAE    RMSESD RsquaredSD     MAESD\n1     2 12.94073 0.3520225 10.551403 1.0723596 0.09809830 0.7221742\n2     3 11.21314 0.5103530  9.152245 0.9208166 0.07104034 0.7258275\n3     4 10.28440 0.5900269  8.392090 0.8083434 0.09138549 0.7327469\n\nAccording to the Backward Stepwise Selection model with natural\nsplines, the top predictors for song energy level are acous and dB.\nExamine Forward Stepwise Selection model output:\n\n\nsummary(for_spline_mod)\n\n\nSubset selection object\n24 Variables  (and intercept)\n              Forced in Forced out\nns(year, 3)1      FALSE      FALSE\nns(year, 3)2      FALSE      FALSE\nns(year, 3)3      FALSE      FALSE\nns(acous, 3)1     FALSE      FALSE\nns(acous, 3)2     FALSE      FALSE\nns(acous, 3)3     FALSE      FALSE\nns(bpm, 3)1       FALSE      FALSE\nns(bpm, 3)2       FALSE      FALSE\nns(bpm, 3)3       FALSE      FALSE\nns(pop, 3)1       FALSE      FALSE\nns(pop, 3)2       FALSE      FALSE\nns(pop, 3)3       FALSE      FALSE\nns(dnce, 3)1      FALSE      FALSE\nns(dnce, 3)2      FALSE      FALSE\nns(dnce, 3)3      FALSE      FALSE\nns(live, 3)1      FALSE      FALSE\nns(live, 3)2      FALSE      FALSE\nns(live, 3)3      FALSE      FALSE\nns(spch, 3)1      FALSE      FALSE\nns(spch, 3)2      FALSE      FALSE\nns(spch, 3)3      FALSE      FALSE\nns(dur, 3)1       FALSE      FALSE\nns(dur, 3)2       FALSE      FALSE\nns(dur, 3)3       FALSE      FALSE\n1 subsets of each size up to 3\nSelection Algorithm: forward\n         ns(year, 3)1 ns(year, 3)2 ns(year, 3)3 ns(acous, 3)1\n1  ( 1 ) \" \"          \" \"          \" \"          \" \"          \n2  ( 1 ) \" \"          \" \"          \" \"          \" \"          \n3  ( 1 ) \" \"          \" \"          \" \"          \" \"          \n         ns(acous, 3)2 ns(acous, 3)3 ns(bpm, 3)1 ns(bpm, 3)2\n1  ( 1 ) \" \"           \"*\"           \" \"         \" \"        \n2  ( 1 ) \"*\"           \"*\"           \" \"         \" \"        \n3  ( 1 ) \"*\"           \"*\"           \" \"         \" \"        \n         ns(bpm, 3)3 ns(pop, 3)1 ns(pop, 3)2 ns(pop, 3)3 ns(dnce, 3)1\n1  ( 1 ) \" \"         \" \"         \" \"         \" \"         \" \"         \n2  ( 1 ) \" \"         \" \"         \" \"         \" \"         \" \"         \n3  ( 1 ) \" \"         \" \"         \" \"         \" \"         \" \"         \n         ns(dnce, 3)2 ns(dnce, 3)3 ns(live, 3)1 ns(live, 3)2\n1  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n2  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n3  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n         ns(live, 3)3 ns(spch, 3)1 ns(spch, 3)2 ns(spch, 3)3\n1  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n2  ( 1 ) \" \"          \" \"          \" \"          \" \"         \n3  ( 1 ) \" \"          \" \"          \"*\"          \" \"         \n         ns(dur, 3)1 ns(dur, 3)2 ns(dur, 3)3\n1  ( 1 ) \" \"         \" \"         \" \"        \n2  ( 1 ) \" \"         \" \"         \" \"        \n3  ( 1 ) \" \"         \" \"         \" \"        \n\nplot(for_spline_mod)\n\n\n\nfor_spline_mod$bestTune\n\n\n  nvmax\n2     3\n\nfor_spline_mod$results\n\n\n  nvmax     RMSE  Rsquared      MAE    RMSESD RsquaredSD     MAESD\n1     2 12.86781 0.3498257 10.55154 0.9235936 0.10765745 0.6993457\n2     3 12.92971 0.3449085 10.51615 0.9716964 0.09693563 0.6348611\n3     4 12.69644 0.3685762 10.30346 0.8366573 0.10280846 0.6553263\n\nAccording to the Forward Stepwise Selection model with natural\nsplines, the top predictor for song energy level is acous.\nExamine LASSO model with natural splines output:\n\n\nsummary(LASSO_spline_mod)\n\n\n            Length Class      Mode     \na0            72   -none-     numeric  \nbeta        1728   dgCMatrix  S4       \ndf            72   -none-     numeric  \ndim            2   -none-     numeric  \nlambda        72   -none-     numeric  \ndev.ratio     72   -none-     numeric  \nnulldev        1   -none-     numeric  \nnpasses        1   -none-     numeric  \njerr           1   -none-     numeric  \noffset         1   -none-     logical  \ncall           5   -none-     call     \nnobs           1   -none-     numeric  \nlambdaOpt      1   -none-     numeric  \nxNames        24   -none-     character\nproblemType    1   -none-     character\ntuneValue      2   data.frame list     \nobsLevels      1   -none-     logical  \nparam          0   -none-     list     \n\nplot(LASSO_spline_mod)\n\n\n\nLASSO_spline_mod$bestTune\n\n\n  alpha    lambda\n6     1 0.5050505\n\n# LASSO_spline_mod$results\n\n\n\nThe lambda value provided by the LASSO model with splines is\n0.5050505.\nGAM with LOESS terms\nFit a GAM using LOESS terms using the set of variables deemed to be\nmost relevant based on your investigations so far.\nHow does test performance of the GAM compare to other models you\nexplored?\nDo you gain any insights from the GAM output plots for each\npredictor?\n\n\nset.seed(253)\ngam_mod <- train(\n    nrgy ~ acous + val + dB,\n    data = top_spotify_new,\n    method = \"gamLoess\",\n    tuneGrid = data.frame(degree = 1, span = seq(0.1, 0.9, by = 0.1)),\n    trControl = trainControl(method = \"cv\", number = 9, selectionFunction = \"best\"),\n    metric = \"MAE\",\n    na.action = na.omit\n)\n\n\n\nExamine GAM with LOESS output:\n\n\ngam_mod$results[3,]\n\n\n  degree span     RMSE  Rsquared      MAE   RMSESD RsquaredSD\n3      1  0.3 10.20914 0.5906928 8.252745 0.892152  0.1020878\n      MAESD\n3 0.8103724\n\n\n\nplot(gam_mod)\n\n\n\n#Metrics for the best model \ngam_mod$results %>%\n    filter(span==gam_mod$bestTune$span)\n\n\n  degree span     RMSE  Rsquared      MAE   RMSESD RsquaredSD\n1      1  0.3 10.20914 0.5906928 8.252745 0.892152  0.1020878\n      MAESD\n1 0.8103724\n\n#Graphing Each Predictor \npar(mfrow = c(3,4)) # Sets up a grid of plots\nplot(gam_mod$finalModel, se = TRUE) # Dashed lines are +/- 2 SEs\n\n\n\n\nGAM with a span of 0.3 offers a MAE of 8.252745, indicating that we\nour predictions for top song energy level would be off by 8.368585\npercentage points in this case. This result is actually better than all\nfour previous models fitted in the first section.\nSummarize investigations\nDecide on an overall best model based on your investigations so far.\nTo do this, make clear your analysis goals. Predictive accuracy?\nInterpretability? A combination of both?\nOverall, based on the output given by all of the models we fitted\nabove, it seems that a GAM with LOESS model achieves the lowest MAE for\nour dataset. For our analysis, since we want to correctly predict the\nenergy level of a popular song, we care about the predictive accuracy of\nthe model. We are also interested in knowing what contributes to an\nenergetic song, thus interpretability is also essential for the model.\nTherefore splines doesn’t seem the most straightforward choice for us,\nwhereas either GAM with LOESS or LASSO seems like a better option.\nSocietal impact\nAre there any harms that may come from your analyses and/or how the\ndata were collected? What cautions do you want to keep in mind when\ncommunicating your work?\nOur models takes a harmless look at the deciding elements of an\nenergetic song, as under the environment of a global pandemic where\nsocial interactions are limited, it is important to look for means to\nmaintain a positive mood, and it seems that listening to uplifting pop\nmusic is a favorable way to do so. Given our dataset, though, since the\nsource is Spotify and Billboard, our scope of pop music is limited and\nmay result in a certain pattern in our predictions. We want to caution\nthat good music choice should by no means be limited, and it should\nalways be optimal to listen to whatever one’s heart desires.\nClassification analysis\n(Methods)\nWe used logistic regression and random forest for building\nclassification models.\nLogistic Regression\nWe converted the predictor “pop” to categorical, assigning the\nobservations with value above 75 to be top songs.\n\n\ntop_spotify_new$IsPop <- \"NO\"\ntop_spotify_new$IsPop[top_spotify_new$pop >= 75] <- \"YES\"\ntable(top_spotify_new$IsPop)\ntable(top_spotify_new$pop)\ntop_spotify_new$IsPop <- factor(top_spotify_new$IsPop)\n\n\n\nWe then fit the logistic regression model predicting whether a given\nsong is a popular song with all other predictors. We selected the\nmetrics Accuracy so that the model we fit would prioritize making the\nmost accurate predictions.\n\n\nset.seed(253)\nlogistic_mod <- train(\n    IsPop ~ .-pop,\n    data = top_spotify_new,\n    method = \"glm\",\n    family = \"binomial\",\n    trControl = trainControl(method = \"cv\", number = 10),\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\nsummary(logistic_mod$results)\n\n\n  parameter            Accuracy          Kappa       \n Length:1           Min.   :0.7124   Min.   :0.2109  \n Class :character   1st Qu.:0.7124   1st Qu.:0.2109  \n Mode  :character   Median :0.7124   Median :0.2109  \n                    Mean   :0.7124   Mean   :0.2109  \n                    3rd Qu.:0.7124   3rd Qu.:0.2109  \n                    Max.   :0.7124   Max.   :0.2109  \n   AccuracySD         KappaSD       \n Min.   :0.02915   Min.   :0.07924  \n 1st Qu.:0.02915   1st Qu.:0.07924  \n Median :0.02915   Median :0.07924  \n Mean   :0.02915   Mean   :0.07924  \n 3rd Qu.:0.02915   3rd Qu.:0.07924  \n Max.   :0.02915   Max.   :0.07924  \n\ncoefficients(logistic_mod$finalModel) %>% exp()\n\n\n  (Intercept)          year           bpm          nrgy          dnce \n1.508016e-192  1.246311e+00  1.000621e+00  9.717299e-01  1.007890e+00 \n           dB          live           val           dur         acous \n 1.147383e+00  9.915256e-01  1.006670e+00  9.968091e-01  1.001887e+00 \n         spch \n 9.881283e-01 \n\nWe also fit the LASSO logistic regression, gaining insight about\nvariable importance.\n\n\ntwoClassSummaryCustom <- function (data, lev = NULL, model = NULL) {\n    if (length(lev) > 2) {\n        stop(paste(\"Your outcome has\", length(lev), \"levels. The twoClassSummary() function isn't appropriate.\"))\n    }\n    caret:::requireNamespaceQuietStop(\"pROC\")\n    if (!all(levels(data[, \"pred\"]) == lev)) {\n        stop(\"levels of observed and predicted data do not match\")\n    }\n    rocObject <- try(pROC::roc(data$obs, data[, lev[1]], direction = \">\", \n        quiet = TRUE), silent = TRUE)\n    rocAUC <- if (inherits(rocObject, \"try-error\")) \n        NA\n    else rocObject$auc\n    out <- c(rocAUC, sensitivity(data[, \"pred\"], data[, \"obs\"], \n        lev[1]), specificity(data[, \"pred\"], data[, \"obs\"], lev[2]))\n    out2 <- postResample(data[, \"pred\"], data[, \"obs\"])\n    out <- c(out, out2[1])\n    names(out) <- c(\"AUC\", \"Sens\", \"Spec\", \"Accuracy\")\n    out\n}\nset.seed(253)\nlasso_logistic_mod <- train(\n    IsPop ~ .-pop,\n    data = top_spotify_new,\n    method = \"glmnet\",\n    family = \"binomial\",\n    tuneGrid = data.frame(alpha = 1, lambda = seq(0, 1, length.out = 100)),\n    trControl = trainControl(method = \"cv\", number = 10, selectionFunction = \"oneSE\", classProbs = TRUE, summaryFunction = twoClassSummaryCustom),\n    metric = \"AUC\",\n    na.action = na.omit\n)\n\nplot(lasso_logistic_mod)\n\n\n\n\n\n\nlasso_logistic_mod$bestTune\nlasso_logistic_mod$results\nlasso_logistic_mod$results %>%\n    filter(lambda==lasso_logistic_mod$bestTune$lambda)\nplot(lasso_logistic_mod$finalModel, xvar = \"lambda\", label = TRUE, col = rainbow(20), ylim = c(-0.5,7))\n\nrownames(lasso_logistic_mod$finalModel$beta)[c(5,3,1)]\n\n\n\nTrees and Random Forest\nWe fit trees and random forest to make predictions as well, using all\nother predictors to predict whether an observation is a popular song or\nnot. The metrics we selected is Accuracy, so that the model would\nprioritize making accurate predictions.\n\n\nset.seed(253)\ntree_mod <- train(\n    IsPop ~ .-pop,\n    data = top_spotify_new,\n    method = \"rpart\",\n    tuneGrid = data.frame(cp = seq(0, 0.5, length.out = 50)),\n    trControl = trainControl(method = \"cv\", number = 10, selectionFunction = \"oneSE\"),\n    metric = \"Accuracy\",\n    na.action = na.omit\n)\n\nplot(tree_mod)\n\n\n\ntree_mod$results %>%\n    filter(cp==tree_mod$bestTune$cp)\n\n\n         cp  Accuracy    Kappa AccuracySD KappaSD\n1 0.1020408 0.7143012 0.181339 0.03461561 0.13422\n\n\n\nrf_mod <- train(\n    IsPop ~ .-pop,\n    data = top_spotify_new,\n    method = \"rf\",\n    tuneGrid = data.frame(mtry = c(2,3,4,5,6,7,8)),\n    trControl = trainControl(method = \"oob\", selectionFunction = \"best\"),\n    metric = \"Accuracy\",\n    ntree = 750, # To force fitting 1000 trees (can help with stability of results)\n    na.action = na.omit\n)\nplot(rf_mod)\n\n\n\nrf_mod$results\n\n\n   Accuracy     Kappa mtry\n1 0.7524917 0.3347524    2\n2 0.7425249 0.3147939    3\n3 0.7342193 0.3007419    4\n4 0.7392027 0.3082283    5\n5 0.7342193 0.2961637    6\n6 0.7358804 0.2947911    7\n7 0.7375415 0.3026788    8\n\nrf_mod$finalModel\n\n\n\nCall:\n randomForest(x = x, y = y, ntree = 750, mtry = min(param$mtry,      ncol(x))) \n               Type of random forest: classification\n                     Number of trees: 750\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 26.08%\nConfusion matrix:\n     NO YES class.error\nNO  384  29  0.07021792\nYES 128  61  0.67724868\n\nOur model is better at identifying songs that are not popular. In our\ncontext, it helps us avoid bad songs, which is preferable than the more\nlenient alternative which is more likely to falsely categorize a song as\npopular.\n\n\nvar_imp_rf <- randomForest::importance(rf_mod$finalModel)\n\n# Sort by importance with dplyr's arrange()\nvar_imp_rf <- data.frame(\n        predictor = rownames(var_imp_rf),\n        MeanDecreaseGini = var_imp_rf[,\"MeanDecreaseGini\"]\n    ) %>%\n    arrange(desc(MeanDecreaseGini))\n\n# Top 10\nhead(var_imp_rf, 10)\n\n\n      predictor MeanDecreaseGini\nyear       year         34.01757\nval         val         29.91566\ndur         dur         29.62221\nnrgy       nrgy         29.08262\nbpm         bpm         27.71691\ndnce       dnce         26.28274\nacous     acous         24.00443\nlive       live         23.57061\nspch       spch         18.39002\ndB           dB         15.18356\n\nIt seems that the most important predictor, given contributions to\ndecreasing the Gini index, is year, which is pretty interesting. This\ntells us that knowing what year the song is released would offer us much\ninsight into whether the song is likely to be popular.\nClassification\nAnalysis (Results- Variable Importance)\nFor our logistic regression model, we utilized a LASSO logistic\nregression to gain insight on variable importance. The results\ndemonstrate that dnce, and bpm are the most important variables for\npredicting the popularity of a song. These results are sensible because\nit is plausible that more upbeat songs that you can dance to will be\nvaluable traits that may lead a song to be more popular.\nIn our random forest model, it shows that year is by far the most\nimportant variable for predicting song popularity, which also\ncorresponds to the most important variable as determined by the variable\nimportance measure of a single decision tree. This is because it lowers\nto Gini index the most on average for all the trees. Year is not very\ninsightful, though, because it is possible that the popular songs\nfeatured in this dataset came more from particular years than others. It\ndoesn’t really help us predict the future popularity of a song. More\ninterestingly, the energy displayed by a song has the second most\nmeaningful mean decrease in the Gini index. Again, this is sensible\nbecause the goal of a song often times is to portray energy to its\nlistener, so it makes sense that songs that accomplish this goal would\nbe more popular. 22\nClassification analysis\n(Summary)\nCompare models\nWe have compared a logistic regression and a decision tree model. To\ncomplement the models, we have ran a LASSO logistic regression and a\nrandom forest. We are trying to predict whether a song will be\nrelatively popular. We have created our own binary variable with a\nthreshold of > 75 in pop to be considered popular (IsPop = YES). In\nall models, the most important variable seemed to be year. In this\ncontext, songs released in a certain year seem to be the most popular.\nOther important variables were energy levels and dance-ibility, both of\nwhich intuitively make sense as they would be more commonly enjoyed\namong music listeners.\nEvaluation metrics\nLogistic Regression Accuracy: 0.7124414\nLogistic Regression Accuracy SD: 0.0291491\nLasso Logistic Regression Accuracy: 0.6860489\nLasso Logistic Regression Accuracy SD: 0.003961554\nLasso Logistic Regression AUC: 0.6715804\nLasso Logistic Regression AUC SD: 0.07519223\nDecision Tree Accuracy: 0.7143012\nDecision Tree Accuracy SD: 0.03461561\nRandom Forest Accuracy: 0.7524917\nRandom Forest Confusion Matrix: NA\nPREDICTED\n\n  NO YES class.error\n  \nNO 380 33 0.07990315\nYES 122 67 0.64550265\nNIR: (413)/(413+189) = 68.6%\nThe NIR is calculated using the whole training set.\nBroadly summarize conclusions from looking at these evaluation\nmetrics and their measures of uncertainty:\nWe can see that the model that gives us the least amount of variance\nis lasso logistic regression. However, we note that we are not trying to\nbuild the model with the smallest variance, as the variance is just a\nway to look at the uncertainty of in estimation of test performance,\nwhich is not so high when using lasso logistic regression. Random Forest\nseems to have the highest accuracy. With an OOB estimate error rate of\n25.75%, this is reflective of our accuracy.\nOverall most preferable\nmodel\nThe overall most preferable model would be our random forest model.\nThe accuracy in this model far outweighs all other models at an accuracy\nof 75.25% Random forests also provide out of bag error estimations,\nwhich give us an accountable measurement of error in our model.\nWith an overall accuracy of 75.2%, and a NIR of 68.6%, we believe\nthis model shows an acceptable amount of error.\nIf using OOB error estimation, display the test (OOB) confusion\nmatrix, and use it to interpret the strengths and weaknesses of the\nfinal model:\nWe can see that we have a sensitivity of (67)/(67+112) = 37.43% and a\nspecificity of (380)/(380+33) = 92.01%. Our model is good at correctly\npredicting songs that won’t be as popular. However, our model is bad at\ncorrectly predicting songs that won’t be popular.\n\n\nPopularSongs <- top_spotify_new[top_spotify_new$pop >= 75, ]\ntable(PopularSongs$year)\n\n\n\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 \n   9   10   10   16   12   24   23   25   32   28 \n\ntable(top_spotify_new$year)\n\n\n\n2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 \n  51   53   35   71   58   95   79   65   64   31 \n\nWe can see that although years are fairly balanced, the years when\nlooking at popular songs seem to be skewed towards later years. This\nmeans makes sense as the more “popular” songs seem to be the more recent\nones.\n\n\n\n",
    "preview": "https://storage.googleapis.com/pr-newsroom-wp/1/2018/11/Spotify_Logo_CMYK_Green.png",
    "last_modified": "2022-05-31T19:52:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/CausalDiagram/",
    "title": "Causal Inference Research",
    "description": "2019 Summer research project funded by the Jr. Faculty-Hub Summer Research Fund. We built a causal diagram through abstracts!",
    "author": [
      {
        "name": "Nicholas Di and Professor Myint",
        "url": {}
      }
    ],
    "date": "2019-08-01",
    "categories": [],
    "contents": "\nResearch Poster\nFirst Year Summer Research!\n\n\n",
    "preview": "https://miro.medium.com/max/512/1*TETnr1maS79Cn3yNveXfFw.png",
    "last_modified": "2022-05-31T14:13:18-05:00",
    "input_file": {}
  }
]
