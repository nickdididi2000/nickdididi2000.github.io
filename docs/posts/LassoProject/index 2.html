<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007ba5; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #007ba5; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #007ba5; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #5e5e5e; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>

<style>
  div.csl-bib-body { }
  div.csl-entry {
    clear: both;
    }
  .hanging div.csl-entry {
    margin-left:2em;
    text-indent:-2em;
  }
  div.csl-left-margin {
    min-width:2em;
    float:left;
  }
  div.csl-right-inline {
    margin-left:2em;
    padding-left:1em;
  }
  div.csl-indent {
    margin-left: 2em;
  }
</style>

  <!--radix_placeholder_meta_tags-->
<title>: Lasso Regression &amp; Simulation</title>

<meta property="description" itemprop="description" content="This project was conducted for the capstone course: STAT 455 Mathematical Statistics. We wrote a report and created a simulation to visualize the bias variance tradeoff with penalized regressions!"/>


<!--  https://schema.org/Article -->
<meta property="article:published" itemprop="datePublished" content="2022-05-05"/>
<meta property="article:created" itemprop="dateCreated" content="2022-05-05"/>
<meta name="article:author" content="Nicholas Di, Ellery Island, and Will Orser"/>

<!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
<meta property="og:title" content=": Lasso Regression &amp; Simulation"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="This project was conducted for the capstone course: STAT 455 Mathematical Statistics. We wrote a report and created a simulation to visualize the bias variance tradeoff with penalized regressions!"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:site_name" content=""/>

<!--  https://dev.twitter.com/cards/types/summary -->
<meta property="twitter:card" content="summary"/>
<meta property="twitter:title" content=": Lasso Regression &amp; Simulation"/>
<meta property="twitter:description" content="This project was conducted for the capstone course: STAT 455 Mathematical Statistics. We wrote a report and created a simulation to visualize the bias variance tradeoff with penalized regressions!"/>

<!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Penalized regression methods for linear models in SAS/STAT;citation_publication_date=2015;citation_author=Funda Gunes"/>
  <meta name="citation_reference" content="citation_title=Prevent children’s exposure to lead;citation_publication_date=2013;citation_author=Trevor Hastie Gareth James;citation_author=Robert Tibshirani"/>
  <meta name="citation_reference" content="citation_title=LASSO regression;citation_author=J. Ranstam;citation_author=J. A. Cook"/>
  <meta name="citation_reference" content="citation_title=Regression shrinkage and selection via the lasso;citation_author=Robert Tibshirani"/>
  <!--radix_placeholder_rmarkdown_metadata-->

<script type="text/json" id="radix-rmarkdown-metadata">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","author","description","preview","date","output","bibliography"]}},"value":[{"type":"character","attributes":{},"value":["Lasso Regression & Simulation"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name"]}},"value":[{"type":"character","attributes":{},"value":["Nicholas Di, Ellery Island, and Will Orser"]}]}]},{"type":"character","attributes":{},"value":["This project was conducted for the capstone course: STAT 455 Mathematical Statistics. We wrote a report and created a simulation to visualize the bias variance tradeoff with penalized regressions!"]},{"type":"character","attributes":{},"value":["https://kirenz.com/post/2019-08-12-python-lasso-regression-auto/featured_hue76a1f1ce8c01e6382d5577465b6e9e2_44866_680x500_fill_q90_lanczos_smart1.jpg"]},{"type":"character","attributes":{},"value":["05-05-2022"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["LassoLibrary.bib"]}]}
</script>
<!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["lasso_vis.png","LassoLibrary.bib","LassoProject_files/anchor-4.2.2/anchor.min.js","LassoProject_files/bowser-1.9.3/bowser.min.js","LassoProject_files/distill-2.2.21/template.v2.js","LassoProject_files/figure-html5/unnamed-chunk-1-1.png","LassoProject_files/figure-html5/unnamed-chunk-10-1.png","LassoProject_files/figure-html5/unnamed-chunk-16-1.png","LassoProject_files/figure-html5/unnamed-chunk-17-1.png","LassoProject_files/header-attrs-2.13/header-attrs.js","LassoProject_files/jquery-3.6.0/jquery-3.6.0.js","LassoProject_files/jquery-3.6.0/jquery-3.6.0.min.js","LassoProject_files/jquery-3.6.0/jquery-3.6.0.min.map","LassoProject_files/popper-2.6.0/popper.min.js","LassoProject_files/tippy-6.2.7/tippy-bundle.umd.min.js","LassoProject_files/tippy-6.2.7/tippy-light-border.css","LassoProject_files/tippy-6.2.7/tippy.css","LassoProject_files/tippy-6.2.7/tippy.umd.min.js","LassoProject_files/webcomponents-2.0.0/webcomponents.js","ridge_vis.png"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
<meta name="distill:offset" content="../.."/>

<script type="application/javascript">

  window.headroom_prevent_pin = false;

  window.document.addEventListener("DOMContentLoaded", function (event) {

    // initialize headroom for banner
    var header = $('header').get(0);
    var headerHeight = header.offsetHeight;
    var headroom = new Headroom(header, {
      tolerance: 5,
      onPin : function() {
        if (window.headroom_prevent_pin) {
          window.headroom_prevent_pin = false;
          headroom.unpin();
        }
      }
    });
    headroom.init();
    if(window.location.hash)
      headroom.unpin();
    $(header).addClass('headroom--transition');

    // offset scroll location for banner on hash change
    // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
    window.addEventListener("hashchange", function(event) {
      window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
    });

    // responsive menu
    $('.distill-site-header').each(function(i, val) {
      var topnav = $(this);
      var toggle = topnav.find('.nav-toggle');
      toggle.on('click', function() {
        topnav.toggleClass('responsive');
      });
    });

    // nav dropdowns
    $('.nav-dropbtn').click(function(e) {
      $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
      $(this).parent().siblings('.nav-dropdown')
         .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $("body").click(function(e){
      $('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $(".nav-dropdown").click(function(e){
      e.stopPropagation();
    });
  });
</script>

<style type="text/css">

/* Theme (user-documented overrideables for nav appearance) */

.distill-site-nav {
  color: rgba(255, 255, 255, 0.8);
  background-color: #0F2E3D;
  font-size: 15px;
  font-weight: 300;
}

.distill-site-nav a {
  color: inherit;
  text-decoration: none;
}

.distill-site-nav a:hover {
  color: white;
}

@media print {
  .distill-site-nav {
    display: none;
  }
}

.distill-site-header {

}

.distill-site-footer {

}


/* Site Header */

.distill-site-header {
  width: 100%;
  box-sizing: border-box;
  z-index: 3;
}

.distill-site-header .nav-left {
  display: inline-block;
  margin-left: 8px;
}

@media screen and (max-width: 768px) {
  .distill-site-header .nav-left {
    margin-left: 0;
  }
}


.distill-site-header .nav-right {
  float: right;
  margin-right: 8px;
}

.distill-site-header a,
.distill-site-header .title {
  display: inline-block;
  text-align: center;
  padding: 14px 10px 14px 10px;
}

.distill-site-header .title {
  font-size: 18px;
  min-width: 150px;
}

.distill-site-header .logo {
  padding: 0;
}

.distill-site-header .logo img {
  display: none;
  max-height: 20px;
  width: auto;
  margin-bottom: -4px;
}

.distill-site-header .nav-image img {
  max-height: 18px;
  width: auto;
  display: inline-block;
  margin-bottom: -3px;
}



@media screen and (min-width: 1000px) {
  .distill-site-header .logo img {
    display: inline-block;
  }
  .distill-site-header .nav-left {
    margin-left: 20px;
  }
  .distill-site-header .nav-right {
    margin-right: 20px;
  }
  .distill-site-header .title {
    padding-left: 12px;
  }
}


.distill-site-header .nav-toggle {
  display: none;
}

.nav-dropdown {
  display: inline-block;
  position: relative;
}

.nav-dropdown .nav-dropbtn {
  border: none;
  outline: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 16px 10px;
  background-color: transparent;
  font-family: inherit;
  font-size: inherit;
  font-weight: inherit;
  margin: 0;
  margin-top: 1px;
  z-index: 2;
}

.nav-dropdown-content {
  display: none;
  position: absolute;
  background-color: white;
  min-width: 200px;
  border: 1px solid rgba(0,0,0,0.15);
  border-radius: 4px;
  box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
  z-index: 1;
  margin-top: 2px;
  white-space: nowrap;
  padding-top: 4px;
  padding-bottom: 4px;
}

.nav-dropdown-content hr {
  margin-top: 4px;
  margin-bottom: 4px;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.nav-dropdown-active {
  display: block;
}

.nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
  color: black;
  padding: 6px 24px;
  text-decoration: none;
  display: block;
  text-align: left;
}

.nav-dropdown-content .nav-dropdown-header {
  display: block;
  padding: 5px 24px;
  padding-bottom: 0;
  text-transform: uppercase;
  font-size: 14px;
  color: #999999;
  white-space: nowrap;
}

.nav-dropdown:hover .nav-dropbtn {
  color: white;
}

.nav-dropdown-content a:hover {
  background-color: #ddd;
  color: black;
}

.nav-right .nav-dropdown-content {
  margin-left: -45%;
  right: 0;
}

@media screen and (max-width: 768px) {
  .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
  .distill-site-header a.nav-toggle {
    float: right;
    display: block;
  }
  .distill-site-header .title {
    margin-left: 0;
  }
  .distill-site-header .nav-right {
    margin-right: 0;
  }
  .distill-site-header {
    overflow: hidden;
  }
  .nav-right .nav-dropdown-content {
    margin-left: 0;
  }
}


@media screen and (max-width: 768px) {
  .distill-site-header.responsive {position: relative; min-height: 500px; }
  .distill-site-header.responsive a.nav-toggle {
    position: absolute;
    right: 0;
    top: 0;
  }
  .distill-site-header.responsive a,
  .distill-site-header.responsive .nav-dropdown {
    display: block;
    text-align: left;
  }
  .distill-site-header.responsive .nav-left,
  .distill-site-header.responsive .nav-right {
    width: 100%;
  }
  .distill-site-header.responsive .nav-dropdown {float: none;}
  .distill-site-header.responsive .nav-dropdown-content {position: relative;}
  .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
    display: block;
    width: 100%;
    text-align: left;
  }
}

/* Site Footer */

.distill-site-footer {
  width: 100%;
  overflow: hidden;
  box-sizing: border-box;
  z-index: 3;
  margin-top: 30px;
  padding-top: 30px;
  padding-bottom: 30px;
  text-align: center;
}

/* Headroom */

d-title {
  padding-top: 6rem;
}

@media print {
  d-title {
    padding-top: 4rem;
  }
}

.headroom {
  z-index: 1000;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
}

.headroom--transition {
  transition: all .4s ease-in-out;
}

.headroom--unpinned {
  top: -100px;
}

.headroom--pinned {
  top: 0;
}

/* adjust viewport for navbar height */
/* helps vertically center bootstrap (non-distill) content */
.min-vh-100 {
  min-height: calc(100vh - 100px) !important;
}

</style>

<script src="../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="../../site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet"/>
<link href="../../site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet"/>
<script src="../../site_libs/headroom-0.9.4/headroom.min.js"></script>
<script src="../../site_libs/autocomplete-0.37.1/autocomplete.min.js"></script>
<script src="../../site_libs/fuse-6.4.1/fuse.min.js"></script>

<script type="application/javascript">

function getMeta(metaName) {
  var metas = document.getElementsByTagName('meta');
  for (let i = 0; i < metas.length; i++) {
    if (metas[i].getAttribute('name') === metaName) {
      return metas[i].getAttribute('content');
    }
  }
  return '';
}

function offsetURL(url) {
  var offset = getMeta('distill:offset');
  return offset ? offset + '/' + url : url;
}

function createFuseIndex() {

  // create fuse index
  var options = {
    keys: [
      { name: 'title', weight: 20 },
      { name: 'categories', weight: 15 },
      { name: 'description', weight: 10 },
      { name: 'contents', weight: 5 },
    ],
    ignoreLocation: true,
    threshold: 0
  };
  var fuse = new window.Fuse([], options);

  // fetch the main search.json
  return fetch(offsetURL('search.json'))
    .then(function(response) {
      if (response.status == 200) {
        return response.json().then(function(json) {
          // index main articles
          json.articles.forEach(function(article) {
            fuse.add(article);
          });
          // download collections and index their articles
          return Promise.all(json.collections.map(function(collection) {
            return fetch(offsetURL(collection)).then(function(response) {
              if (response.status === 200) {
                return response.json().then(function(articles) {
                  articles.forEach(function(article) {
                    fuse.add(article);
                  });
                })
              } else {
                return Promise.reject(
                  new Error('Unexpected status from search index request: ' +
                            response.status)
                );
              }
            });
          })).then(function() {
            return fuse;
          });
        });

      } else {
        return Promise.reject(
          new Error('Unexpected status from search index request: ' +
                      response.status)
        );
      }
    });
}

window.document.addEventListener("DOMContentLoaded", function (event) {

  // get search element (bail if we don't have one)
  var searchEl = window.document.getElementById('distill-search');
  if (!searchEl)
    return;

  createFuseIndex()
    .then(function(fuse) {

      // make search box visible
      searchEl.classList.remove('hidden');

      // initialize autocomplete
      var options = {
        autoselect: true,
        hint: false,
        minLength: 2,
      };
      window.autocomplete(searchEl, options, [{
        source: function(query, callback) {
          const searchOptions = {
            isCaseSensitive: false,
            shouldSort: true,
            minMatchCharLength: 2,
            limit: 10,
          };
          var results = fuse.search(query, searchOptions);
          callback(results
            .map(function(result) { return result.item; })
          );
        },
        templates: {
          suggestion: function(suggestion) {
            var img = suggestion.preview && Object.keys(suggestion.preview).length > 0
              ? `<img src="${offsetURL(suggestion.preview)}"</img>`
              : '';
            var html = `
              <div class="search-item">
                <h3>${suggestion.title}</h3>
                <div class="search-item-description">
                  ${suggestion.description || ''}
                </div>
                <div class="search-item-preview">
                  ${img}
                </div>
              </div>
            `;
            return html;
          }
        }
      }]).on('autocomplete:selected', function(event, suggestion) {
        window.location.href = offsetURL(suggestion.path);
      });
      // remove inline display style on autocompleter (we want to
      // manage responsive display via css)
      $('.algolia-autocomplete').css("display", "");
    })
    .catch(function(error) {
      console.log(error);
    });

});

</script>

<style type="text/css">

.nav-search {
  font-size: x-small;
}

/* Algolioa Autocomplete */

.algolia-autocomplete {
  display: inline-block;
  margin-left: 10px;
  vertical-align: sub;
  background-color: white;
  color: black;
  padding: 6px;
  padding-top: 8px;
  padding-bottom: 0;
  border-radius: 6px;
  border: 1px #0F2E3D solid;
  width: 180px;
}


@media screen and (max-width: 768px) {
  .distill-site-nav .algolia-autocomplete {
    display: none;
    visibility: hidden;
  }
  .distill-site-nav.responsive .algolia-autocomplete {
    display: inline-block;
    visibility: visible;
  }
  .distill-site-nav.responsive .algolia-autocomplete .aa-dropdown-menu {
    margin-left: 0;
    width: 400px;
    max-height: 400px;
  }
}

.algolia-autocomplete .aa-input, .algolia-autocomplete .aa-hint {
  width: 90%;
  outline: none;
  border: none;
}

.algolia-autocomplete .aa-hint {
  color: #999;
}
.algolia-autocomplete .aa-dropdown-menu {
  width: 550px;
  max-height: 70vh;
  overflow-x: visible;
  overflow-y: scroll;
  padding: 5px;
  margin-top: 3px;
  margin-left: -150px;
  background-color: #fff;
  border-radius: 5px;
  border: 1px solid #999;
  border-top: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion {
  cursor: pointer;
  padding: 5px 4px;
  border-bottom: 1px solid #eee;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion:last-of-type {
  border-bottom: none;
  margin-bottom: 2px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item {
  overflow: hidden;
  font-size: 0.8em;
  line-height: 1.4em;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item h3 {
  font-size: 1rem;
  margin-block-start: 0;
  margin-block-end: 5px;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-description {
  display: inline-block;
  overflow: hidden;
  height: 2.8em;
  width: 80%;
  margin-right: 4%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview {
  display: inline-block;
  width: 15%;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img {
  height: 3em;
  width: auto;
  display: none;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img[src] {
  display: initial;
}

.algolia-autocomplete .aa-dropdown-menu .aa-suggestion.aa-cursor {
  background-color: #eee;
}
.algolia-autocomplete .aa-dropdown-menu .aa-suggestion em {
  font-weight: bold;
  font-style: normal;
}

</style>


<!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

<style type="text/css">

body {
  background-color: white;
}

.pandoc-table {
  width: 100%;
}

.pandoc-table>caption {
  margin-bottom: 10px;
}

.pandoc-table th:not([align]) {
  text-align: left;
}

.pagedtable-footer {
  font-size: 15px;
}

d-byline .byline {
  grid-template-columns: 2fr 2fr;
}

d-byline .byline h3 {
  margin-block-start: 1.5em;
}

d-byline .byline .authors-affiliations h3 {
  margin-block-start: 0.5em;
}

.authors-affiliations .orcid-id {
  width: 16px;
  height:16px;
  margin-left: 4px;
  margin-right: 4px;
  vertical-align: middle;
  padding-bottom: 2px;
}

d-title .dt-tags {
  margin-top: 1em;
  grid-column: text;
}

.dt-tags .dt-tag {
  text-decoration: none;
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0em 0.4em;
  margin-right: 0.5em;
  margin-bottom: 0.4em;
  font-size: 70%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

d-article table.gt_table td,
d-article table.gt_table th {
  border-bottom: none;
}

.html-widget {
  margin-bottom: 2.0em;
}

.l-screen-inset {
  padding-right: 16px;
}

.l-screen .caption {
  margin-left: 10px;
}

.shaded {
  background: rgb(247, 247, 247);
  padding-top: 20px;
  padding-bottom: 20px;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .html-widget {
  margin-bottom: 0;
  border: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .shaded-content {
  background: white;
}

.text-output {
  margin-top: 0;
  line-height: 1.5em;
}

.hidden {
  display: none !important;
}

d-article {
  padding-top: 2.5rem;
  padding-bottom: 30px;
}

d-appendix {
  padding-top: 30px;
}

d-article>p>img {
  width: 100%;
}

d-article h2 {
  margin: 1rem 0 1.5rem 0;
}

d-article h3 {
  margin-top: 1.5rem;
}

d-article iframe {
  border: 1px solid rgba(0, 0, 0, 0.1);
  margin-bottom: 2.0em;
  width: 100%;
}

/* Tweak code blocks */

d-article div.sourceCode code,
d-article pre code {
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
}

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: auto;
}

d-article div.sourceCode {
  background-color: white;
}

d-article div.sourceCode pre {
  padding-left: 10px;
  font-size: 12px;
  border-left: 2px solid rgba(0,0,0,0.1);
}

d-article pre {
  font-size: 12px;
  color: black;
  background: none;
  margin-top: 0;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

d-article pre a {
  border-bottom: none;
}

d-article pre a:hover {
  border-bottom: none;
  text-decoration: underline;
}

d-article details {
  grid-column: text;
  margin-bottom: 0.8em;
}

@media(min-width: 768px) {

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: visible !important;
}

d-article div.sourceCode pre {
  padding-left: 18px;
  font-size: 14px;
}

d-article pre {
  font-size: 14px;
}

}

figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

/* CSS for d-contents */

.d-contents {
  grid-column: text;
  color: rgba(0,0,0,0.8);
  font-size: 0.9em;
  padding-bottom: 1em;
  margin-bottom: 1em;
  padding-bottom: 0.5em;
  margin-bottom: 1em;
  padding-left: 0.25em;
  justify-self: start;
}

@media(min-width: 1000px) {
  .d-contents.d-contents-float {
    height: 0;
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: center;
    padding-right: 3em;
    padding-left: 2em;
  }
}

.d-contents nav h3 {
  font-size: 18px;
  margin-top: 0;
  margin-bottom: 1em;
}

.d-contents li {
  list-style-type: none
}

.d-contents nav > ul {
  padding-left: 0;
}

.d-contents ul {
  padding-left: 1em
}

.d-contents nav ul li {
  margin-top: 0.6em;
  margin-bottom: 0.2em;
}

.d-contents nav a {
  font-size: 13px;
  border-bottom: none;
  text-decoration: none
  color: rgba(0, 0, 0, 0.8);
}

.d-contents nav a:hover {
  text-decoration: underline solid rgba(0, 0, 0, 0.6)
}

.d-contents nav > ul > li > a {
  font-weight: 600;
}

.d-contents nav > ul > li > ul {
  font-weight: inherit;
}

.d-contents nav > ul > li > ul > li {
  margin-top: 0.2em;
}


.d-contents nav ul {
  margin-top: 0;
  margin-bottom: 0.25em;
}

.d-article-with-toc h2:nth-child(2) {
  margin-top: 0;
}


/* Figure */

.figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

.figure img {
  width: 100%;
}

.figure .caption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

.figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

.figure .caption a {
  color: rgba(0, 0, 0, 0.6);
}

.figure .caption b,
.figure .caption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

/* Citations */

d-article .citation {
  color: inherit;
  cursor: inherit;
}

div.hanging-indent{
  margin-left: 1em; text-indent: -1em;
}

/* Citation hover box */

.tippy-box[data-theme~=light-border] {
  background-color: rgba(250, 250, 250, 0.95);
}

.tippy-content > p {
  margin-bottom: 0;
  padding: 2px;
}


/* Tweak 1000px media break to show more text */

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }

  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
  figure .caption, .figure .caption, figure figcaption {
    font-size: 13px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}


/* Get the citation styles for the appendix (not auto-injected on render since
   we do our own rendering of the citation appendix) */

d-appendix .citation-appendix,
.d-appendix .citation-appendix {
  font-size: 11px;
  line-height: 15px;
  border-left: 1px solid rgba(0, 0, 0, 0.1);
  padding-left: 18px;
  border: 1px solid rgba(0,0,0,0.1);
  background: rgba(0, 0, 0, 0.02);
  padding: 10px 18px;
  border-radius: 3px;
  color: rgba(150, 150, 150, 1);
  overflow: hidden;
  margin-top: -12px;
  white-space: pre-wrap;
  word-wrap: break-word;
}

/* Include appendix styles here so they can be overridden */

d-appendix {
  contain: layout style;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-top: 60px;
  margin-bottom: 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  color: rgba(0,0,0,0.5);
  padding-top: 60px;
  padding-bottom: 48px;
}

d-appendix h3 {
  grid-column: page-start / text-start;
  font-size: 15px;
  font-weight: 500;
  margin-top: 1em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.65);
}

d-appendix h3 + * {
  margin-top: 1em;
}

d-appendix ol {
  padding: 0 0 0 15px;
}

@media (min-width: 768px) {
  d-appendix ol {
    padding: 0 0 0 30px;
    margin-left: -30px;
  }
}

d-appendix li {
  margin-bottom: 1em;
}

d-appendix a {
  color: rgba(0, 0, 0, 0.6);
}

d-appendix > * {
  grid-column: text;
}

d-appendix > d-footnote-list,
d-appendix > d-citation-list,
d-appendix > distill-appendix {
  grid-column: screen;
}

/* Include footnote styles here so they can be overridden */

d-footnote-list {
  contain: layout style;
}

d-footnote-list > * {
  grid-column: text;
}

d-footnote-list a.footnote-backlink {
  color: rgba(0,0,0,0.3);
  padding-left: 0.5em;
}



/* Anchor.js */

.anchorjs-link {
  /*transition: all .25s linear; */
  text-decoration: none;
  border-bottom: none;
}
*:hover > .anchorjs-link {
  margin-left: -1.125em !important;
  text-decoration: none;
  border-bottom: none;
}

/* Social footer */

.social_footer {
  margin-top: 30px;
  margin-bottom: 0;
  color: rgba(0,0,0,0.67);
}

.disqus-comments {
  margin-right: 30px;
}

.disqus-comment-count {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  cursor: pointer;
}

#disqus_thread {
  margin-top: 30px;
}

.article-sharing a {
  border-bottom: none;
  margin-right: 8px;
}

.article-sharing a:hover {
  border-bottom: none;
}

.sidebar-section.subscribe {
  font-size: 12px;
  line-height: 1.6em;
}

.subscribe p {
  margin-bottom: 0.5em;
}


.article-footer .subscribe {
  font-size: 15px;
  margin-top: 45px;
}


.sidebar-section.custom {
  font-size: 12px;
  line-height: 1.6em;
}

.custom p {
  margin-bottom: 0.5em;
}

/* Styles for listing layout (hide title) */
.layout-listing d-title, .layout-listing .d-title {
  display: none;
}

/* Styles for posts lists (not auto-injected) */


.posts-with-sidebar {
  padding-left: 45px;
  padding-right: 45px;
}

.posts-list .description h2,
.posts-list .description p {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
}

.posts-list .description h2 {
  font-weight: 700;
  border-bottom: none;
  padding-bottom: 0;
}

.posts-list h2.post-tag {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  padding-bottom: 12px;
}
.posts-list {
  margin-top: 60px;
  margin-bottom: 24px;
}

.posts-list .post-preview {
  text-decoration: none;
  overflow: hidden;
  display: block;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding: 24px 0;
}

.post-preview-last {
  border-bottom: none !important;
}

.posts-list .posts-list-caption {
  grid-column: screen;
  font-weight: 400;
}

.posts-list .post-preview h2 {
  margin: 0 0 6px 0;
  line-height: 1.2em;
  font-style: normal;
  font-size: 24px;
}

.posts-list .post-preview p {
  margin: 0 0 12px 0;
  line-height: 1.4em;
  font-size: 16px;
}

.posts-list .post-preview .thumbnail {
  box-sizing: border-box;
  margin-bottom: 24px;
  position: relative;
  max-width: 500px;
}
.posts-list .post-preview img {
  width: 100%;
  display: block;
}

.posts-list .metadata {
  font-size: 12px;
  line-height: 1.4em;
  margin-bottom: 18px;
}

.posts-list .metadata > * {
  display: inline-block;
}

.posts-list .metadata .publishedDate {
  margin-right: 2em;
}

.posts-list .metadata .dt-authors {
  display: block;
  margin-top: 0.3em;
  margin-right: 2em;
}

.posts-list .dt-tags {
  display: block;
  line-height: 1em;
}

.posts-list .dt-tags .dt-tag {
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0.3em 0.4em;
  margin-right: 0.2em;
  margin-bottom: 0.4em;
  font-size: 60%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

.posts-list img {
  opacity: 1;
}

.posts-list img[data-src] {
  opacity: 0;
}

.posts-more {
  clear: both;
}


.posts-sidebar {
  font-size: 16px;
}

.posts-sidebar h3 {
  font-size: 16px;
  margin-top: 0;
  margin-bottom: 0.5em;
  font-weight: 400;
  text-transform: uppercase;
}

.sidebar-section {
  margin-bottom: 30px;
}

.categories ul {
  list-style-type: none;
  margin: 0;
  padding: 0;
}

.categories li {
  color: rgba(0, 0, 0, 0.8);
  margin-bottom: 0;
}

.categories li>a {
  border-bottom: none;
}

.categories li>a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
}

.categories .active {
  font-weight: 600;
}

.categories .category-count {
  color: rgba(0, 0, 0, 0.4);
}


@media(min-width: 768px) {
  .posts-list .post-preview h2 {
    font-size: 26px;
  }
  .posts-list .post-preview .thumbnail {
    float: right;
    width: 30%;
    margin-bottom: 0;
  }
  .posts-list .post-preview .description {
    float: left;
    width: 45%;
  }
  .posts-list .post-preview .metadata {
    float: left;
    width: 20%;
    margin-top: 8px;
  }
  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.5em;
    font-size: 16px;
  }
  .posts-with-sidebar .posts-list {
    float: left;
    width: 75%;
  }
  .posts-with-sidebar .posts-sidebar {
    float: right;
    width: 20%;
    margin-top: 60px;
    padding-top: 24px;
    padding-bottom: 24px;
  }
}


/* Improve display for browsers without grid (IE/Edge <= 15) */

.downlevel {
  line-height: 1.6em;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  margin: 0;
}

.downlevel .d-title {
  padding-top: 6rem;
  padding-bottom: 1.5rem;
}

.downlevel .d-title h1 {
  font-size: 50px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

.downlevel .d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  margin-top: 0;
}

.downlevel .d-byline {
  padding-top: 0.8em;
  padding-bottom: 0.8em;
  font-size: 0.8rem;
  line-height: 1.8em;
}

.downlevel .section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
}

.downlevel .d-article {
  font-size: 1.06rem;
  line-height: 1.7em;
  padding-top: 1rem;
  padding-bottom: 2rem;
}


.downlevel .d-appendix {
  padding-left: 0;
  padding-right: 0;
  max-width: none;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.5);
  padding-top: 40px;
  padding-bottom: 48px;
}

.downlevel .footnotes ol {
  padding-left: 13px;
}

.downlevel .base-grid,
.downlevel .distill-header,
.downlevel .d-title,
.downlevel .d-abstract,
.downlevel .d-article,
.downlevel .d-appendix,
.downlevel .distill-appendix,
.downlevel .d-byline,
.downlevel .d-footnote-list,
.downlevel .d-citation-list,
.downlevel .distill-footer,
.downlevel .appendix-bottom,
.downlevel .posts-container {
  padding-left: 40px;
  padding-right: 40px;
}

@media(min-width: 768px) {
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
  padding-left: 150px;
  padding-right: 150px;
  max-width: 900px;
}
}

.downlevel pre code {
  display: block;
  border-left: 2px solid rgba(0, 0, 0, .1);
  padding: 0 0 0 20px;
  font-size: 14px;
}

.downlevel code, .downlevel pre {
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

.downlevel .posts-list .post-preview {
  color: inherit;
}



</style>

<script type="application/javascript">

function is_downlevel_browser() {
  if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                 window.navigator.userAgent)) {
    return true;
  } else {
    return window.load_distill_framework === undefined;
  }
}

// show body when load is complete
function on_load_complete() {

  // add anchors
  if (window.anchors) {
    window.anchors.options.placement = 'left';
    window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
  }


  // set body to visible
  document.body.style.visibility = 'visible';

  // force redraw for leaflet widgets
  if (window.HTMLWidgets) {
    var maps = window.HTMLWidgets.findAll(".leaflet");
    $.each(maps, function(i, el) {
      var map = this.getMap();
      map.invalidateSize();
      map.eachLayer(function(layer) {
        if (layer instanceof L.TileLayer)
          layer.redraw();
      });
    });
  }

  // trigger 'shown' so htmlwidgets resize
  $('d-article').trigger('shown');
}

function init_distill() {

  init_common();

  // create front matter
  var front_matter = $('<d-front-matter></d-front-matter>');
  $('#distill-front-matter').wrap(front_matter);

  // create d-title
  $('.d-title').changeElementType('d-title');

  // create d-byline
  var byline = $('<d-byline></d-byline>');
  $('.d-byline').replaceWith(byline);

  // create d-article
  var article = $('<d-article></d-article>');
  $('.d-article').wrap(article).children().unwrap();

  // move posts container into article
  $('.posts-container').appendTo($('d-article'));

  // create d-appendix
  $('.d-appendix').changeElementType('d-appendix');

  // flag indicating that we have appendix items
  var appendix = $('.appendix-bottom').children('h3').length > 0;

  // replace footnotes with <d-footnote>
  $('.footnote-ref').each(function(i, val) {
    appendix = true;
    var href = $(this).attr('href');
    var id = href.replace('#', '');
    var fn = $('#' + id);
    var fn_p = $('#' + id + '>p');
    fn_p.find('.footnote-back').remove();
    var text = fn_p.html();
    var dtfn = $('<d-footnote></d-footnote>');
    dtfn.html(text);
    $(this).replaceWith(dtfn);
  });
  // remove footnotes
  $('.footnotes').remove();

  // move refs into #references-listing
  $('#references-listing').replaceWith($('#refs'));

  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    var id = $(this).attr('id');
    $('.d-contents a[href="#' + id + '"]').parent().remove();
    appendix = true;
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
  });

  // show d-appendix if we have appendix content
  $("d-appendix").css('display', appendix ? 'grid' : 'none');

  // localize layout chunks to just output
  $('.layout-chunk').each(function(i, val) {

    // capture layout
    var layout = $(this).attr('data-layout');

    // apply layout to markdown level block elements
    var elements = $(this).children().not('details, div.sourceCode, pre, script');
    elements.each(function(i, el) {
      var layout_div = $('<div class="' + layout + '"></div>');
      if (layout_div.hasClass('shaded')) {
        var shaded_content = $('<div class="shaded-content"></div>');
        $(this).wrap(shaded_content);
        $(this).parent().wrap(layout_div);
      } else {
        $(this).wrap(layout_div);
      }
    });


    // unwrap the layout-chunk div
    $(this).children().unwrap();
  });

  // remove code block used to force  highlighting css
  $('.distill-force-highlighting-css').parent().remove();

  // remove empty line numbers inserted by pandoc when using a
  // custom syntax highlighting theme
  $('code.sourceCode a:empty').remove();

  // load distill framework
  load_distill_framework();

  // wait for window.distillRunlevel == 4 to do post processing
  function distill_post_process() {

    if (!window.distillRunlevel || window.distillRunlevel < 4)
      return;

    // hide author/affiliations entirely if we have no authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
    if (!have_authors)
      $('d-byline').addClass('hidden');

    // article with toc class
    $('.d-contents').parent().addClass('d-article-with-toc');

    // strip links that point to #
    $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

    // add orcid ids
    $('.authors-affiliations').find('.author').each(function(i, el) {
      var orcid_id = front_matter.authors[i].orcidID;
      if (orcid_id) {
        var a = $('<a></a>');
        a.attr('href', 'https://orcid.org/' + orcid_id);
        var img = $('<img></img>');
        img.addClass('orcid-id');
        img.attr('alt', 'ORCID ID');
        img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
        a.append(img);
        $(this).append(a);
      }
    });

    // hide elements of author/affiliations grid that have no value
    function hide_byline_column(caption) {
      $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
    }

    // affiliations
    var have_affiliations = false;
    for (var i = 0; i<front_matter.authors.length; ++i) {
      var author = front_matter.authors[i];
      if (author.affiliation !== "&nbsp;") {
        have_affiliations = true;
        break;
      }
    }
    if (!have_affiliations)
      $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

    // published date
    if (!front_matter.publishedDate)
      hide_byline_column("Published");

    // document object identifier
    var doi = $('d-byline').find('h3:contains("DOI")');
    var doi_p = doi.next().empty();
    if (!front_matter.doi) {
      // if we have a citation and valid citationText then link to that
      if ($('#citation').length > 0 && front_matter.citationText) {
        doi.html('Citation');
        $('<a href="#citation"></a>')
          .text(front_matter.citationText)
          .appendTo(doi_p);
      } else {
        hide_byline_column("DOI");
      }
    } else {
      $('<a></a>')
         .attr('href', "https://doi.org/" + front_matter.doi)
         .html(front_matter.doi)
         .appendTo(doi_p);
    }

     // change plural form of authors/affiliations
    if (front_matter.authors.length === 1) {
      var grid = $('.authors-affiliations');
      grid.children('h3:contains("Authors")').text('Author');
      grid.children('h3:contains("Affiliations")').text('Affiliation');
    }

    // remove d-appendix and d-footnote-list local styles
    $('d-appendix > style:first-child').remove();
    $('d-footnote-list > style:first-child').remove();

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // hoverable references
    $('span.citation[data-cites]').each(function() {
      var refs = $(this).attr('data-cites').split(" ");
      var refHtml = refs.map(function(ref) {
        return "<p>" + $('#ref-' + ref).html() + "</p>";
      }).join("\n");
      window.tippy(this, {
        allowHTML: true,
        content: refHtml,
        maxWidth: 500,
        interactive: true,
        interactiveBorder: 10,
        theme: 'light-border',
        placement: 'bottom-start'
      });
    });

    // clear polling timer
    clearInterval(tid);

    // show body now that everything is ready
    on_load_complete();
  }

  var tid = setInterval(distill_post_process, 50);
  distill_post_process();

}

function init_downlevel() {

  init_common();

   // insert hr after d-title
  $('.d-title').after($('<hr class="section-separator"/>'));

  // check if we have authors
  var front_matter = JSON.parse($("#distill-front-matter").html());
  var have_authors = front_matter.authors && front_matter.authors.length > 0;

  // manage byline/border
  if (!have_authors)
    $('.d-byline').remove();
  $('.d-byline').after($('<hr class="section-separator"/>'));
  $('.d-byline a').remove();

  // remove toc
  $('.d-contents').remove();

  // move appendix elements
  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
  });


  // inject headers into references and footnotes
  var refs_header = $('<h3></h3>');
  refs_header.text('References');
  $('#refs').prepend(refs_header);

  var footnotes_header = $('<h3></h3');
  footnotes_header.text('Footnotes');
  $('.footnotes').children('hr').first().replaceWith(footnotes_header);

  // move appendix-bottom entries to the bottom
  $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
  $('.appendix-bottom').remove();

  // remove appendix if it's empty
  if ($('.d-appendix').children().length === 0)
    $('.d-appendix').remove();

  // prepend separator above appendix
  $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

  // trim code
  $('pre>code').each(function(i, val) {
    $(this).html($.trim($(this).html()));
  });

  // move posts-container right before article
  $('.posts-container').insertBefore($('.d-article'));

  $('body').addClass('downlevel');

  on_load_complete();
}


function init_common() {

  // jquery plugin to change element types
  (function($) {
    $.fn.changeElementType = function(newType) {
      var attrs = {};

      $.each(this[0].attributes, function(idx, attr) {
        attrs[attr.nodeName] = attr.nodeValue;
      });

      this.replaceWith(function() {
        return $("<" + newType + "/>", attrs).append($(this).contents());
      });
    };
  })(jQuery);

  // prevent underline for linked images
  $('a > img').parent().css({'border-bottom' : 'none'});

  // mark non-body figures created by knitr chunks as 100% width
  $('.layout-chunk').each(function(i, val) {
    var figures = $(this).find('img, .html-widget');
    if ($(this).attr('data-layout') !== "l-body") {
      figures.css('width', '100%');
    } else {
      figures.css('max-width', '100%');
      figures.filter("[width]").each(function(i, val) {
        var fig = $(this);
        fig.css('width', fig.attr('width') + 'px');
      });

    }
  });

  // auto-append index.html to post-preview links in file: protocol
  // and in rstudio ide preview
  $('.post-preview').each(function(i, val) {
    if (window.location.protocol === "file:")
      $(this).attr('href', $(this).attr('href') + "index.html");
  });

  // get rid of index.html references in header
  if (window.location.protocol !== "file:") {
    $('.distill-site-header a[href]').each(function(i,val) {
      $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
    });
  }

  // add class to pandoc style tables
  $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
  $('.kable-table').children('table').addClass('pandoc-table');

  // add figcaption style to table captions
  $('caption').parent('table').addClass("figcaption");

  // initialize posts list
  if (window.init_posts_list)
    window.init_posts_list();

  // implmement disqus comment link
  $('.disqus-comment-count').click(function() {
    window.headroom_prevent_pin = true;
    $('#disqus_thread').toggleClass('hidden');
    if (!$('#disqus_thread').hasClass('hidden')) {
      var offset = $(this).offset();
      $(window).resize();
      $('html, body').animate({
        scrollTop: offset.top - 35
      });
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  if (is_downlevel_browser())
    init_downlevel();
  else
    window.addEventListener('WebComponentsReady', init_distill);
});

</script>

<!--/radix_placeholder_distill-->
  <script src="../../site_libs/header-attrs-2.13/header-attrs.js"></script>
  <script src="../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="../../site_libs/popper-2.6.0/popper.min.js"></script>
  <link href="../../site_libs/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="../../site_libs/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="../../site_libs/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="../../site_libs/anchor-4.2.2/anchor.min.js"></script>
  <script src="../../site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="../../site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="../../site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
<!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Lasso Regression & Simulation","description":"This project was conducted for the capstone course: STAT 455 Mathematical Statistics. We wrote a report and created a simulation to visualize the bias variance tradeoff with penalized regressions!","authors":[{"author":"Nicholas Di, Ellery Island, and Will Orser","authorURL":"#","affiliation":"&nbsp;","affiliationURL":"#","orcidID":""}],"publishedDate":"2022-05-05T00:00:00.000-04:00","citationText":"Orser, 2022"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<a href="../../index.html" class="title"></a>
<input id="distill-search" class="nav-search hidden" type="text" placeholder="Search..."/>
</div>
<div class="nav-right">
<a href="../../index.html">Home</a>
<a href="../../about.html">About Me</a>
<a href="../../projects.html">Projects</a>
<a href="../../CV.html">CV</a>
<a href="https://www.linkedin.com/in/nicholas-di-96322a170/">
<i class="fa fa-linkedin" aria-hidden="true"></i>
</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div>
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Lasso Regression &amp; Simulation</h1>
<!--radix_placeholder_categories-->
<!--/radix_placeholder_categories-->
<p><p>This project was conducted for the capstone course: STAT 455
Mathematical Statistics. We wrote a report and created a simulation to
visualize the bias variance tradeoff with penalized regressions!</p></p>
</div>

<div class="d-byline">
  
  Nicholas Di, Ellery Island, and Will Orser
  
<br/>05-05-2022
</div>

<div class="d-article">
<h2 id="introduction">Introduction</h2>
<p>Lasso, an abbreviation for “least absolute shrinkage and selection
operator”, was developed independently in the field of geophysics in
1986 (“Lasso (statistics)”). The technique was rediscovered, named, and
popularized by statistician Robert Tibshirani in 1996, in his paper
“Regression Shrinkage and Selection via the Lasso”. The topic of lasso
stood out to our group as an option for the final project because we
have all had experiences applying the technique in our Machine Learning
courses. Lasso is also connected to the section of our Mathematical
Statistics course devoted to linear models. In particular, lasso was
developed as a method to overcome certain complaints that data analysts
had with ordinary least squares (OLS) regression models, namely,
prediction accuracy and interpretation. OLS estimates often have low
bias but high variance, meaning that prediction accuracy can sometimes
be improved by shrinking or setting to zero some regression
coefficients. Further, OLS models typically contain a large number of
predictors; we often would like to narrow this down to a smaller subset
that exhibits the strongest effects <span class="citation"
data-cites="JSTOR">(<a href="#ref-JSTOR"
role="doc-biblioref">Tibshirani, n.d.</a>)</span>.</p>
<p>Lasso falls under the category of penalized or regularized regression
methods. Penalized regression methods keep all the predictor variables
in a model but constrain or regularize their regression coefficients by
shrinking them towards zero. In certain cases, if the amount of
shrinkage is large enough, these methods can also serve as variable
selection techniques by shrinking some coefficients to zero <span
class="citation" data-cites="SaSInstitude">(<a href="#ref-SaSInstitude"
role="doc-biblioref">Gunes 2015</a>)</span>. This is the case with
lasso, which provides both variable selection and regularization to
enhance the prediction accuracy and the interpretability of the
resulting statistical model. Lasso was originally developed for use on
linear regression models, but is easily extended to other statistical
models including generalized linear models, generalized estimating
equations, and proportional hazards models (“Lasso (statistics)”). In
terms of real world applications, lasso is commonly used to handle
genetic data because the number of potential predictors is often large
relative to the number of observations and there is often little prior
knowledge to inform variable selection <span class="citation"
data-cites="BJSSociety">(<a href="#ref-BJSSociety"
role="doc-biblioref">Ranstam and Cook, n.d.</a>)</span>.</p>
<p>The sources we explored to learn about lasso in greater depth were
“LASSO regression”, a brief overview of the technique written by J.
Ranstam and J.A. Cook, Tibshirani’s paper mentioned above, and the
chapter on lasso in An Introduction to Statistical Learning (ISLR; a
statistics textbook commonly used in Machine Learning courses) by Gareth
James et al. </p>
<p>Ranstam and Cook provide a nice introductory look into lasso,
explaining the motivation behind the method (standard regression models
often overfit the data and overestimate the model’s predictive power), a
general description of how lasso works including the role of
cross-validation in selecting the tuning parameter <span
class="math inline">\(\lambda\)</span>, and some of the limitations of
the method.</p>
<p>Tibshirani’s paper proposes a new method for estimation in linear
models (“the lasso”), explains the mathematical derivation of this
method, and presents the results of various simulation studies,
comparing the novel method to more established methods of variable
selection and regularization, subset selection and ridge regression.
Tibshirani concludes by examining the relative merits of the three
methods in different scenarios, stating that lasso performs best in
situations where the predictors represent a small to medium number of
moderate-sized effects.</p>
<p>ISLR provided us with the most comprehensive (and understandable)
look into lasso. ISLR explains the mathematics involved in lasso and
provides an in-depth comparison to ridge regression at the mathematical,
geometrical, and functional levels. The textbook concludes that neither
method will universally dominate the other, but that lasso tends to
perform better in situations where only a relatively small number of
predictors have substantial coefficients, while ridge regression tends
to perform better when the response variable is a function of many
predictors, all with coefficients of relatively equal size. Finally,
ISLR proved extremely useful to us because it included various graphs
and visualizations that illustrate how and why lasso works the way it
does.</p>
<p>In the background section of this report, we will describe the
mathematical underpinnings of the lasso, ridge regression and OLS
regression. This will include notation, an explanation of the “penalty
term” used in lasso and ridge regression, and alternate interpretations
of how lasso and ridge regression work. In the main results section, we
derive the estimators for OLS and ridge regression and create a
simulation to understand the lasso estimators. We will introduce the
set-up for a simulation experiment using R that demonstrates the merits
and drawbacks of using lasso in comparison to OLS regression. Then, we
will compare relevant aspects of the models: regression coefficients,
error metrics, and the bias and variance of model predictions. The
discussion section summarizes the main takeaways of our research.</p>
<h2 id="background">Background</h2>
<h3 id="overfitting-and-the-bias-variance-tradeoff">Overfitting and the
Bias-Variance Tradeoff</h3>
<p>When models are created, a specific set of data is used to ‘train’
them. From this training data, all the coefficients and other parameters
of the model are determined. Even though a model is trained on a very
specific set of data, it is often applied to other data sets. A model
that is ‘overfit’ to the training data will make accurate predictions
for the training data, but will make significantly less accurate
predictions when applied to different data. Overfitting occurs when the
model is too sensitive to the training data and ends up picking up on,
and modeling, random quirks of this subset of data. We wish to avoid
overfitting our models to ensure that they are able to make accurate
predictions on unknown data <span class="citation"
data-cites="Springer">(<a href="#ref-Springer"
role="doc-biblioref">Gareth James and Tibshirani 2013</a>)</span>.</p>
<p>Two important properties of a model and its parameters are bias and
variance. Bias is the difference between the average value that the
model predicts and the true average; we want our model to be pinpointing
the correct average, but this is often extremely challenging to do
because models are simplifications of more complicated phenomena.
Variance describes how much the estimates of a model would change if the
model was fit using a different dataset. We do not want our model
estimates to fluctuate widely when different data is used; this is an
indication that the model is not capturing trends common to all the
data. Overfit models tend to have low bias, but high variance – they are
able to very accurately capture the trends of the training data, but
they do not generalize well to other data. Ideally, we would like to
minimize both bias and variance, but it turns out that these two
properties are interrelated. Decreasing bias tends to increase variance
and decreasing variance tends to increase bias. When constructing a
model, the goal is balance between bias and variance effectively to
yield an accurate, yet more general model <span class="citation"
data-cites="Springer">(<a href="#ref-Springer"
role="doc-biblioref">Gareth James and Tibshirani 2013</a>)</span>.</p>
<h3 id="variable-selection">Variable Selection</h3>
<p>Whenever we are trying to model data with many possible predictors,
we want to determine which variables are important for predicting the
outcome variable. We could include every predictor but often this yields
a complicated and less meaningful model. Variable selection is the
ability of some models to choose which variables are irrelevant to the
model and which variables help predict the outcome variable. Models
accomplish variable selection by setting a variable’s coefficient equal
to 0. Variable selection is an extremely useful ability of some models,
especially when data context cannot inform variable selection <span
class="citation" data-cites="Springer">(<a href="#ref-Springer"
role="doc-biblioref">Gareth James and Tibshirani 2013</a>)</span>.</p>
<h3 id="ordinary-least-squares-estimation">Ordinary Least Squares
Estimation</h3>
<p>In ordinary least squares estimation (OLS), we attempt to find a
linear model that best fits the data. Our model is a polynomial <span
class="math inline">\(\hat{y} = \beta_0 +\beta_1x_1 + \beta_2x_2 +
\space ... \space + \beta_nx_n\)</span> with unknown coefficients <span
class="math inline">\(\beta_0, \space \beta_1, \space \beta_2, \space
.., \space \beta_n\)</span>. In the method of least squares, we find the
values of these coefficients that minimize the distance between the true
<span class="math inline">\(y\)</span> values and the predicted <span
class="math inline">\(y\)</span> values <span
class="math inline">\(\hat{y}\)</span>. We define this distance as a
residual: <span class="math inline">\(y_i- \hat{y}\)</span>. To get an
overall estimate of the prediction error of our model, we compute the
residual for each observation, square the residuals and sum these values
<span class="citation" data-cites="Springer">(<a href="#ref-Springer"
role="doc-biblioref">Gareth James and Tibshirani 2013</a>)</span>. We
can write this as:</p>
<p><span class="math display">\[
\sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n (y_i - [\beta_0
+\beta_1x_1 + \space ...  \space + \beta_nx_n])^2 \\
= \sum_{i=1}^n ( y_i +\beta_0 - \sum_{j=1}^p \beta_jx_{ij} )^2
\]</span> We can summarize the least squares method as: <span
class="math display">\[
\text{argmin}_{\beta_0,..., \beta_n}\sum_{i=1}^n ( y_i +\beta_0 -
\sum_{j=1}^p \beta_jx_{ij} )^2
\]</span> Instead of using standard mathematical notation, we can write
linear models and the least squares method in matrix notation. In matrix
notation, a linear model is written as:</p>
<p><span class="math display">\[\mathbf{y} =
\mathbf{X}\boldsymbol\beta  + \boldsymbol\epsilon, \text{ where }
E[\boldsymbol\epsilon] = \mathbf{0}\]</span>.</p>
<p><span class="math inline">\(\mathbf{y}\)</span> is the vector of
outcomes, <span class="math inline">\(\boldsymbol\beta\)</span> is the
vector of covariates, and <span
class="math inline">\(\mathbf{X}\)</span> is the matrix of covariates:
<span class="math display">\[\mathbf{y} = \begin{pmatrix} y_1 \\ y_2 \\
\vdots \\ y_n \end{pmatrix}; \space\boldsymbol\beta = \begin{pmatrix}
\beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{pmatrix}; \space \mathbf{X}
= \begin{pmatrix} 1 &amp; x_{11} &amp; \cdots &amp; x_{p1} \\ 1 &amp;
x_{12} &amp; \cdots &amp; x_{p2} \\ \vdots &amp; \vdots &amp; \ddots
&amp; \vdots \\ 1 &amp; x_{1n} &amp; \cdots &amp; x_{pn}
\end{pmatrix}.\]</span> The least squares estimation method then
becomes:</p>
<p><span class="math display">\[\text{argmin}_{\boldsymbol\beta}
(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top(\mathbf{y} -
\mathbf{X}\boldsymbol\beta)\]</span>.</p>
<h3 id="problems-with-ordinary-least-squares-estimation">Problems with
Ordinary Least Squares Estimation</h3>
<p>OLS models are incredibly useful and form the basis of many other
models, but they have problems that other models can address. OLS models
tend to overfit the data, leading to highly variable predictions when
they are applied to new data. They have high variance, especially when
making predictions on the extreme, and thus do not generalize to new
contexts. Additionally, they cannot perform variable selection, making
the models challenging to interpret when there are a large number of
predictors. Furthermore, OLS models struggle when predictors are
correlated <span class="citation" data-cites="Springer">(<a
href="#ref-Springer" role="doc-biblioref">Gareth James and Tibshirani
2013</a>)</span>. Because of these problems, OLS models are not
appropriate in many circumstances, even when a linear model is a good
option.</p>
<h3 id="lasso">Lasso</h3>
<p>Lasso is an adjustment to the linear regression framework. In a lasso
model, the goal is the same as for OLS model: minimize the RSS. However,
we add an additional penalty term, shown in red below, that limits the
values of the coefficients <span class="citation"
data-cites="Springer">(<a href="#ref-Springer"
role="doc-biblioref">Gareth James and Tibshirani 2013</a>)</span>.
Specifically, lasso is defined as:</p>
<p><span class="math display">\[\text{argmin}_{\beta_j}\sum_{i=1}^n (
y_i +\beta_0 - \sum_{j=1}^p \beta_jx_{ij} )^2 + \color{red}{\lambda
\sum_{j=1}^p |\beta_j|}\]</span></p>
<p>When minimizing this quantity as a whole, we are minimizing each
component – both the RSS and the penalty term. Minimizing the penalty
term, for a given <span class="math inline">\(\lambda\)</span>, has the
effect of reducing the values of the coefficients towards zero <span
class="citation" data-cites="Springer">(<a href="#ref-Springer"
role="doc-biblioref">Gareth James and Tibshirani 2013</a>)</span>. The
constant <span class="math inline">\(\lambda\)</span> allows us to
control how much the coefficients are shrunk towards zero and is thus
considered a tuning parameter for lasso models. Large <span
class="math inline">\(\lambda\)</span> values weight the penalty term
heavily, so the coefficient values must be very small to minimize the
overall function. Small <span class="math inline">\(\lambda\)</span>
values reduce the importance of the penalty term allowing the
coefficients to be larger. In the extreme, if <span
class="math inline">\(\lambda\)</span> is infinitely large, the
coefficients would all become zero; if <span
class="math inline">\(\lambda\)</span> is zero, the coefficients would
be the OLS solution <span class="citation" data-cites="Springer">(<a
href="#ref-Springer" role="doc-biblioref">Gareth James and Tibshirani
2013</a>)</span>. We discuss how to choose <span
class="math inline">\(\lambda\)</span> in the next section.</p>
<p>There is an alternate formulation of lasso that reveals how it is a
constrained optimization problem. In this formulation, we define lasso
as: <span class="math display">\[
\text{argmin}_{\beta_j}\sum_{i=1}^n ( y_i +\beta_0 - \sum_{j=1}^p
\beta_jx_{ij} )^2  \text{; subject to }  \sum_{j=1}^p |\beta_j| \le s.
\]</span> In this formulation it is clear that the goal remains to
minimize the RSS; however, the values of the coefficients are subjected
to an additional constraint. Instead of using the tuning parameter <span
class="math inline">\(\lambda\)</span>, the tuning parameter <span
class="math inline">\(s\)</span> is used. For large values of <span
class="math inline">\(s\)</span>, the coefficients are unconstrained and
can have large values. Small values of <span
class="math inline">\(s\)</span> impose a tight constraint on the
coefficients, forcing them to be small <span class="citation"
data-cites="Springer">(<a href="#ref-Springer"
role="doc-biblioref">Gareth James and Tibshirani 2013</a>)</span>. With
this formulation of lasso, we can visualize the relationship between the
RSS and the constraint in a two predictors setting. With two predictors,
the constraint region is defined as <span
class="math inline">\(|\beta_1| + |\beta_2| \le s\)</span>; this is a
diamond with height <span class="math inline">\(s\)</span>. In the graph
below, the blue diamond is the constraint region, the red ellipses
represent contour lines of the RSS, and <span
class="math inline">\(\hat{\beta}\)</span> is the OLS solution (the
absolute minimum of the RSS). In a lasso model, the goal is to find the
smallest RSS that is within the constraint region; in this graph, that
is the point where the ellipses intersect the diamond at its top corner
<span class="citation" data-cites="Springer">(<a href="#ref-Springer"
role="doc-biblioref">Gareth James and Tibshirani 2013</a>)</span>.</p>
<p><img src="lasso_vis.png" /></p>
<h3 id="selecting-the-tuning-parameter">Selecting the Tuning
Parameter</h3>
<p>The tuning parameter is often selected using cross validation. With
cross validation, the data are randomly divided into equally sized
groups called folds. In one iteration, k-1 folds are reserved for
training the model and 1 fold is reserved for testing the model. The
error in the predictions generated by the model is computed for the test
fold. This process is repeated until all the folds are used for testing.
Then, the average test error is computed across all the folds. For
selecting <span class="math inline">\(\lambda\)</span>, we compute cross
validated error metrics for many different values of <span
class="math inline">\(\lambda\)</span> and choose a value of <span
class="math inline">\(\lambda\)</span> that leads to low error <span
class="citation" data-cites="Springer">(<a href="#ref-Springer"
role="doc-biblioref">Gareth James and Tibshirani 2013</a>)</span>.</p>
<h3 id="comparison-to-ridge-regression">Comparison to Ridge
Regression</h3>
<p>Ridge regression is another technique that modifies the OLS framework
by constraining the values of the coefficients. Ridge regression is
defined as: <span
class="math display">\[\text{argmin}_{\beta_j}\sum_{i=1}^n ( y_i
+\beta_0 - \sum_{j=1}^p \beta_jx_{ij} )^2 + \color{red}{\lambda
\sum_{j=1}^p (\beta_j)^2}\]</span>. We can see that ridge regression is
nearly identical to lasso; the only difference is in the penalty term
(shown above in red). Instead of taking the absolute value of the
coefficients, ridge regression squares the coefficients (James et al.,
2013). We can consider the constrained optimization formulation of ridge
regression, as we did for lasso: <span class="math display">\[
\text{argmin}_{\beta_j}\sum_{i=1}^n ( y_i +\beta_0 - \sum_{j=1}^p
\beta_jx_{ij} )^2  \text{; subject to }  \sum_{j=1}^p (\beta_j)^2 \le s.
\]</span> With two predictors, the constraint region becomes a circle:
<span class="math inline">\(\beta_1^2 + \beta_2^2 \le s^2\)</span>
(James et al., 2013). We can construct a very similar graph to the one
above:</p>
<p><img src="ridge_vis.png" /> By comparing these two graph, we can tell
that the only difference between lasso and ridge regression are their
constraint regions. In the next section, we discuss an important
implication of this difference.</p>
<h3 id="the-constraint-region-and-variable-selection">The Constraint
Region and Variable Selection</h3>
<p>Lasso’s constraint region allows it to perform variable selection,
while ridge regression’s does not. In the two dimensional example,
lasso’s constraint region is a diamond. In a diamond, the points that
line farthest from the center, the points that are most likely to
intersect with the RSS contours, are the corners. These corners lie on
the axes; if an RSS contour intersects the constraint region at a
corner, one coefficient will be set to 0. If a coefficient is set to 0,
it is selected out of the model. For ridge regression’s circular
constraint region, all of the points on the perimeter lie equidistant to
the center – no point is more likely to intersect an RSS contour than
any other point. So, the contours lines do not intersect at an axis for
ridge regression, making it impossible for this technique to perform
variable selection <span class="citation" data-cites="Springer">(<a
href="#ref-Springer" role="doc-biblioref">Gareth James and Tibshirani
2013</a>)</span>.</p>
<h3 id="benefits-of-lasso-and-ridge-regression">Benefits of Lasso and
Ridge Regression</h3>
<p>Both lasso and ridge regression are able to make more accurate
predictions than OLS in many contexts. Lasso and ridge regression are
often more accurate than OLS because they sacrifice a small increase in
bias for a significant reduction in variance. Both ridge regression and
lasso perform well in a variety of contexts, but the variable selection
property of lasso is a significant advantage. Lasso models have fewer
predictors, making them easier to interpret. Ridge regression, because
it includes every variable in the model, outperforms lasso when all of
the predictors are related to the outcome. On the other hand, lasso
outperforms ridge regression when only a few of the predictors are
related to the outcome <span class="citation" data-cites="Springer">(<a
href="#ref-Springer" role="doc-biblioref">Gareth James and Tibshirani
2013</a>)</span>.</p>
<p>In the main results section, we will derived the variance of OLS and
ridge regression estimators and perform a simulation to examine bias and
variance in lasso estimators.</p>
<h2 id="main-results">Main Results</h2>
<h3 id="deriving-ols-ridge-regression-and-lasso-estimators">Deriving
OLS, Ridge Regression and Lasso Estimators</h3>
<h4 id="ols">OLS</h4>
<p>As described above, the OLS problem can be written as <span
class="math inline">\(\text{argmin}_{\boldsymbol\beta} (\mathbf{y} -
\mathbf{X}\boldsymbol\beta)^\top(\mathbf{y} -
\mathbf{X}\boldsymbol\beta)\)</span>.</p>
<p>We can derive the OLS estimate for <span
class="math inline">\(\boldsymbol\beta\)</span>:</p>
<span class="math display">\[\begin{aligned}

&amp;\text{argmin}_{\boldsymbol\beta} (\mathbf{y} -
\mathbf{X}\boldsymbol\beta)^\top(\mathbf{y} -
\mathbf{X}\boldsymbol\beta) \\

&amp;= \frac{\partial}{\partial \boldsymbol\beta} (\mathbf{y}^\top
\mathbf{y} - \mathbf{y}^\top\mathbf{X}\boldsymbol\beta  -
\boldsymbol\beta^T\mathbf{X}^Ty + \boldsymbol\beta^\top \mathbf{X}^\top
\mathbf{X} \boldsymbol\beta) \\


&amp;= \frac{\partial}{\partial \boldsymbol\beta} (\mathbf{y}^\top
\mathbf{y} - 2\mathbf{y}^\top\mathbf{X}\boldsymbol\beta +
\boldsymbol\beta^\top \mathbf{X}^\top \mathbf{X} \boldsymbol\beta) \\

&amp;= -2\mathbf{X}^\top\mathbf{y} + 2 \mathbf{X}^\top \mathbf{X}
\boldsymbol\beta \\

0 &amp;\stackrel{set}{=} -2\mathbf{X}^\top\mathbf{y} + 2 \mathbf{X}^\top
\mathbf{X} \boldsymbol\beta \\

2 \mathbf{X}^\top \mathbf{X} \boldsymbol\beta &amp;=
2\mathbf{X}^\top\mathbf{y} \\

(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^\top \mathbf{X} \boldsymbol\beta
&amp;= (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^\top\mathbf{y} \\

\hat{\boldsymbol\beta}&amp; = (\mathbf{X}^T\mathbf{X})^{-1}
\mathbf{X}^\top\mathbf{y}

\end{aligned}\]</span>
<h4 id="ridge-regression">Ridge Regression</h4>
<p>In ridge regression, the formula we are trying to minimize is <span
class="math inline">\(\sum_{i=1}^n(y_i - \beta_0 - \sum_{j=1}^p\beta_j
x_{ij})^2 + \lambda\sum_{j=1}^p \beta_j^2\)</span>. We can write this in
matrix notation as: <span class="math inline">\((\mathbf{y} -
\mathbf{X}\boldsymbol\beta)^\top(\mathbf{y} -
\mathbf{X}\boldsymbol\beta) + \lambda
\boldsymbol\beta^T\boldsymbol\beta\)</span>. We can minimize this in
much the same way as in OLS:</p>
<span class="math display">\[\begin{aligned}
&amp;\text{argmin}_{\boldsymbol\beta} (\mathbf{y} -
\mathbf{X}\boldsymbol\beta)^\top(\mathbf{y} -
\mathbf{X}\boldsymbol\beta) + \lambda \boldsymbol\beta^T\boldsymbol\beta
\\
&amp;= \frac{\partial}{\partial \boldsymbol\beta} (\mathbf{y}^\top
\mathbf{y} - 2\mathbf{y}^\top\mathbf{X}\boldsymbol\beta +
\boldsymbol\beta^\top \mathbf{X}^\top \mathbf{X} \boldsymbol\beta +
\lambda \boldsymbol\beta^T\boldsymbol\beta) \\
&amp;= -2\mathbf{X}^\top\mathbf{y} + 2 \mathbf{X}^\top \mathbf{X}
\boldsymbol\beta + 2\lambda\boldsymbol\beta \\
0 &amp;\stackrel{set}{=} -2\mathbf{X}^\top\mathbf{y} + 2 \mathbf{X}^\top
\mathbf{X} \boldsymbol\beta + 2\lambda\boldsymbol\beta\\
\mathbf{X}^\top \mathbf{X} \boldsymbol\beta + \lambda\boldsymbol\beta
&amp;= \mathbf{X}^\top\mathbf{y} \\
(\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I}) \boldsymbol\beta &amp;=
\mathbf{X}^\top\mathbf{y} \\
(\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I}) (\mathbf{X}^\top
\mathbf{X} + \lambda\mathbf{I}) ^{-1}\boldsymbol\beta &amp;=
\mathbf{X}^\top\mathbf{y}(\mathbf{X}^\top \mathbf{X} +
\lambda\mathbf{I}) ^{-1}\\
\boldsymbol\beta &amp;= \mathbf{X}^\top\mathbf{y}(\mathbf{X}^\top
\mathbf{X} + \lambda\mathbf{I}) ^{-1}\\
\end{aligned}\]</span>
<h4 id="considering-a-simple-case">Considering a Simple Case</h4>
<p>We can consider a simple case: <span
class="math inline">\(\mathbf{X}\)</span> is a diagonal matrix with 1’s
on the diagonals and 0’s on all the off diagonals, the number of
predictors equals the number of cases, and we force the intercept to go
through the origin. This case allows us simplify our OLS and ridge
regression estimators. For OLS, the solution is <span
class="math inline">\(\boldsymbol\beta = \mathbf{y}\)</span> and for
ridge regression the solution becomes <span
class="math inline">\(\boldsymbol\beta =
\frac{\mathbf{y}}{1+\lambda}\)</span>. Applying this simple case to find
the estimators is helpful particularly for Lasso. Unlike OLS and Ridge
Regression, there is no closed form solution for <span
class="math inline">\(\boldsymbol\beta\)</span> for Lasso. To derive any
estimators for Lasso, we must consider this simple case.</p>
<h4 id="lasso-estimators-in-a-simple-case">Lasso Estimators in a Simple
Case</h4>
<p>For lasso, we can not find a general closed form solution for <span
class="math inline">\(\boldsymbol\beta\)</span>, so we will derive the
lasso estimates for <span
class="math inline">\(\boldsymbol\beta\)</span> for the simple case
described above. We will not use matrix notation in order to easily
apply the assumptions of our simple case.</p>
<p>Remember that we can write the general form of lasso as:</p>
<span class="math display">\[\begin{aligned}

\text{argmin}_{\beta}\sum_{i=1}^n(y_i - \beta_0 - \sum_{j=1}^p\beta_j
x_{ij})^2 + \lambda\sum_{j=1}^p |\beta_j|

\end{aligned}\]</span>
<p>If we apply our simplifying assumptions, we can write:</p>
<span class="math display">\[\begin{aligned}

\text{argmin}_{\beta}\sum_{j=1}^p(y_i - \beta_1)^2 + \lambda|\beta_1|

\end{aligned}\]</span>
<p>With these assumptions, we can find a closed form solution for <span
class="math inline">\(\beta\)</span>:</p>
<span class="math display">\[\begin{aligned}

&amp;\text{argmin}_{\beta}(y_i - \beta_1)^2 + \lambda|\beta_1| \\

&amp;= \frac{\partial}{\partial \beta} \left( (y_j - \beta_1)^2 +
\lambda|\beta_1| \right) \\

&amp;= \frac{\partial}{\partial \beta} \left( y_j^2 - 2y_j\beta_1 +
\beta_1^2 + \lambda|\beta_1| \right) \\

&amp;=  - 2y_j + 2\beta_1 + \lambda sign(\beta_1) \\

\end{aligned}\]</span>
<p>To solve for <span class="math inline">\(\beta_1\)</span>, we must
consider different regions: (1) when <span class="math inline">\(\beta_1
&lt; 0\)</span>, (2) when <span class="math inline">\(\beta_1 &gt;
0\)</span> and (3) when <span class="math inline">\(\beta_1 =
0\)</span>.</p>
<ol type="1">
<li>when <span class="math inline">\(\beta_1 &lt; 0\)</span> or when
<span class="math inline">\(y_j &lt; - \lambda/2\)</span>:</li>
</ol>
<span class="math display">\[\begin{aligned}

0 &amp;\stackrel{set}{=} - 2y_j + 2\beta_1 - \lambda \\

\beta_1 &amp;= y_j + \lambda/2 \\

\end{aligned}\]</span>
<ol start="2" type="1">
<li>when <span class="math inline">\(\beta_1 &gt; 0\)</span> or when
<span class="math inline">\(y_j &gt; \lambda/2\)</span>:</li>
</ol>
<span class="math display">\[\begin{aligned}

0 &amp;\stackrel{set}{=} - 2y_j + 2\beta_1 + \lambda \\

\beta_1 &amp;= y_j - \lambda/2 \\

\end{aligned}\]</span>
<ol start="3" type="1">
<li>when <span class="math inline">\(\beta_1 = 0\)</span>:</li>
</ol>
<span class="math display">\[\begin{aligned}

\text{when } \beta_1 = 0 \text{ or when } |y_i| \le \lambda/2 : \\
0

\end{aligned}\]</span>
<h4 id="visualizing-the-simple-case-estimators">Visualizing the Simple
Case Estimators</h4>
<p>The graph below shows the simple case coefficient estimates for OLS,
ridge regression and lasso as a function of the data <span
class="math inline">\(y_j\)</span>. We can see from that graph, and from
the equations derived above, that ridge regression scales the
coefficient estimates by the same factor, <span
class="math inline">\(1/(1+\lambda)\)</span>, regardless of the value of
<span class="math inline">\(y_j\)</span>. Since it is impossible to
divide a non-zero number by any value and get 0, ridge regression cannot
set any coefficient to zero unless it is already 0. However, lasso
performs shrinkage in a different way, allowing some coefficients to be
0. Lasso changes the values of the coefficients by adding or subtracting
<span class="math inline">\(\lambda/2\)</span>, depending on the
corresponding <span class="math inline">\(y_j\)</span>. If <span
class="math inline">\(y_j\)</span> is inside the region <span
class="math inline">\((-\lambda/2, \lambda/2)\)</span>, the coefficient
is shrunk to 0.</p>
<div class="layout-chunk" data-layout="l-body">
<p><img src="LassoProject_files/figure-html5/unnamed-chunk-1-1.png" width="624" /></p>
</div>
<h2
id="deriving-bias-and-variance-of-ols-and-ridge-regression-estimators">Deriving
Bias and Variance of OLS and Ridge Regression Estimators</h2>
<h3 id="ols-1">OLS</h3>
<h4 id="bias">Bias</h4>
<p>We will assume that <span class="math inline">\(\mathbf{y} =
\mathbf{X}\boldsymbol\beta + \boldsymbol\epsilon\)</span> and that <span
class="math inline">\(E[\boldsymbol\epsilon] = \mathbf{0}\)</span>. We
can show that the least squares estimator <span
class="math inline">\(\hat{\boldsymbol\beta} =
(\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^\top\mathbf{y}\)</span> is an
unbiased estimator of <span
class="math inline">\(\boldsymbol\beta\)</span>:</p>
<span class="math display">\[\begin{aligned}

E[\hat{\boldsymbol\beta}_{OLS}] &amp;= E[(\mathbf{X}^T\mathbf{X})^{-1}
\mathbf{X}^\top\mathbf{y}]\\
&amp;= (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^\top E[\mathbf{y}],
\text{ since X is fixed} \\
&amp;= (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^\top
E[\mathbf{X}\boldsymbol\beta  + \boldsymbol\epsilon], \text{ by
assumption}\\
&amp;= (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^\top
(\mathbf{X}\boldsymbol\beta  + E[\boldsymbol\epsilon])\\
&amp;= (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^\top
(\mathbf{X}\boldsymbol\beta  + 0), \text{ by assumption}\\
&amp;=(\mathbf{X}^T\mathbf{X})^{-1}
(\mathbf{X}^\top\mathbf{X})\boldsymbol\beta \\
&amp;= \boldsymbol\beta

\end{aligned}\]</span>
<h4 id="variance">Variance</h4>
<p>We will assume that <span class="math inline">\(\mathbf{y} =
\mathbf{X}\boldsymbol\beta + \boldsymbol\epsilon\)</span>, <span
class="math inline">\(E[\boldsymbol\epsilon] = \mathbf{0}\)</span> and
that <span class="math inline">\(Var[\boldsymbol\epsilon] = \sigma^2
\mathbf{I}\)</span>. We can show that the variance of the least squares
estimator <span class="math inline">\(\hat{\boldsymbol\beta} =
(\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^\top\mathbf{y}\)</span> is
<span class="math inline">\(Var[\hat{\boldsymbol\beta}] =
\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}\)</span>:</p>
<span class="math display">\[\begin{aligned}

Var[\hat{\boldsymbol\beta}_{OLS}] &amp;=
Var[(\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^\top\mathbf{y}]\\

&amp;= (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^\top
Var[\mathbf{y}]((\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^\top)^\top,
\text{ since } Var(\mathbf{Ax}) =
\mathbf{A}Var(\mathbf{x})\mathbf{A}^\top \\

&amp;= (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^\top Var[\mathbf{y}]
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}, \text{ since }
(\mathbf{AB})^\top = \mathbf{B}^\top\mathbf{A}^\top \text{ and }
(\mathbf{A}^{-1})^\top = (\mathbf{A}^\top)^{-1} \\

&amp;= (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^\top
Var[\mathbf{X}\boldsymbol\beta  + \boldsymbol\epsilon]
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}, \text{ by assumption}\\

&amp;= (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^\top
Var[\boldsymbol\epsilon] \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}, \text{
since } \mathbf{X} \text{ and } \boldsymbol{\beta} \text{ are fixed}\\

&amp;= (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^\top
(\sigma^2\mathbf{I}) \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}, \text{ by
assumption} \\

&amp;= \sigma^2(\mathbf{X}^T\mathbf{X})^{-1} (\mathbf{X}^\top
\mathbf{X})(\mathbf{X}^T\mathbf{X})^{-1} \\

&amp;= \sigma^2(\mathbf{X}^T\mathbf{X})^{-1} \\

\end{aligned}\]</span>
<h3 id="ridge-regression-1">Ridge Regression</h3>
<h4 id="bias-1">Bias</h4>
<p>We will assume that <span class="math inline">\(\mathbf{y} =
\mathbf{X}\boldsymbol\beta + \boldsymbol\epsilon\)</span> and that <span
class="math inline">\(E[\boldsymbol\epsilon] = \mathbf{0}\)</span>. We
can show that the ridge regression estimator <span
class="math inline">\(\boldsymbol\beta = (\mathbf{X}^\top \mathbf{X} +
\lambda\mathbf{I}) ^{-1}\mathbf{X}^\top\mathbf{y}\)</span> is a biased
estimator of <span class="math inline">\(\boldsymbol\beta\)</span>
(Taboga):</p>
<span class="math display">\[\begin{aligned}

E[\hat{\boldsymbol\beta}_{ridge}] &amp;= E[(\mathbf{X}^\top \mathbf{X} +
\lambda\mathbf{I}) ^{-1}\mathbf{X}^\top\mathbf{y}]\\

&amp;= E[(\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I})
^{-1}\mathbf{X}^\top (\mathbf{X}\boldsymbol\beta +
\boldsymbol\epsilon)], \text{ by assumption} \\

&amp;= E[(\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I})
^{-1}\mathbf{X}^\top (\mathbf{X}\boldsymbol\beta) + (\mathbf{X}^\top
\mathbf{X} + \lambda\mathbf{I}) ^{-1}\mathbf{X}^\top
(\boldsymbol\epsilon)] \\

&amp;= E[(\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I})
^{-1}\mathbf{X}^\top (\mathbf{X}\boldsymbol\beta)] + E[(\mathbf{X}^\top
\mathbf{X} + \lambda\mathbf{I}) ^{-1}\mathbf{X}^\top
(\boldsymbol\epsilon)] \\

&amp;= (\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I})
^{-1}\mathbf{X}^\top (\mathbf{X}\boldsymbol\beta) + (\mathbf{X}^\top
\mathbf{X} + \lambda\mathbf{I}) ^{-1}\mathbf{X}^\top
E[(\boldsymbol\epsilon)], \text{ since } \mathbf{X} \text{ and }
\boldsymbol{\beta} \text{ are fixed} \\

&amp;= (\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I})
^{-1}\mathbf{X}^\top (\mathbf{X}\boldsymbol\beta) + (\mathbf{X}^\top
\mathbf{X} + \lambda\mathbf{I}) ^{-1}\mathbf{X}^\top (0), \text{ by
assumption } \\

&amp;= (\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I})
^{-1}\mathbf{X}^\top \mathbf{X}\boldsymbol\beta  \\

\end{aligned}\]</span>
<p>Since <span class="math inline">\(E[\hat{\boldsymbol\beta}_{ridge}] =
(\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I}) ^{-1}\mathbf{X}^\top
\mathbf{X}\boldsymbol\beta\)</span>, the ridge regression estimator for
<span class="math inline">\(\boldsymbol{\beta}\)</span> will always be
biased, unless <span class="math inline">\(\lambda = 0\)</span>. If
<span class="math inline">\(\lambda = 0\)</span>, the ridge regression
estimator is equal to the OLS estimator, which we showed above is
unbiased.</p>
<h3 id="variance-1">Variance</h3>
<p>We will assume that <span class="math inline">\(\mathbf{y} =
\mathbf{X}\boldsymbol\beta + \boldsymbol\epsilon\)</span>, <span
class="math inline">\(E[\boldsymbol\epsilon] = \mathbf{0}\)</span> and
that <span class="math inline">\(Var[\boldsymbol\epsilon] = \sigma^2
\mathbf{I}\)</span>. We can show that the variance of the ridge
regression estimator is <span
class="math inline">\(\sigma^2(\mathbf{X}^\top \mathbf{X} +
\lambda\mathbf{I}) ^{-1}\mathbf{X}^\top \mathbf{X} (\mathbf{X}^\top
\mathbf{X} + \lambda\mathbf{I})^{-1}\)</span> <span class="citation"
data-cites="StatLect">(<a href="#ref-StatLect"
role="doc-biblioref">Taboga, n.d.</a>)</span>:</p>
<span class="math display">\[\begin{aligned}
Var[\hat{\boldsymbol\beta}_{ridge}] &amp;= Var((\mathbf{X}^\top
\mathbf{X} + \lambda\mathbf{I}) ^{-1}\mathbf{X}^\top\mathbf{y})\\

&amp;= (\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I})
^{-1}\mathbf{X}^\top Var(\mathbf{y}) ((\mathbf{X}^\top \mathbf{X} +
\lambda\mathbf{I}) ^{-1}\mathbf{X}^\top)^\top, \text{ since }
Var(\mathbf{Ax}) = \mathbf{A}Var(\mathbf{x})\mathbf{A}^\top \\

&amp;= (\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I})
^{-1}\mathbf{X}^\top Var(\mathbf{X}\boldsymbol\beta +
\boldsymbol\epsilon) ((\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I})
^{-1}\mathbf{X}^\top)^\top, \text{ by assumption } \\

&amp;= (\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I})
^{-1}\mathbf{X}^\top (Var(\mathbf{X}\boldsymbol\beta) +
Var(\boldsymbol\epsilon)) ((\mathbf{X}^\top \mathbf{X} +
\lambda\mathbf{I}) ^{-1}\mathbf{X}^\top)^\top \\

&amp;= (\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I})
^{-1}\mathbf{X}^\top Var(\boldsymbol\epsilon) ((\mathbf{X}^\top
\mathbf{X} + \lambda\mathbf{I}) ^{-1}\mathbf{X}^\top)^\top, \text{ since
} \mathbf{X} \text{ and } \boldsymbol{\beta} \text{ are fixed} \\

&amp;= (\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I})
^{-1}\mathbf{X}^\top (\sigma^2\mathbf{I}) ((\mathbf{X}^\top \mathbf{X} +
\lambda\mathbf{I}) ^{-1}\mathbf{X}^\top)^\top, \text{ by assumption
}  \\

&amp;= \sigma^2(\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I})
^{-1}\mathbf{X}^\top) ((\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I})
^{-1}\mathbf{X}^\top)^\top \\

&amp;= \sigma^2(\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I})
^{-1}\mathbf{X}^\top \mathbf{X} ((\mathbf{X}^\top \mathbf{X} +
\lambda\mathbf{I}) ^{-1})^\top \\

&amp; = \sigma^2(\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I})
^{-1}\mathbf{X}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X} +
\lambda\mathbf{I})^{-1} \\

\end{aligned}\]</span>
<p>We can show that the variance of the ridge regression estimator is
equal to the variance of the OLS estimator when <span
class="math inline">\(\lambda = 0\)</span>:</p>
<span class="math display">\[\begin{aligned}

Var[\hat{\boldsymbol\beta}_{ridge}] \text{ when } \lambda = 0: \\
&amp;= \sigma^2(\mathbf{X}^\top \mathbf{X} + 0\mathbf{I})
^{-1}\mathbf{X}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X} +
0\mathbf{I})^{-1} \\
&amp;= \sigma^2(\mathbf{X}^\top \mathbf{X}) ^{-1}\mathbf{X}^\top
\mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \\
&amp;= \sigma^2(\mathbf{X}^\top \mathbf{X}) ^{-1} =
Var[\hat{\boldsymbol\beta}_{OLS}]

\end{aligned}\]</span>
<p>Importantly, the variance of the ridge regression estimator is always
smaller than the variance of the OLS estimator when <span
class="math inline">\(\lambda&gt;0\)</span>. To see that this is true,
we can consider the case when <span
class="math inline">\(\mathbf{X}\)</span> is a 1 by 1 matrix with value
1 ([1]) and <span class="math inline">\(\lambda = 1\)</span>:</p>
<span class="math display">\[\begin{aligned}

Var[\hat{\boldsymbol\beta}_{ridge}] &amp;= \sigma^2(\mathbf{X}^\top
\mathbf{X} + \lambda\mathbf{I}) ^{-1}\mathbf{X}^\top \mathbf{X}
(\mathbf{X}^\top \mathbf{X} + \lambda\mathbf{I})^{-1} \\

&amp;= \sigma^2(1 *1 + 1) ^{-1}1*1 (1*1 + 1)^{-1} \\

&amp;= \sigma^2(2) ^{-1}(2)^{-1} \\

&amp;= \frac{\sigma^2}{4}

\end{aligned}\]</span>
<span class="math display">\[\begin{aligned}

Var[\hat{\boldsymbol\beta}_{OLS}] &amp;=
\sigma^2(\mathbf{X}^T\mathbf{X})^{-1} \\

&amp;= \sigma^2(1 *1) ^{-1} \\

&amp;= \frac{\sigma^2}{1} = \sigma^2

\end{aligned}\]</span>
<p>From this simple case, we can see that <span
class="math inline">\(Var[\hat{\boldsymbol\beta}_{ridge}]\)</span> is
smaller than <span
class="math inline">\(Var[\hat{\boldsymbol\beta}_{OLS}]\)</span>. This
holds true for all cases when <span
class="math inline">\(\lambda&gt;0\)</span>, but the proof of that is
beyond the scope of this project <span class="citation"
data-cites="StatLect">(<a href="#ref-StatLect"
role="doc-biblioref">Taboga, n.d.</a>)</span>.</p>
<h3 id="lasso-1">Lasso</h3>
<p>Lasso, unlike OLS and ridge regression, does not have closed form
solutions for the bias and variance of its estimator. To examine the
bias and variance of lasso estimators, we constructed a simulation and
we discuss the results of the simulation in the next section.</p>
<h2 id="simulation">Simulation</h2>
<p>For the simulation, we generated a dataset of 9 variables, 3 of which
are highly correlated with one another. The 9th variable is the <span
class="math inline">\(y\)</span> variable that we will be trying to
predict. This outcome variable is a linear combination of 2 correlated
variables, 3 independent variables, and some noninformative variables.
We also added some measurement error to <span
class="math inline">\(y\)</span>. The true form of <span
class="math inline">\(y\)</span> is as follows: <span
class="math inline">\(y = 0v_1 +2v_2 +2v_3+ 5v_4 +5v_5 +5v_6 + 3v_7 +
0v_8+\text{rnorm}(0,6)\)</span>. The rnorm adds measurement noise to
model. First, we fit an OLS model to the data, and then we fit a lasso
regression model. We compare the coefficient estimates for both the OLS
model and the lasso model to the true coefficient estimates. We also
examine the bias and variance of the estimates from both models.</p>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<h4 id="coefficient-estimates">Coefficient Estimates</h4>
<div class="layout-chunk" data-layout="l-body">
<pre><code># A tibble: 9 × 5
  term        estimate std.error statistic  p.value
  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
1 (Intercept)  -0.673    0.378      -1.78  7.46e- 2
2 V1            0.0184   0.0476      0.386 7.00e- 1
3 V2            2.13     0.108      19.8   7.76e-86
4 V3            1.86     0.0905     20.5   1.02e-91
5 V4            5.06     0.0301    168.    0       
6 V5            5.01     0.00845   593.    0       
7 V6            4.99     0.00597   837.    0       
8 V7            3.01     0.0204    148.    0       
9 V8           -0.0122   0.0302     -0.405 6.86e- 1</code></pre>
</div>
<p>The table above provides the coefficient estimates and their standard
errors for the linear model. For the correlated variables, (<span
class="math inline">\(v_1, v_2, v_3\)</span>), the standard errors are
higher than for the noncorrelated variables because the linear model
struggles to deal with multicollinearity. The linear model can
distinguish between variables with true non-zero coefficients and
noninformative variables, but it did not set the coefficients of the
noninformative variables exactly to 0.</p>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code># A tibble: 1 × 2
  penalty .config               
    &lt;dbl&gt; &lt;chr&gt;                 
1     0.1 Preprocessor1_Model001</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code># A tibble: 9 × 3
  term        estimate penalty
  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;
1 (Intercept)     2.72   0.351
2 V1              0      0.351
3 V2              2.11   0.351
4 V3              1.74   0.351
5 V4              4.88   0.351
6 V5              4.96   0.351
7 V6              4.96   0.351
8 V7              2.89   0.351
9 V8              0      0.351</code></pre>
</div>
<p>The table above displays the coefficient estimates generated by the
lasso model. Unlike the OLS model, lasso was able to set the
coefficient’s of the noninformative variables exactly to 0.</p>
<h4 id="model-accuracy">Model Accuracy</h4>
<div class="layout-chunk" data-layout="l-body">
<pre><code># A tibble: 2 × 6
  .metric .estimator  mean     n std_err .config             
  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
1 mae     standard    4.81    10  0.0352 Preprocessor1_Model1
2 rmse    standard    6.02    10  0.0451 Preprocessor1_Model1</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code># A tibble: 2 × 6
  .metric .estimator  mean     n std_err .config             
  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
1 mae     standard    4.85    10  0.0342 Preprocessor1_Model1
2 rmse    standard    6.08    10  0.0457 Preprocessor1_Model1</code></pre>
</div>
<p>The two tables above show the overall accuracy of the two models
according to two different error metrics MAE and RMSE. Comparing the
models’ accuracy reveals that the lasso model is slightly less accurate
than the OLS model; however, this difference is very small. Although the
lasso model is less accurate, it’s ability to set coefficients to 0 and
thus perform variable selection is a significant advantage over the OLS
model. In the next section, we will examine how the lasso coefficient
estimates change as we alter the value of the tuning parameter <span
class="math inline">\(\lambda\)</span>.</p>
<h4 id="changing-the-tuning-parameter">Changing the Tuning
Parameter</h4>
<div class="layout-chunk" data-layout="l-body">
<p><img src="LassoProject_files/figure-html5/unnamed-chunk-10-1.png" width="624" /></p>
</div>
<p>This graph depicts what happens to the coefficient estimates as <span
class="math inline">\(\lambda\)</span> increases. As <span
class="math inline">\(\lambda\)</span> reaches 50, all of the
coefficients are set to 0. However, the coefficients are not set to 0 at
the same time. Both the coefficients of <span
class="math inline">\(v_1\)</span> and <span
class="math inline">\(v_8\)</span>, the noninformative variables, were
set to 0 with a very small <span class="math inline">\(\lambda\)</span>.
The most important variable (because of its large variance), <span
class="math inline">\(v_6\)</span>, is set to 0 only for very large
values of <span class="math inline">\(\lambda\)</span>.</p>
<h4 id="the-bias-and-variance-of-the-coefficient-estimates">The Bias and
Variance of the Coefficient Estimates</h4>
<p>To get estimates for the bias and variance of the coefficient
estimate for both models, we sampled 100 different datasets of
coefficient values from the larger dataset generated in the
beginning.</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>              Estimate Std. Error    t value     Pr(&gt;|t|)
(Intercept) -4.4961576 4.06832662 -1.1051614 2.720031e-01
V1           0.1536838 0.48783342  0.3150333 7.534576e-01
V2           1.9078509 1.26265671  1.5109814 1.342580e-01
V3           2.0410304 0.98715975  2.0675787 4.151895e-02
V4           5.1588137 0.34129732 15.1153069 1.502605e-26
V5           4.9461718 0.07327357 67.5028130 1.595056e-79
V6           5.0132068 0.05598747 89.5415868 1.594911e-90
V7           3.2342516 0.19213361 16.8333459 1.076086e-29
V8           0.3539362 0.31637034  1.1187402 2.661954e-01</code></pre>
<pre><code>             (Intercept)        V1       V2      V3       V4       V5
my_estimates   -4.496158 0.1536838 1.907851 2.04103 5.158814 4.946172
                   V6       V7        V8
my_estimates 5.013207 3.234252 0.3539362</code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">
<p><img src="LassoProject_files/figure-html5/unnamed-chunk-16-1.png" width="624" /></p>
</div>
<p>This graph visualizes how frequently <span
class="math inline">\(v_4\)</span> had a specific coefficient value for
each model. By comparing the most commonly occuring coefficient value
for lasso and for OLS to the true value, we can tell that the lasso
coefficient is more biased than the OLS coefficient. However, the
variance of the lasso coefficient is far smaller than the variance for
OLS coefficient.</p>
<div class="layout-chunk" data-layout="l-body">
<p><img src="LassoProject_files/figure-html5/unnamed-chunk-17-1.png" width="624" /></p>
</div>
<p>This graph depicts the bias and variance for a noninformative
variable for both models. In this graph, the reduction in variance in
the lasso model is even more extreme than in the graph for the
informative variable. While both models seem to be relatively unbiased,
the lasso model’s small variance will yield more accurate predictions
overall.</p>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">
<pre><code>           Bias_lm Variance_lm  Bias_lasso Variance_lasso
V1_lm  0.066834354 0.253835703  0.01883497   0.0026286929
V2_lm  0.055503772 1.288218122  0.09150388   0.0756530955
V3_lm -0.121160270 0.902640797 -0.27157368   0.0691781158
V4_lm  0.059540947 0.091731644 -0.11984989   0.0077073755
V5_lm  0.013835419 0.007307476 -0.04399253   0.0006592307
V6_lm -0.010163536 0.003733533 -0.04331724   0.0002988397
V7_lm -0.002287099 0.042577869 -0.11252372   0.0043010517
V8_lm  0.006798568 0.112422037  0.00087243   0.0002184819
      Actual Value
V1_lm            0
V2_lm            2
V3_lm            2
V4_lm            5
V5_lm            5
V6_lm            5
V7_lm            3
V8_lm            0</code></pre>
</div>
<p>This table shows the average bias and variance for each coefficient
for both the OLS and lasso model. Overall, the variances for the
coefficients in the lasso model are much smaller than the variances in
the OLS model, but the biases are larger for the lasso model
coefficients.</p>
<h2 id="discusion">Discusion</h2>
<p>To conclude our report, we will briefly discuss the relevance,
limitations, and applications of lasso regression. Lasso is relevant
because of its ability to address the shortcomings of OLS regression
models. Specifically, lasso is able to account for multicollinearity of
predictor variables and correct for overfitting in situations with a
large number of predictors. Furthermore, unlike some penalized
regression methods (e.g., ridge regression) lasso has the ability to
perform variable selection, by shrinking the regression coefficients of
certain predictors to zero, thus improving model interpretability.</p>
<p>In the main results section, we derived the estimators for OLS and
ridge regression and the bias and variance of these estimators.
Additionally, we included relevant outputs and visualizations from a
simulation experiment in which we compared the performance of lasso and
OLS in modeling a fictitious dataset. There were two main takeaways from
our simulation experiment. First, lasso, unlike OLS, performs variable
selection by shrinking the coefficients of uninformative predictors to
zero. In the coefficient output tables, we saw that lasso set the
coefficients of uninformative predictors (which we had given a true
value of zero in the data creation stage) to zero, while OLS gave these
variables very small nonzero coefficient values. Thus, lasso helps to
simplify the model (and prevent overfitting) by eliminating predictors
with negligible effects on the output. The second main takeaway was that
lasso, in comparison to OLS, provides an advantage in terms of the
bias-variance tradeoff. The density plots from our simulations show how
lasso returns predictor coefficient estimates that are slightly more
biased, but much less variable.</p>
<p>In spite of the results of our simulation, it is important to
recognize that lasso is not a cure-all for the issues of overfitting and
multicollinearity and does not remove the need to validate a model on a
test dataset. The primary limitation of lasso is that it trades off
potential bias in estimating individual parameters for a better expected
overall prediction. In other words, under the lasso approach, regression
coefficients may not be reliably interpreted in terms of independent
risk factors, as the model’s focus is on the best combined prediction,
not the accuracy of the estimation and interpretation of the
contribution of individual variables. Also, lasso may underperform in
comparison to ridge regression in situations where the predictor
variables account for a large number of small effects on the response
variable.</p>
<p>In the real world, lasso is commonly used to handle genetic data
because the number of potential predictors is often large relative to
the number of observations and there is often little prior knowledge to
inform variable selection (Ranstam &amp; Cook 1). Lasso also has
applications in economics and finance, helping to predict events like
corporate bankruptcy. Besides these specific fields of application,
lasso is also implementable in any situation where multiple linear
regression would apply. Multiple linear regression has wide-ranging
applications, but to provide a specific example, it is often used in
medical research. Researchers may want to test whether there is a
relationship between various categorical variables (e.g., drug treatment
group, patient sex), quantitative variables (e.g., patient age, cardiac
output), and a quantitative outcome (e.g., blood pressure). Multiple
linear regression allows researchers to test for this relationship, as
well as quantify its direction and strength. Lasso regression may come
into play in scenarios where multicollinearity exists (e.g., patient
height and weight), there are a large number of predictors (and it is
likely some are uninformative), and when it is important to have
less-variable predictions for model coefficients.</p>
<h2 id="link-to-simulation-download">Link to Simulation Download</h2>
<p><a
href="https://drive.google.com/drive/folders/1yofRmpLm_sarUtlkGlucpnTjlIlnBx33?usp=sharing">Here</a></p>
<h2 id="references">References</h2>
<div class="sourceCode" id="cb9"><pre
class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<div id="refs" class="references csl-bib-body hanging-indent"
role="doc-bibliography">
<div id="ref-Springer" class="csl-entry" role="doc-biblioentry">
Gareth James, Trevor Hastie, Daniela Witten, and Robert Tibshirani.
2013. <span>“Prevent Children’s Exposure to Lead.”</span> <em>An
Introduction to Statistical Learning</em>. <a
href="https://www.ime.unicamp.br/~dias/Intoduction%20to%20Statistical%20Learning.pdf">https://www.ime.unicamp.br/~dias/Intoduction%20to%20Statistical%20Learning.pdf</a>.
</div>
<div id="ref-SaSInstitude" class="csl-entry" role="doc-biblioentry">
Gunes, Funda. 2015. <span>“Penalized Regression Methods for Linear
Models in SAS/STAT.”</span> <em>Childhood Lead Exposure: Annual Blood
Lead Levels - MN Data</em>. <a
href="https://support.sas.com/rnd/app/stat/papers/2015/PenalizedRegression_LinearModels.pdf">https://support.sas.com/rnd/app/stat/papers/2015/PenalizedRegression_LinearModels.pdf</a>.
</div>
<div id="ref-BJSSociety" class="csl-entry" role="doc-biblioentry">
Ranstam, J., and J. A. Cook. n.d. <span>“LASSO Regression.”</span>
<em>British Journal of Surgery</em>. <a
href="https://bjssjournals.onlinelibrary.wiley.com/doi/10.1002/bjs.10895">https://bjssjournals.onlinelibrary.wiley.com/doi/10.1002/bjs.10895</a>.
</div>
<div id="ref-StatLect" class="csl-entry" role="doc-biblioentry">
Taboga, Marco. n.d. <span>“Ridge Regression.”</span> <a
href="https://www.statlect.com/fundamentals-of-statistics/ridge-regression">https://www.statlect.com/fundamentals-of-statistics/ridge-regression</a>.
</div>
<div id="ref-JSTOR" class="csl-entry" role="doc-biblioentry">
Tibshirani, Robert. n.d. <span>“Regression Shrinkage and Selection via
the Lasso.”</span> <em>Journal of the Royal Statistical Society</em>. <a
href="https://www.jstor.org/stable/2346178">https://www.jstor.org/stable/2346178</a>.
</div>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
  <h3 id="references">References</h3>
  <div id="references-listing"></div>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
